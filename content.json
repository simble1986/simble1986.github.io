{"pages":[{"title":"About","permalink":"http://blog.simble.xyz/about/index.html","text":"走不完的旅程，就有说不完的故事 关于博主 已过而立之年的网络测试工程师一名，从事网络安全6年有余。现居苏州 理想是走遍全世界 可以很文艺，也可以很技术 文艺的时候，弹弹琴，拍拍照，旅旅游（且算作是文艺吧） 技术的时候，可以熬夜鼓捣树莓派，可以把游戏扔在一边折腾docker，可以一次又一次的折腾网站玩儿，还可以…… 先等会儿，我先去收拾一下我家布丁，丫的又再咬我的拖鞋 我的马蜂窝 关于博客 应该会包含三部分内容 一些技术笔记 旅行随笔/游记 晒晒狗 关于布丁 出生于2014年12月12日，可能是购物送的吧，哈哈。其实是前同事家的母金毛生的，专门挑了最活波的一条。也跟着我从北京来到了苏州"},{"title":"categories","permalink":"http://blog.simble.xyz/categories/index.html","text":""},{"title":"留言板","permalink":"http://blog.simble.xyz/guestbook/index.html","text":"既然来了，就是一种缘分，留下点什么吧:cat:"},{"title":"Tagcloud","permalink":"http://blog.simble.xyz/tags/index.html","text":""}],"posts":[{"title":"Ubuntu16.04 VPP环境搭建","permalink":"http://blog.simble.xyz/post/c80fffe0.html","text":"环境 环境准备 VmWare虚拟环境 Host需求：2cpu，4G内存，3块网卡 Ubuntu16.04 拓扑 搭建典型的c2s拓扑 通过vsphere创建两个虚拟交换机 host1的第二个接口与vpp的第二个接口连在同一个交换机上 host2的第二个接口与vpp的第三个接口连在同一个交换机上 VPP安装先决条件 Ubuntu安装git（git默认安装） Ubuntu安装dpdk，并绑定PCI的另外两个接口到dpdk 安装 官方文档提供了多种安装方式，并且推荐使用vagrant包，由于这里使用的是虚拟环境，则使用直接安装的方式 获取源码 1$ git clone https://gerrit.fd.io/r/vpp 安装vpp 1$ cd vpp Step1 可以先执行一下make查看可以执行哪些操作 Step2 - make 12345678$ make build-release...by executing \"make install-dep\"Makefile:262: recipe for target '/root/vpp/build-root/.deps.ok' failedmake: *** [/root/vpp/build-root/.deps.ok] Error 1 如果遇到以上错误，则意味着缺少依赖包 1$ make install-dep 依赖包安装完成后，再次执行make build-release Step3 - Build deb包 1$ make pkg-deb Step4 - 安装VPP packages 1$ dpkg -i /vpp/build-root/*.deb 中间可能会遇到vpp-api-python的错误，使用apt安装后重新安装packages 1$ apt install vpp-api-python 配置 123mkdir -p /etc/vppcp ./build-root/deb/debian/vpp/etc/vpp/startup.conf /etc/vpp/cp ./build-root/deb/debian/vpp/etc/sysctl.d/80-vpp.conf /etc/sysctl.d/ 修改/etc/vpp/startup.conf 123456789101112131415161718192021222324252627282930313233343536unix &#123; nodaemon log /var/log/vpp/vpp.log full-coredump #cli-listen /run/vpp/cli.sock cli-listen 0.0.0.0:5002 gid vpp&#125;api-trace &#123; on&#125;api-segment &#123; gid vpp&#125;socksvr &#123; default&#125;cpu &#123; main-core 0 workers 2&#125;dpdk &#123; dev 0000:0b:00.0 &#123;num-rx-queues 2&#125; dev 0000:13:00.0 &#123;num-rx-queues 2&#125; num-mbufs 128000 socket-mem 1024,1024 &#125;plugins &#123; path /root/vpp/build-root/install-vpp-native/vpp/lib/vpp_plugins&#125; 启动VPP 1$ service vpp start 查看状态 123456789101112131415161718192021222324$ systemctl status vpp.service● vpp.service - vector packet processing engine Loaded: loaded (/lib/systemd/system/vpp.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2018-11-15 16:00:09 CST; 4min 52s ago Process: 19959 ExecStopPost=/bin/rm -f /dev/shm/db /dev/shm/global_vm /dev/shm/vpe-api (code=exited, status=0/SUCCESS) Process: 19985 ExecStartPre=/sbin/modprobe uio_pci_generic (code=exited, status=0/SUCCESS) Process: 19981 ExecStartPre=/bin/rm -f /dev/shm/db /dev/shm/global_vm /dev/shm/vpe-api (code=exited, status=0/SUCCESS) Main PID: 19989 (vpp_main) Tasks: 6 Memory: 55.9M CPU: 9min 46.890s CGroup: /system.slice/vpp.service └─19989 /usr/bin/vpp -c /etc/vpp/startup.confNov 15 16:00:09 fdio /usr/bin/vpp[19989]: load_one_vat_plugin:67: Loaded plugin: nat_test_plugin.soNov 15 16:00:09 fdio /usr/bin/vpp[19989]: load_one_vat_plugin:67: Loaded plugin: lb_test_plugin.soNov 15 16:00:09 fdio /usr/bin/vpp[19989]: load_one_vat_plugin:67: Loaded plugin: gtpu_test_plugin.soNov 15 16:00:09 fdio /usr/bin/vpp[19989]: load_one_vat_plugin:67: Loaded plugin: avf_test_plugin.soNov 15 16:00:09 fdio /usr/bin/vpp[19989]: load_one_vat_plugin:67: Loaded plugin: acl_test_plugin.soNov 15 16:00:09 fdio /usr/bin/vpp[19989]: load_one_vat_plugin:67: Loaded plugin: memif_test_plugin.soNov 15 16:00:09 fdio /usr/bin/vpp[19989]: load_one_vat_plugin:67: Loaded plugin: stn_test_plugin.soNov 15 16:00:09 fdio vpp[19989]: /usr/bin/vpp[19989]: dpdk: EAL init args: -c 7 -n 4 --huge-dir /run/vpp/hugepages --file-prefix vpp -w 0000:0b:00.0 -w 0000:13:00.0 --master-lcorNov 15 16:00:09 fdio /usr/bin/vpp[19989]: dpdk: EAL init args: -c 7 -n 4 --huge-dir /run/vpp/hugepages --file-prefix vpp -w 0000:0b:00.0 -w 0000:13:00.0 --master-lcore 0 --socketNov 15 16:00:10 fdio vnet[19989]: dpdk_ipsec_process:1015: not enough DPDK crypto resources, default to OpenSSL 虽然有警告信息，但似乎不影响使用 测试 根据startup.conf中的配置不同，选择不同的连接vpp方式 登陆VPP CLI 123456789101112131415$ telnet 127.0.0.1 5002Trying 127.0.0.1...Connected to 127.0.0.1.Escape character is '^]'. _______ _ _ _____ ___ __/ __/ _ \\ (_)__ | | / / _ \\/ _ \\ _/ _// // / / / _ \\ | |/ / ___/ ___/ /_/ /____(_)_/\\___/ |___/_/ /_/vpp# show interface Name Idx State MTU (L3/IP4/IP6/MPLS) Counter CountGigabitEthernet13/0/0 2 down 9000/0/0/0GigabitEthernetb/0/0 1 down 9000/0/0/0local0 0 down 0/0/0/0vpp# 开启端口 1234567891011121314vpp# set interface state GigabitEthernet13/0/0 upvpp# set interface state GigabitEthernetb/0/0 upvpp# show interface Name Idx State MTU (L3/IP4/IP6/MPLS) Counter CountGigabitEthernet13/0/0 2 up 9000/0/0/0 rx packets 1 rx bytes 60 drops 1 ip4 1GigabitEthernetb/0/0 1 up 9000/0/0/0 rx packets 4 rx bytes 240 drops 4 ip4 1local0 0 down 0/0/0/0vpp# 配置为switch模式 12345vpp# set interface l2 bridge GigabitEthernet13/0/0 1vpp# set interface l2 bridge GigabitEthernetb/0/0 1vpp# show bridge-domain BD-ID Index BSN Age(min) Learning U-Forwrd UU-Flood Flooding ARP-Term BVI-Intf 1 1 0 off on on flood on off N/A 测试连通性 c2s Host1: 192.168.0.2/24 Host2: 192.168.0.3/24 通过host1 ping host2 配置loopback口测试连通性 123456789101112131415vpp# create loopback interfaceloop0vpp# set interface ip address loop0 192.168.0.1/24vpp# set interface state loop0 upvpp# set interface l2 bridge loop0 1 bvivpp# show bridge-domain BD-ID Index BSN Age(min) Learning U-Forwrd UU-Flood Flooding ARP-Term BVI-Intf 1 1 0 off on on flood on off loop0vpp# ping 192.168.0.2116 bytes from 192.168.0.2: icmp_seq=2 ttl=64 time=.8704 ms116 bytes from 192.168.0.2: icmp_seq=3 ttl=64 time=.2564 ms116 bytes from 192.168.0.2: icmp_seq=4 ttl=64 time=.2653 ms116 bytes from 192.168.0.2: icmp_seq=5 ttl=64 time=.3413 msStatistics: 5 sent, 4 received, 20% packet loss set interface l2 bridge loop0 1 bvi中的bvi意味着这个接口将用来接收、发送以及转发该bridge domain的报文"},{"title":"Ubuntu16.04上安装DPDK","permalink":"http://blog.simble.xyz/post/879abefb.html","text":"DPDK安装 DPDK（Data Plane Development Kit）是一个用来进行包数据处理加速的软件库 从git获取源码 1$ git clone git://dpdk.org/dpdk 创建环境变量 1234$ cd ~/dpdk$ export RTE_SDK=`pwd`$ export DESTDIR=`pwd`$ export RTE_TARGET=x86_64-default-linuxapp-gcc 因为这些环境变量总是会用到，可以将其放入一个文件，如env.source，使用source env.source 123456$ cd ~/dpdk$ more env.sourceexport RTE_SDK=`pwd`export DESTDIR=`pwd`export RTE_TARGET=x86_64-default-linuxapp-gcc$ source env.source 开始安装 使用dpdk-setup.sh脚本进行安装 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061./usertools/dpdk-setup.sh------------------------------------------------------------------------------ RTE_SDK exported as /root/dpdk---------------------------------------------------------------------------------------------------------------------------------------- Step 1: Select the DPDK environment to build----------------------------------------------------------[1] arm64-armv8a-linuxapp-clang[2] arm64-armv8a-linuxapp-gcc[3] arm64-dpaa2-linuxapp-gcc[4] arm64-dpaa-linuxapp-gcc[5] arm64-stingray-linuxapp-gcc[6] arm64-thunderx-linuxapp-gcc[7] arm64-xgene1-linuxapp-gcc[8] arm-armv7a-linuxapp-gcc[9] i686-native-linuxapp-gcc[10] i686-native-linuxapp-icc[11] ppc_64-power8-linuxapp-gcc[12] x86_64-native-bsdapp-clang[13] x86_64-native-bsdapp-gcc[14] x86_64-native-linuxapp-clang[15] x86_64-native-linuxapp-gcc[16] x86_64-native-linuxapp-icc[17] x86_x32-native-linuxapp-gcc---------------------------------------------------------- Step 2: Setup linuxapp environment----------------------------------------------------------[18] Insert IGB UIO module[19] Insert VFIO module[20] Insert KNI module[21] Setup hugepage mappings for non-NUMA systems[22] Setup hugepage mappings for NUMA systems[23] Display current Ethernet/Crypto device settings[24] Bind Ethernet/Crypto device to IGB UIO module[25] Bind Ethernet/Crypto device to VFIO module[26] Setup VFIO permissions---------------------------------------------------------- Step 3: Run test application for linuxapp environment----------------------------------------------------------[27] Run test application ($RTE_TARGET/app/test)[28] Run testpmd application in interactive mode ($RTE_TARGET/app/testpmd)---------------------------------------------------------- Step 4: Other tools----------------------------------------------------------[29] List hugepage info from /proc/meminfo---------------------------------------------------------- Step 5: Uninstall and system cleanup----------------------------------------------------------[30] Unbind devices from IGB UIO or VFIO driver[31] Remove IGB UIO module[32] Remove VFIO module[33] Remove KNI module[34] Remove hugepage mappings[35] Exit ScriptOption: Step1 根据自己环境选择相应的build，如我是64位的Intel架构的环境，则选择**[15]** 12345678...Installation in /root/dpdk/ complete------------------------------------------------------------------------------ RTE_TARGET exported as x86_64-native-linuxapp-gcc------------------------------------------------------------------------------Press enter to continue ... Step2 选择**[18]**来家在哪里igb_uio模块 选择**[21]**来创建Hugepage，这里我输入了128 选择**[24]**来绑定PCI网卡 123456789101112131415161718192021222324252627282930Option: 21Removing currently reserved hugepagesUnmounting /mnt/huge and removing directory Input the number of 2048kB hugepages Example: to have 128MB of hugepages available in a 2MB huge page system, enter '64' to reserve 64 * 2MB pagesNumber of pages: 128Reserving hugepagesCreating /mnt/huge and mounting as hugetlbfsPress enter to continue .........Option: 24......Other Compress devices======================&lt;none&gt;Enter PCI address of device to bind to IGB UIO driver: 0b:00.0Network devices using DPDK-compatible driver============================================0000:0b:00.0 'VMXNET3 Ethernet Controller 07b0' drv=igb_uio unused=vmxnet3,uio_pci_generic0000:13:00.0 'VMXNET3 Ethernet Controller 07b0' drv=igb_uio unused=vmxnet3,uio_pci_generic 确认需要使用的PCI网卡为drv=igb_uio则说明绑定成功 Step3 - 测试 123456789101112131415161718192021Option: 27 Enter hex bitmask of cores to execute test app on Example: to execute app on cores 0 to 7, enter 0xffbitmask: 0x3Launching appsudo: x86_64-default-linuxapp-gcc/app/test: command not foundPress enter to continue ......Option: 28 Enter hex bitmask of cores to execute testpmd app on Example: to execute app on cores 0 to 7, enter 0xffbitmask: 0x3Launching appsudo: x86_64-default-linuxapp-gcc/app/testpmd: command not found 执行27和28都有可能报错，提示command not found，此时，可以先退出安装脚本 进入/dpdk/x86_64-native-linuxapp-gcc/app目录，会看到testpmd存在于目录下，运行测试，正常状况时，会如下显示 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859$ ./testpmdEAL: Detected 8 lcore(s)EAL: Detected 1 NUMA nodesEAL: Multi-process socket /var/run/dpdk/rte/mp_socketEAL: Probing VFIO support...EAL: PCI device 0000:03:00.0 on NUMA socket -1EAL: Invalid NUMA socket, default to 0EAL: probe driver: 15ad:7b0 net_vmxnet3EAL: PCI device 0000:0b:00.0 on NUMA socket -1EAL: Invalid NUMA socket, default to 0EAL: probe driver: 15ad:7b0 net_vmxnet3EAL: PCI device 0000:13:00.0 on NUMA socket -1EAL: Invalid NUMA socket, default to 0EAL: probe driver: 15ad:7b0 net_vmxnet3testpmd: create a new mbuf pool &lt;mbuf_pool_socket_0&gt;: n=203456, size=2176, socket=0testpmd: preferred mempool ops selected: ring_mp_mcConfiguring Port 0 (socket 0)...... TX queue: 0 TX desc=0 - TX free threshold=0 TX threshold registers: pthresh=0 hthresh=0 wthresh=0 TX offloads=0x0 - TX RS bit threshold=0Press enter to exitTelling cores to stop...Waiting for lcores to finish... ---------------------- Forward statistics for port 0 ---------------------- RX-packets: 57 RX-dropped: 0 RX-total: 57 TX-packets: 57 TX-dropped: 0 TX-total: 57 ---------------------------------------------------------------------------- ---------------------- Forward statistics for port 1 ---------------------- RX-packets: 57 RX-dropped: 0 RX-total: 57 TX-packets: 57 TX-dropped: 0 TX-total: 57 ---------------------------------------------------------------------------- +++++++++++++++ Accumulated forward statistics for all ports+++++++++++++++ RX-packets: 114 RX-dropped: 0 RX-total: 114 TX-packets: 114 TX-dropped: 0 TX-total: 114 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++Done.Shutting down port 0...Stopping ports...DoneClosing ports...DoneShutting down port 1...Stopping ports...DoneClosing ports...DoneBye... HugePage问题解决 问题 当运行测试时或者testpmd，可能会遇到如下问题 12345678910111213141516$ ./testpmdEAL: Detected 8 lcore(s)EAL: Detected 1 NUMA nodesEAL: Multi-process socket /var/run/dpdk/rte/mp_socketEAL: No free hugepages reported in hugepages-2048kBEAL: No free hugepages reported in hugepages-2048kBEAL: FATAL: Cannot get hugepage information.EAL: Cannot get hugepage information.PANIC in main():Cannot init EAL5: [./testpmd(_start+0x29) [0x498829]]4: [/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0) [0x7f8a0fee0830]]3: [./testpmd(main+0xc48) [0x48f528]]2: [./testpmd(__rte_panic+0xbb) [0x47eb09]]1: [./testpmd(rte_dump_stack+0x2b) [0x5c8a1b]]Aborted (core dumped) 问题原因及解决 这说明Hugepage不够用，可以先查看系统内存状况 1234567$ cat /proc/meminfo | grep HugeAnonHugePages: 0 kBHugePages_Total: 665HugePages_Free: 0HugePages_Rsvd: 0HugePages_Surp: 537Hugepagesize: 2048 kB 很显然，这里总共有665，太小了，不够用，需要修改系统相关内容 12345678$ echo 2048 &gt; /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages$ cat /proc/meminfo | grep HugeAnonHugePages: 0 kBHugePages_Total: 2048HugePages_Free: 1080HugePages_Rsvd: 0HugePages_Surp: 0Hugepagesize: 2048 kB 注 如果系统重启或者重新编译，则该值会被重新刷新为默认值，需要重新设置"},{"title":"OVS学习笔记——常用命令练习","permalink":"http://blog.simble.xyz/post/4c6bf73b.html","text":"控制管理 创建网桥 1$ ovs-vsctl add-br testbr 查看网桥和端口 1234567$ ovs-vsctl showb66c0897-27c9-441a-9486-42cfb65a4649 Bridge testbr Port testbr Interface testbr type: internal ovs_version: \"2.5.5\" 网桥端口操作 12$ ovs-vsctl add-port br0 eth1$ ovs-vsctl del-port br0 eth1 查看流表 123$ ovs-ofctl dump-flows testbrNXST_FLOW reply (xid=0x4): cookie=0x0, duration=364.789s, table=0, n_packets=0, n_bytes=0, idle_age=364, priority=0 actions=NORMAL 控制器设置 123456789101112131415161718192021222324252627282930# 设置控制器$ ovs-vsctl set-controller testbr tcp:10.180.9.62:6633$ ovs-vsctl showb66c0897-27c9-441a-9486-42cfb65a4649 Bridge testbr Controller \"tcp:10.180.9.62:6633\" Port testbr Interface testbr type: internal ovs_version: \"2.5.5\"# 查看控制器列表$ ovs-vsctl list controller_uuid : 2fe35662-3f4f-446b-9296-6f1eae38ba5econnection_mode : []controller_burst_limit: []controller_rate_limit: []enable_async_messages: []external_ids : &#123;&#125;inactivity_probe : []is_connected : truelocal_gateway : []local_ip : []local_netmask : []max_backoff : []other_config : &#123;&#125;role : otherstatus : &#123;sec_since_connect=\"4\", state=ACTIVE&#125;target : \"tcp:10.180.9.62:6633\"# 删除控制器$ ovs-vsctl del-controller testbr 接口相关 1234567891011121314$ ovs-ofctl dump-ports s1OFPST_PORT reply (xid=0x2): 3 ports port LOCAL: rx pkts=0, bytes=0, drop=94, errs=0, frame=0, over=0, crc=0 tx pkts=0, bytes=0, drop=0, errs=0, coll=0 port 1: rx pkts=124, bytes=8418, drop=0, errs=0, frame=0, over=0, crc=0 tx pkts=130, bytes=8898, drop=0, errs=0, coll=0 port 2: rx pkts=123, bytes=8340, drop=0, errs=0, frame=0, over=0, crc=0 tx pkts=130, bytes=8886, drop=0, errs=0, coll=0$ ovs-appctl dpif/showsystem@ovs-system: hit:318465 missed:735 s1: s1 65534/3: (internal) s1-eth1 1/2: (system) s1-eth2 2/1: (system) 流表类 流表操作 查看流表 1234$ ovs-ofctl dump-flows s1NXST_FLOW reply (xid=0x4): cookie=0x0, duration=356.689s, table=0, n_packets=708, n_bytes=42480, idle_age=0, priority=65535,dl_dst=01:80:c2:00:00:0e,dl_type=0x88cc actions=CONTROLLER:65535 cookie=0x0, duration=356.699s, table=0, n_packets=94, n_bytes=7488, idle_age=346, priority=0 actions=CONTROLLER:65535 添加普通流表 123456$ ovs-ofctl add-flow s1 in_port=1,actions=drop$ ovs-ofctl dump-flows s1NXST_FLOW reply (xid=0x4): cookie=0x0, duration=441.879s, table=0, n_packets=878, n_bytes=52680, idle_age=0, priority=65535,dl_dst=01:80:c2:00:00:0e,dl_type=0x88cc actions=CONTROLLER:65535 cookie=0x0, duration=2.861s, table=0, n_packets=0, n_bytes=0, idle_age=2, in_port=1 actions=drop cookie=0x0, duration=441.889s, table=0, n_packets=94, n_bytes=7488, idle_age=432, priority=0 actions=CONTROLLER:65535 按照匹配删除流表 12345$ ovs-ofctl del-flows s1 \"in_port=1\"mininet&gt; sh ovs-ofctl dump-flows s1NXST_FLOW reply (xid=0x4): cookie=0x0, duration=521.249s, table=0, n_packets=1036, n_bytes=62160, idle_age=0, priority=65535,dl_dst=01:80:c2:00:00:0e,dl_type=0x88cc actions=CONTROLLER:65535 cookie=0x0, duration=521.259s, table=0, n_packets=94, n_bytes=7488, idle_age=511, priority=0 actions=CONTROLLER:65535 常用匹配项 VLAN Tag 123456$ vs-ofctl add-flow s1 priority=401,in_port=1,dl_vlan=777,actions=output:2$ ovs-ofctl dump-flows s1NXST_FLOW reply (xid=0x4): cookie=0x0, duration=663.043s, table=0, n_packets=1318, n_bytes=79080, idle_age=0, priority=65535,dl_dst=01:80:c2:00:00:0e,dl_type=0x88cc actions=CONTROLLER:65535 cookie=0x0, duration=3.022s, table=0, n_packets=0, n_bytes=0, idle_age=3, priority=401,in_port=1,dl_vlan=777 actions=output:2 cookie=0x0, duration=663.053s, table=0, n_packets=94, n_bytes=7488, idle_age=653, priority=0 actions=CONTROLLER:65535 MAC 123456$ ovs-ofctl add-flow s3 in_port=1,dl_src=0a:f6:95:7e:c6:4a/0a:f6:95:7e:c6:4a,action=output:3$ ovs-ofctl add-flow s3 in_port=1,dl_dst=be:7c:6a:e9:e6:b1/be:7c:6a:e9:e6:b1,action=output:2$ sh ovs-ofctl dump-flows s3NXST_FLOW reply (xid=0x4): cookie=0x0, duration=69.067s, table=0, n_packets=0, n_bytes=0, idle_age=69, in_port=1,dl_src=0a:f6:95:7e:c6:4a/0a:f6:95:7e:c6:4a actions=output:3 cookie=0x0, duration=14.496s, table=0, n_packets=0, n_bytes=0, idle_age=14, in_port=1,dl_dst=be:7c:6a:e9:e6:b1/be:7c:6a:e9:e6:b1 actions=output:2 IP 12345678$ ovs-ofctl add-flow s3 ip,in_port=1,nw_src=192.168.0.0/16,action=drop$ ovs-ofctl add-flow s3 ip,in_port=1,nw_dst=192.168.0.0/16,action=drop$ ovs-ofctl dump-flows s3NXST_FLOW reply (xid=0x4): cookie=0x0, duration=119.033s, table=0, n_packets=119, n_bytes=7140, idle_age=0, priority=65535,dl_dst=01:80:c2:00:00:0e,dl_type=0x88cc actions=CONTROLLER:65535 cookie=0x0, duration=28.864s, table=0, n_packets=0, n_bytes=0, idle_age=28, ip,in_port=1,nw_src=192.168.0.0/16 actions=drop cookie=0x0, duration=10.036s, table=0, n_packets=0, n_bytes=0, idle_age=10, ip,in_port=1,nw_dst=192.168.0.0/16 actions=drop cookie=0x0, duration=119.057s, table=0, n_packets=90, n_bytes=7164, idle_age=109, priority=0 actions=CONTROLLER:65535 其他 匹配项 关键字 条件 举例 以太网类型 dl_type in_port=1,dl_type=0x0806,actions=output:2 协议号 nw_proto 指定dl_type=0x0800或者ip ip,in_port=1,nw_proto=1,actions=output:2 TCP flags tcp_flags 指定TCP tcp,tcp_flags=ack,actions=output:2 一些速记符 速记符 匹配项 ip dl_type=0x800 ipv6 dl_type=0x86dd icmp dl_type=0x0800,nw_proto=1 icmp6 dl_type=0x86dd,nw_proto=58 tcp dl_type=0x0800,nw_proto=6 tcp6 dl_type=0x86dd,nw_proto=6 udp dl_type=0x0800,nw_proto=17 udp6 dl_type=0x86dd,nw_proto=17 arp dl_type=0x0806 指令动作（actions） 基础动作 动作 说明 举例 normal L2/L3处理 actions=normal output 出接口 actions=output:2 group 指定的group actions=group:1 flood 从所有物理接口转发出去，除了入接口和已关闭flooding的接口 actions=flood all 从所有物理接口转发出去，除了入接口 actions=all local 转发给本地网桥 actions=local in_port 从入接口转发出去 actions=in_port controller 以packet-in消息上送给控制器 actions=controller drop 丢弃数据包 actions=drop 修改VLAN ID 关键字： mod_vlan_vid 举例 1$ ovs-ofctl add-flow s1 in_port=1,actions=mod_vlan_vid:1034,output:2 剥除VLAN 关键字： strip_vlan 举例 1$ ovs-ofctl add-flow s1 in_port=1,actions=strip_vlan,output:2 弹出最外层VLAN 关键字： pop_vlan 举例 1$ ovs-ofctl add-flow br0 in_port=1,dl_type=0x8100,dl_vlan=777,actions=pop_vlan,output:2 修改源/目的MAC 关键字：mod_dl_src / mod_dl_dst 举例 12$ ovs-ofctl add-flow s1 in_port=1,actions=mod_dl_src:01:80:c2:00:00:0e,output:2$ ovs-ofctl add-flow s1 in_port=1,actions=mod_dl_dst:01:80:c2:00:00:0e,output:2 修改源/目的IP 关键字： mod_nw_src/mod_nw_dst 举例 12$ ovs-ofctl add-flow s1 in_port=1,actions=mod_nw_src:192.168.0.10,output:2$ ovs-ofctl add-flow s1 in_port=1,actions=mod_nw_dst:192.168.0.10,output:2 修改TCP/UDP端口 关键字：mod_tp_src/mod_tp_dst 举例 1234$ ovs-ofctl add-flow s1 tcp,in_port=1,actions=mod_tp_src:1039,output:2$ ovs-ofctl add-flow s1 tcp,in_port=1,actions=mod_tp_dst:21,output:2$ ovs-ofctl add-flow s1 udp,in_port=1,actions=mod_tp_src:1039,output:2$ ovs-ofctl add-flow s1 udp,in_port=1,actions=mod_tp_dst:53,output:2 VxLan 创建VxLAN接口 12345678910111213141516171819$ ovs-vsctl add-port s3 vxlan1 -- set Interface vxlan1 type=vxlan options:remote_ip=1.1.1.1 ofport_request=2000$ ovs-vsctl show Bridge \"s3\" Controller \"tcp:10.180.9.62:6633\" Controller \"ptcp:6636\" fail_mode: secure Port \"s3-eth2\" Interface \"s3-eth2\" Port \"vxlan1\" Interface \"vxlan1\" type: vxlan options: &#123;remote_ip=\"1.1.1.1\"&#125; Port \"s3-eth3\" Interface \"s3-eth3\" Port \"s3-eth1\" Interface \"s3-eth1\" Port \"s3\" Interface \"s3\" type: internal VxLAN流表 123456$ ovs-ofctl add-flow s3 ip,in_port=1,nw_dst=192.168.0.0/16,actions=output:2000$ ovs-ofctl add-flow s3 in_port=2000,actions=output:1$ ovs-ofctl dump-flows s3NXST_FLOW reply (xid=0x4): cookie=0x0, duration=35.227s, table=0, n_packets=0, n_bytes=0, idle_age=35, ip,in_port=1,nw_dst=192.168.0.0/16 actions=output:2000 cookie=0x0, duration=2.469s, table=0, n_packets=0, n_bytes=0, idle_age=2, in_port=2000 actions=output:1 实验 拓扑 12345678910111213 +------------+ | s2 | +---+----+---+ | | +----------+ +----------+ | |+--------+--------+ +--------+--------+| s3 | | s4 |+---+---------+---+ +---+---------+---+ | | | |+---+--+ +--+---+ +---+--+ +--+---+| h1 | | h2 | | h3 | | h4 |+------+ +------+ +------+ +------+ 实验要求 h1可以与h3通信，但不可以与h2和h4通信 h2可以与h4通信，但不可以与h1和h3通信 具体操作 由h1送出的报文，在s3上打上vlan tag 1000 随后s3将报文送往s2 s2收到s3的vlan1000的报文，直接转送s4 s4收到vlan1000的报文后，剥离vlan，送到h3 h3收到请求报文后，返回响应报文，送往s4 s4收到h3的报文后，打上vlan tag 1000 随后s4将报文送往s2 s2收到s4的vlan1000的报文，直接送往s3 s3收到s2的vlan1000报文后，剥离vlan，送往h1 h1-h3流表 1234567891011121314$ ovs-ofctl dump-flows s2NXST_FLOW reply (xid=0x4): cookie=0x0, duration=1664.027s, table=0, n_packets=79, n_bytes=4026, idle_age=803, in_port=1,dl_vlan=1000 actions=output:2 cookie=0x0, duration=1642.112s, table=0, n_packets=10, n_bytes=852, idle_age=803, in_port=2,dl_vlan=1000 actions=output:1$ ovs-ofctl dump-flows s3NXST_FLOW reply (xid=0x4): cookie=0x0, duration=2826.459s, table=0, n_packets=330, n_bytes=14700, idle_age=807, in_port=1 actions=mod_vlan_vid:1000,output:3 cookie=0x0, duration=1895.062s, table=0, n_packets=10, n_bytes=852, idle_age=807, in_port=3,dl_vlan=1000 actions=strip_vlan,output:1$ ovs-ofctl dump-flows s4NXST_FLOW reply (xid=0x4): cookie=0x0, duration=2776.175s, table=0, n_packets=229, n_bytes=10010, idle_age=810, in_port=1 actions=mod_vlan_vid:1000,output:3 cookie=0x0, duration=1507.500s, table=0, n_packets=10, n_bytes=852, idle_age=810, in_port=3,dl_vlan=1000 actions=strip_vlan,output:1 h2-h4流表 1234567891011121314$ ovs-ofctl dump-flows s2NXST_FLOW reply (xid=0x4): cookie=0x0, duration=827.167s, table=0, n_packets=60, n_bytes=2816, idle_age=698, in_port=2,dl_vlan=2000 actions=output:1 cookie=0x0, duration=812.342s, table=0, n_packets=60, n_bytes=2816, idle_age=698, in_port=1,dl_vlan=2000 actions=output:2 $ ovs-ofctl dump-flows s3NXST_FLOW reply (xid=0x4): cookie=0x0, duration=1347.703s, table=0, n_packets=60, n_bytes=2576, idle_age=702, in_port=2 actions=mod_vlan_vid:2000,output:3 cookie=0x0, duration=710.639s, table=0, n_packets=3, n_bytes=194, idle_age=702, in_port=3,dl_vlan=2000 actions=strip_vlan,output:2 $ ovs-ofctl dump-flows s4NXST_FLOW reply (xid=0x4): cookie=0x0, duration=1133.570s, table=0, n_packets=60, n_bytes=2576, idle_age=705, in_port=2 actions=mod_vlan_vid:2000,output:3 cookie=0x0, duration=1088.590s, table=0, n_packets=60, n_bytes=2816, idle_age=705, in_port=3,dl_vlan=2000 actions=strip_vlan,output:2 命令列表 h1-h3 123456ovs-ofctl add-flow s2 in_port=1,dl_vlan=1000,actions=output:2ovs-ofctl add-flow s2 in_port=2,dl_vlan=1000,actions=output:1ovs-ofctl add-flow s3 in_port=1,actions=mod_vlan_vid:1000,output:3ovs-ofctl add-flow s3 in_port=3,dl_vlan=1000,actions=strip_vlan,output:1ovs-ofctl add-flow s4 in_port=1,actions=mod_vlan_vid:1000,output:3ovs-ofctl add-flow s4 in_port=3,dl_vlan=1000,actions=strip_vlan,output:1 h2-h4 123456ovs-ofctl add-flow s2 in_port=2,dl_vlan=2000,actions=output:1ovs-ofctl add-flow s2 in_port=1,dl_vlan=2000,actions=output:2ovs-ofctl add-flow s3 in_port=2,actions=mod_vlan_vid:2000,output:3ovs-ofctl add-flow s3 in_port=3,dl_vlan=2000,actions=strip_vlan,output:2ovs-ofctl add-flow s4 in_port=2,actions=mod_vlan_vid:2000,output:3ovs-ofctl add-flow s4 in_port=3,dl_vlan=2000,actions=strip_vlan,output:2"},{"title":"Mininet自定义topo","permalink":"http://blog.simble.xyz/post/dd7d5c81.html","text":"mininet自带topo 通过mn -h可以看到Mininet自带的几种topo类型，分别有Linear, minimal, reversed, single, torus和tree类型，但有时候这些类型无法满足需求，需要自定义topo 12345678910$ mn --helpUsage: mn [options](type mn -h for details)... --topo=TOPO linear|minimal|reversed|single|torus|tree[,param=value ...] linear=LinearTopo torus=TorusTopo tree=TreeTopo single=SingleSwitchTopo reversed=SingleSwitchReversedTopo minimal=MinimalTopo 获取示例 Mininet提供了topo-2sw-2host的示例，可以通过Mininet github的custom目录下获取 12345678910111213141516171819202122232425262728293031\"\"\"Custom topology exampleTwo directly connected switches plus a host for each switch: host --- switch --- switch --- hostAdding the 'topos' dict with a key/value pair to generate our newly definedtopology enables one to pass in '--topo=mytopo' from the command line.\"\"\"from mininet.topo import Topoclass MyTopo( Topo ): \"Simple topology example.\" def __init__( self ): \"Create custom topo.\" # Initialize topology Topo.__init__( self ) # Add hosts and switches leftHost = self.addHost( 'h1' ) rightHost = self.addHost( 'h2' ) leftSwitch = self.addSwitch( 's3' ) rightSwitch = self.addSwitch( 's4' ) # Add links self.addLink( leftHost, leftSwitch ) self.addLink( leftSwitch, rightSwitch ) self.addLink( rightSwitch, rightHost )topos = &#123; 'mytopo': ( lambda: MyTopo() ) &#125; 自定义topo 如要创建如下topo 1234567891011 +------------+ +----------+ s3 +---------+ | +------------+ | | | +----+----+ +----+----+ +-----+ s1 +-----+ +-----+ s2 +-----+ | +---------+ | | +---------+ | | | | |+--+--+ +--+--+ +--+--+ +--+--+| h1 | | h2 | | h3 | | h4 |+-----+ +-----+ +-----+ +-----+ 代码 123456789101112131415161718192021222324252627282930from mininet.topo import Topoclass MyTopo( Topo ): \"Simple topology example.\" def __init__( self ): \"Create custom topo.\" # Initialize topology Topo.__init__( self ) # Add hosts and switches h1 = self.addHost( 'h1' ) h2 = self.addHost( 'h2' ) h3 = self.addHost( 'h3' ) h4 = self.addHost( 'h4' ) s1 = self.addSwitch( 's1' ) s2 = self.addSwitch( 's2' ) s3 = self.addSwitch( 's3' ) # Add links self.addLink( h1, s1 ) self.addLink( s1, h2 ) self.addLink( s1, s3 ) self.addLink( s3, s2 ) self.addLink( h3, s2 ) self.addLink( s2, h4 )topos = &#123; 'mytopo': ( lambda: MyTopo() ) &#125; 使用自定义topo 1234567891011121314151617root@mininet:~# mn --custom ./testtopo.py --topo mytopo --controller=remote,ip=127.0.0.1,port=6633*** Creating network*** Adding controller*** Adding hosts:h1 h2 h3 h4*** Adding switches:s1 s2 s3*** Adding links:(h1, s1) (h3, s2) (s1, h2) (s1, s3) (s2, h4) (s3, s2)*** Configuring hostsh1 h2 h3 h4*** Starting controllerc0*** Starting 3 switchess1 s2 s3 ...*** Starting CLI:mininet&gt; 查看连接状态 123456789101112131415mininet&gt; linksh1-eth0&lt;-&gt;s1-eth1 (OK OK)h3-eth0&lt;-&gt;s2-eth2 (OK OK)s1-eth2&lt;-&gt;h2-eth0 (OK OK)s1-eth3&lt;-&gt;s3-eth1 (OK OK)s2-eth3&lt;-&gt;h4-eth0 (OK OK)s3-eth2&lt;-&gt;s2-eth1 (OK OK)mininet&gt; pingall*** Ping: testing ping reachabilityh1 -&gt; h2 h3 h4h2 -&gt; h1 h3 h4h3 -&gt; h1 h2 h4h4 -&gt; h1 h2 h3*** Results: 0% dropped (12/12 received)mininet&gt;"},{"title":"Opendaylight Oxygen环境准备","permalink":"http://blog.simble.xyz/post/5594d322.html","text":"安装Opendaylight 环境准备 基础包 1$ apt-get install unzip lrzsz 安装jdk 1$ apt-get install openjdk-8-jdk 设置JAVA_HOME 在/etc/environment的末尾添加JAVA_HOME=&quot;/usr/lib/jvm/java-8-openjdk-amd64&quot;，需要退出当前终端重新登陆 获取安装包 https://docs.opendaylight.org/en/latest/downloads.html 运行karaf 123456789101112131415161718192021$ unzip karaf-0.8.3.zip ...$ cd karaf-0.8.3/$ ./bin/karafApache Karaf starting up. Press Enter to open the shell now...100% [========================================================================]Karaf started in 1s. Bundle stats: 54 active, 55 total ________ ________ .__ .__ .__ __ \\_____ \\ ______ ____ ____ \\______ \\ _____ ___.__.| | |__| ____ | |___/ |_ / | \\\\____ \\_/ __ \\ / \\ | | \\\\__ \\&lt; | || | | |/ ___\\| | \\ __\\ / | \\ |_&gt; &gt; ___/| | \\| ` \\/ __ \\\\___ || |_| / /_/ &gt; Y \\ | \\_______ / __/ \\___ &gt;___| /_______ (____ / ____||____/__\\___ /|___| /__| \\/|__| \\/ \\/ \\/ \\/\\/ /_____/ \\/Hit '&lt;tab&gt;' for a list of available commandsand '[cmd] --help' for help on a specific command.Hit '&lt;ctrl-d&gt;' or type 'system:shutdown' or 'logout' to shutdown OpenDaylight.opendaylight-user@root&gt; 安装feature 说起来，这真的是一件让人崩溃的事情，不同的版本，安装feature不同，在什么都还不懂的情况下安装feature，遇到了无数的问题，终于当我将要换到更老的版本之前（0.7.3），让我找到了**Oxygen（0.8.3）**的feature 1opendaylight-user@root&gt;feature:install odl-restconf odl-l2switch-switch odl-dlux-core odl-dluxapps-nodes odl-dluxapps-topology odl-dluxapps-yangui odl-dluxapps-yangvisualizer odl-dluxapps-yangman 我有必要把这些feature再次列出来 12345678feature:install odl-restconffeature:install odl-l2switch-switch feature:install odl-dlux-core feature:install odl-dluxapps-nodes feature:install odl-dluxapps-topology feature:install odl-dluxapps-yangui feature:install odl-dluxapps-yangvisualizer feature:install odl-dluxapps-yangman 务必按照顺序安装，如果出现错误，删了目录重新来过吧 登陆Web 使用admin:admin登陆即可"},{"title":"Mininet基础使用","permalink":"http://blog.simble.xyz/post/46d30180.html","text":"Mininet是什么 Mininet是一个网络模拟器，可以创建虚拟主机，交换机，控制器和链接的网络。 Mininet的交换机支持OpenFlow，可实现高度灵活的自定义路由和SDN，为开发和测试SDN提供实验环境 Mininet用途 为开发OpenFlow应用程序提供简单而廉价的网络测试平台 允许多个并发开发人员在同一拓扑上独立工作 支持系统级回归测试，这些测试可重复且易于打包 支持复杂的拓扑测试，无需连接物理网络 包括具有拓扑感知和OpenFlow感知的CLI，用于调试或运行网络范围的测试 支持任意自定义拓扑，并包括一组基本的参数化拓扑 可以在没有编程的情况下开箱即用 提供了一个简单易用的**Python API，**用于网络创建和实验 安装 Ubuntu16.04 1apt install mininet 测试 1sudo mn --test pingall 使用 常用命令 进入mininet命令行模式 12345678910111213141516171819root@mininet:~# mn*** No default OpenFlow controller found for default switch!*** Falling back to OVS Bridge*** Creating network*** Adding controller*** Adding hosts:h1 h2*** Adding switches:s1*** Adding links:(h1, s1) (h2, s1)*** Configuring hostsh1 h2*** Starting controller*** Starting 1 switchess1 ...*** Starting CLI:mininet&gt; 查看节点 1234mininet&gt; nodesavailable nodes are:h1 h2 s1mininet&gt; 可以看到当前包含3个节点，包括两个host，一个switch 查看连接状况 12345mininet&gt; neth1 h1-eth0:s1-eth1h2 h2-eth0:s1-eth2s1 lo: s1-eth1:h1-eth0 s1-eth2:h2-eth0mininet&gt; h1的eth0与s1的eth1相连 h2的eth0与s1的eth2相连 查看详细信息 12345mininet&gt; dump&lt;Host h1: h1-eth0:10.0.0.1 pid=6896&gt;&lt;Host h2: h2-eth0:10.0.0.2 pid=6899&gt;&lt;OVSBridge s1: lo:127.0.0.1,s1-eth1:None,s1-eth2:None pid=6905&gt;mininet&gt; 环境清理 123456789101112131415161718192021222324# mn -c*** Removing excess controllers/ofprotocols/ofdatapaths/pings/noxeskillall controller ofprotocol ofdatapath ping nox_core lt-nox_core ovs-openflowd ovs-controller udpbwtest mnexec ivs 2&gt; /dev/nullkillall -9 controller ofprotocol ofdatapath ping nox_core lt-nox_core ovs-openflowd ovs-controller udpbwtest mnexec ivs 2&gt; /dev/nullpkill -9 -f \"sudo mnexec\"*** Removing junk from /tmprm -f /tmp/vconn* /tmp/vlogs* /tmp/*.out /tmp/*.log*** Removing old X11 tunnels*** Removing excess kernel datapathsps ax | egrep -o 'dp[0-9]+' | sed 's/dp/nl:/'*** Removing OVS datapathsovs-vsctl --timeout=1 list-brovs-vsctl --timeout=1 list-br*** Removing all links of the pattern foo-ethXip link show | egrep -o '([-_.[:alnum:]]+-eth[[:digit:]]+)'ip link show*** Killing stale mininet node processespkill -9 -f mininet:*** Shutting down stale tunnelspkill -9 -f Tunnel=Ethernetpkill -9 -f .ssh/mnrm -f ~/.ssh/mn/**** Cleanup complete.root@mininet:~# 节点命令 1234567891011121314151617181920mininet&gt; h1 ifconfigh1-eth0 Link encap:Ethernet HWaddr a2:5f:da:ed:6a:74 inet addr:10.0.0.1 Bcast:10.255.255.255 Mask:255.0.0.0 inet6 addr: fe80::a05f:daff:feed:6a74/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:15 errors:0 dropped:0 overruns:0 frame:0 TX packets:8 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:1206 (1.2 KB) TX bytes:648 (648.0 B)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)mininet&gt; Run http server 1mininet&gt; h1 python -m SimpleHTTPServer 80 &amp; Http client 123456789101112mininet&gt; h2 wget h1--2018-11-07 10:52:29-- http://10.0.0.1/Connecting to 10.0.0.1:80... connected.HTTP request sent, awaiting response... 200 OKLength: 370 [text/html]Saving to: 'index.html'index.html 100%[===================&gt;] 370 --.-KB/s in 0s2018-11-07 10:52:29 (62.3 MB/s) - 'index.html' saved [370/370]mininet&gt;"},{"title":"OpenvSwitch学习笔记1","permalink":"http://blog.simble.xyz/post/775edb62.html","text":"功能支持 VLAN with trunk and access ports 绑定NIC(with/without LAC) 可见性：NetFlow, sFlow®, and mirroring QoS Geneve, GRE, VXLAN, STT, and LISP 隧道 连接故障管理 openflow 1.0及其扩展 使用Linux内核进行高性能转发 主要组成部分 ovs-vswitchd: 实现交换功能的守护进程，与Linux内核模块实现flow-based switching ovsdb-server: 用以保存ovs配置信息的轻量级的数据库 ovs-dpctl: 用以配置交换机内核模块的工具 ovs-vsctl: 查看和更新ovs配置信息的工具 ovs-ofctl: 配置和查看OpenFlow的控制和交换。主要用来操作OpenFlow流表 场景 多服务器虚拟化部署场景 高动态的end-points 维护的是逻辑抽象 状态迁移 响应网络动态修改 维护逻辑标签 包处理流程 如上图 当包被从物理网卡上收到之后，如果是第一次收到包，ovs的kernel datapath不知道该如何处理，于是，将其送往ovs-vswitchd。 ovs-vswitchd决定这个包该如何处理之后，回送到kernel datapath kernel datapath根据ovs-vswitchd执行相应的动作，并缓存这个动作 当再次收到相关包之后，kernel datapath已经存在之前缓存好的动作，则直接执行该动作 包处理流程 因为Flow table在内核中有一份，当从物理网卡收到包后，通过key查找内核中的flow table，即可以得到action，然后执行action 如果没有查找到，则通过upcall调用，将数据包以netlink协议上传到vswitchd vswitchd将数据包在ovsdb中进行查表匹配，若能匹配，则转到第五步 若不能匹配，则通过 OpenFlow协议与控制器通信，控制器下发流表项，Vswitchd解析流表项得到相应的动作，同时将流表存入ovsdb。 将匹配的流表项通过netlink下发到内核的Flow-table中 通过reinject，使用netlink将包重新送回内核 匹配流表项并根据相应的动作执行"},{"title":"Hexo开启disqus评论系统","permalink":"http://blog.simble.xyz/post/c4780247.html","text":"之前一直使用Hexo的Next主题，开启的是Valine评论系统。有博文来说明配置过程。然而，一个主题再好，看的时间久了，还是想要换换。于是使用了现在的MaterialFlow，刚好也换换评论系统 相比Valine，disqus开启真的是太简单了 注册disqus账号 其实我很早之前就注册了账号，直接使用facebook关联过去的. 网站上开启 登陆disqus后，在首页点击右侧设置-Add Disqus To Site 在随后弹出的页面点击Start - I want to install Disqus on my site 配置Hexo 修改hexo的_config.yml，增加disqus_shortname 1disqus_shortname: your_shortname 大功告成 欢迎大家留言"},{"title":"openstack接口状态异常","permalink":"http://blog.simble.xyz/post/257b0787.html","text":"错误信息 neutron错误日志Exit code: 2; Stdin: ; Stdout: ; Stderr: sudo: unable to resolve host pxea4badb2 问题现象 openstack的router中创建了一个联通两个vlan的路由器，发现两个接口轮流处于build状态 查看日志 /var/log/neutron/neutron-linuxbridge-agent.log 1234567891011121314151617181920212223242526115265 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent File \"/usr/lib/python2.7/dist-packages/oslo_utils/excutils.py\", line 196, in force_reraise115266 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent six.reraise(self.type_, self.value, self.tb)115267 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent File \"/usr/lib/python2.7/dist-packages/neutron/plugins/ml2/drivers/linuxbridge/agent/linuxbridge_neut ron_agent.py\", line 419, in add_tap_interface115268 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent tap_device_name, device_owner)115269 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent File \"/usr/lib/python2.7/dist-packages/neutron/plugins/ml2/drivers/linuxbridge/agent/linuxbridge_neut ron_agent.py\", line 451, in _add_tap_interface115270 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent segmentation_id)115271 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent File \"/usr/lib/python2.7/dist-packages/neutron/plugins/ml2/drivers/linuxbridge/agent/linuxbridge_neut ron_agent.py\", line 403, in ensure_physical_in_bridge115272 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent physical_interface)115273 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent File \"/usr/lib/python2.7/dist-packages/neutron/plugins/ml2/drivers/linuxbridge/agent/linuxbridge_neut ron_agent.py\", line 221, in ensure_flat_bridge115274 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent gateway):115275 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent File \"/usr/lib/python2.7/dist-packages/neutron/plugins/ml2/drivers/linuxbridge/agent/linuxbridge_neut ron_agent.py\", line 362, in ensure_bridge115276 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent self.update_interface_ip_details(bridge_name, interface, ips, gateway)115277 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent File \"/usr/lib/python2.7/dist-packages/neutron/plugins/ml2/drivers/linuxbridge/agent/linuxbridge_neut ron_agent.py\", line 301, in update_interface_ip_details115278 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent dst_device.addr.add(cidr=ip['cidr'])115279 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent File \"/usr/lib/python2.7/dist-packages/neutron/agent/linux/ip_lib.py\", line 597, in add115280 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent self._as_root([net.version], tuple(args))115281 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent File \"/usr/lib/python2.7/dist-packages/neutron/agent/linux/ip_lib.py\", line 384, in _as_root115282 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent use_root_namespace=use_root_namespace)115283 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent File \"/usr/lib/python2.7/dist-packages/neutron/agent/linux/ip_lib.py\", line 96, in _as_root115284 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent log_fail_as_error=self.log_fail_as_error)115285 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent File \"/usr/lib/python2.7/dist-packages/neutron/agent/linux/ip_lib.py\", line 105, in _execute115286 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent log_fail_as_error=log_fail_as_error)115287 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent File \"/usr/lib/python2.7/dist-packages/neutron/agent/linux/utils.py\", line 146, in execute115288 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent raise ProcessExecutionError(msg, returncode=returncode)115289 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent ProcessExecutionError: Exit code: 2; Stdin: ; Stdout: ; Stderr: sudo: unable to resolve host pxea4badb2 17413115290 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent RTNETLINK answers: File exists 问题原因 sudo: unable to resolve host pxea4badb2 无法解析主机名 查看/etc/hosts中缺少了本地hostname的IP地址映射 Exit code: 2; Stdin: ; Stdout: ; Stderr: 问题比较不直观，google之后，有人说是因为bridge上配置了IP，导致冲突了 经查看controller上，发现一个bridge和一个无力网卡配置了同样的IP，可能是因为之前使用flat网络后切换为vlan网络，没有清理openstack中创建的配置导致。 将bridge上的IP地址删除后，恢复正常"},{"title":"Ubuntu在Openstack中启动密码注入不生效问题定位1","permalink":"http://blog.simble.xyz/post/c2a7a06a.html","text":"问题说明 使用Ubuntu的cloud镜像创建实例，在创建时使用脚本修改密码 12345#!/bin/bashpasswd ubuntu&lt;&lt;EOFubuntuubuntuEOF 系统启动后，发现密码修改未成功 定位思路 查看实例启动日志 ➡️ 查看openstack的neutron agent状态 ➡️ 查看meta-data服务日志 step by step 查看实例启动日志 123[ 731.435163] cloud-init[845]: 2018-09-11 01:42:24,574 - url_helper.py[WARNING]: Calling 'http://169.254.169.254/2009-04-04/meta-data/instance-id' failed [119/120s]: request error [HTTPConnectionPool(host='169.254.169.254', port=80): Max retries exceeded with url: /2009-04-04/meta-data/instance-id (Caused by NewConnectionError('&lt;requests.packages.urllib3.connection.HTTPConnection object at 0x7fa26f1eb6d8&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',))][ 738.460169] cloud-init[845]: 2018-09-11 01:42:31,597 - DataSourceEc2.py[CRITICAL]: Giving up on md from ['http://169.254.169.254/2009-04-04/meta-data/instance-id'] after 126 seconds[ 738.470572] cloud-init[845]: 2018-09-11 01:42:31,608 - util.py[WARNING]: Getting data from &lt;class 'cloudinit.sources.DataSourceCloudStack.DataSourceCloudStack'&gt; failed 以上日志说明cloud-init在向meta-data server获取meta-data时失败 查看neutron agent状态 123456789101112root@pxea4badb217413:~# neutron agent-list+--------------------------------------+--------------------+-----------------+-------------------+-------+----------------+---------------------------+| id | agent_type | host | availability_zone | alive | admin_state_up | binary |+--------------------------------------+--------------------+-----------------+-------------------+-------+----------------+---------------------------+| 459740a7-b60c-4db7-9ee0-3b45d6c3b2de | Linux bridge agent | pxe1418773526e5 | | :-) | True | neutron-linuxbridge-agent || 4ec74c6a-c392-4216-bddd-789ec5aa3d86 | DHCP agent | pxea4badb217413 | nova | :-) | True | neutron-dhcp-agent || 6d992669-fbe6-412c-a717-5c2b61a2b901 | Metadata agent | pxea4badb217413 | | :-) | True | neutron-metadata-agent || 77dc04a5-0e43-4907-be3a-6f165e77807c | Linux bridge agent | pxe74867aee16bc | | :-) | True | neutron-linuxbridge-agent || c54c7593-5973-41ed-86c8-7e22ac1e95ec | Linux bridge agent | pxea4badb217413 | | :-) | True | neutron-linuxbridge-agent || e882e27d-45de-4cb3-9284-7e9101e2b39b | L3 agent | pxea4badb217413 | nova | :-) | True | neutron-l3-agent |+--------------------------------------+--------------------+-----------------+-------------------+-------+----------------+---------------------------+root@pxea4badb217413:~# 检查neutron-metadata日志 123456789101112131415161718192021222018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent [-] Failed reporting state!2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent Traceback (most recent call last):2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent File \"/usr/lib/python2.7/dist-packages/neutron/agent/metadata/agent.py\", line 266, in _report_state2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent use_call=self.agent_state.get('start_flag'))2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent File \"/usr/lib/python2.7/dist-packages/neutron/agent/rpc.py\", line 87, in report_state2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent return method(context, 'report_state', **kwargs)2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent File \"/usr/lib/python2.7/dist-packages/oslo_messaging/rpc/client.py\", line 158, in call2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent retry=self.retry)2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent File \"/usr/lib/python2.7/dist-packages/oslo_messaging/transport.py\", line 90, in _send2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent timeout=timeout, retry=retry)2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent File \"/usr/lib/python2.7/dist-packages/oslo_messaging/_drivers/amqpdriver.py\", line 470, in send2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent retry=retry)2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent File \"/usr/lib/python2.7/dist-packages/oslo_messaging/_drivers/amqpdriver.py\", line 459, in _send2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent result = self._waiter.wait(msg_id, timeout)2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent File \"/usr/lib/python2.7/dist-packages/oslo_messaging/_drivers/amqpdriver.py\", line 342, in wait2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent message = self.waiters.get(msg_id, timeout=timeout)2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent File \"/usr/lib/python2.7/dist-packages/oslo_messaging/_drivers/amqpdriver.py\", line 244, in get2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent 'to message ID %s' % msg_id)2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent MessagingTimeout: Timed out waiting for a reply to message ID 5b145001c0a34d37a68337821e64908f2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent2018-09-05 19:07:06.405 10617 WARNING oslo.service.loopingcall [-] Function 'neutron.agent.metadata.agent.UnixDomainMetadataProxy._report_state' run outlasted interval by 30.00 sec 12342018-09-06 16:23:48.658 1238 ERROR oslo.messaging._drivers.impl_rabbit [-] AMQP server on controller:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 1 seconds.2018-09-06 16:23:49.752 1238 ERROR oslo.messaging._drivers.impl_rabbit [-] AMQP server on controller:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 2 seconds.2018-09-06 16:23:51.766 1238 ERROR oslo.messaging._drivers.impl_rabbit [-] AMQP server on controller:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 4 seconds.2018-09-06 16:23:55.779 1238 ERROR oslo.messaging._drivers.impl_rabbit [-] AMQP server on controller:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 6 seconds. 以上显示为AMQP server连接不到 检查meta-data的配置 123456789 /etc/neutron/metadata_agent.ini # IP address used by Nova metadata server. (string value)#nova_metadata_ip = 127.0.0.1nova_metadata_ip = controller# TCP Port used by Nova metadata server. (port value)# Minimum value: 0# Maximum value: 65535#nova_metadata_port = 8775 查看controller上端口监听状况 1234567891011121314151617181920212223242526272829303132root@pxea4badb217413:~# netstat -lnpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1149/sshdtcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN 2316/exim4tcp 0 0 127.0.0.1:6010 0.0.0.0:* LISTEN 18284/12tcp 0 0 127.0.0.1:6011 0.0.0.0:* LISTEN 18861/20tcp 0 0 0.0.0.0:9696 0.0.0.0:* LISTEN 1243/pythontcp 0 0 0.0.0.0:6080 0.0.0.0:* LISTEN 1247/pythontcp 0 0 0.0.0.0:8774 0.0.0.0:* LISTEN 1237/pythontcp 0 0 0.0.0.0:8775 0.0.0.0:* LISTEN 1237/pythontcp 0 0 0.0.0.0:9191 0.0.0.0:* LISTEN 1242/pythontcp 0 0 0.0.0.0:25672 0.0.0.0:* LISTEN 2922/beam.smptcp 0 0 0.0.0.0:8776 0.0.0.0:* LISTEN 1235/pythontcp 0 0 10.160.17.196:3306 0.0.0.0:* LISTEN 1769/mysqldtcp 0 0 10.160.17.196:11211 0.0.0.0:* LISTEN 2443/memcachedtcp 0 0 0.0.0.0:9292 0.0.0.0:* LISTEN 1246/pythontcp6 0 0 :::21 :::* LISTEN 833/vsftpdtcp6 0 0 :::22 :::* LISTEN 1149/sshdtcp6 0 0 ::1:25 :::* LISTEN 2316/exim4tcp6 0 0 ::1:6010 :::* LISTEN 18284/12tcp6 0 0 ::1:6011 :::* LISTEN 18861/20tcp6 0 0 :::35357 :::* LISTEN 3904/apache2tcp6 0 0 :::5000 :::* LISTEN 3904/apache2tcp6 0 0 :::80 :::* LISTEN 3904/apache2tcp6 0 0 :::8081 :::* LISTEN 4749/cmatcp6 0 0 :::4369 :::* LISTEN 2586/epmdudp 0 0 0.0.0.0:123 0.0.0.0:* 3218/chronydudp 0 0 0.0.0.0:161 0.0.0.0:* 3189/snmpdudp 0 0 0.0.0.0:323 0.0.0.0:* 3218/chronydudp6 0 0 :::123 :::* 3218/chronydudp6 0 0 :::323 :::* 3218/chronyd 以上，发现5762端口未监听 尝试启动rabbitmq-server 123service rabbitmq-server start * Starting RabbitMQ Messaging Server rabbitmq-server * RabbitMQ Messaging Server already running [ OK ] 再次查看端口监听状态 1tcp6 0 0 :::5672 :::* LISTEN 2922/beam.smp 重启meta-data的service，发现日志已经不再报错了 然而，这竟然不是最终原因，最终的问题发现是网络问题"},{"title":"sqlalchemy连接docker的mysql问题记录","permalink":"http://blog.simble.xyz/post/6e2fcf3d.html","text":"又开始鼓捣flask+mysql了 👶 不成想，又一次踩了好多雷 😂 有一种打怪升级的感觉 环境说明 Mac+flask docker+mysql 问题1 create_engine报错 错误信息 1234567891011&gt;&gt;&gt; from sqlalchemy import create_engine&gt;&gt;&gt; engine = create_engine('mysql+mysqldb://root:123456@localhost/beta_monitor')Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; File \"/Users/bqi/venv/beta-monitor/lib/python2.7/site-packages/sqlalchemy/engine/__init__.py\", line 424, in create_engine return strategy.create(*args, **kwargs) File \"/Users/bqi/venv/beta-monitor/lib/python2.7/site-packages/sqlalchemy/engine/strategies.py\", line 81, in create dbapi = dialect_cls.dbapi(**dbapi_args) File \"/Users/bqi/venv/beta-monitor/lib/python2.7/site-packages/sqlalchemy/dialects/mysql/mysqldb.py\", line 102, in dbapi return __import__('MySQLdb')ImportError: No module named MySQLdb 问题原因 没有安装python的mysql包，需要安装mysql-python和/或mysqlclient，然后就遇到了第二个问题 问题2 pip install mysql-python失败 错误信息 123456789101112131415161718⇒ pip install mysql-pythonCollecting mysql-python Downloading https://files.pythonhosted.org/packages/a5/e9/51b544da85a36a68debe7a7091f068d802fc515a3a202652828c73453cad/MySQL-python-1.2.5.zip (108kB) 100% |████████████████████████████████| 112kB 153kB/s Complete output from command python setup.py egg_info: sh: mysql_config: command not found Traceback (most recent call last): File \"&lt;string&gt;\", line 1, in &lt;module&gt; File \"/private/var/folders/3z/tqw46wwj7xb1d2ftp578x5vm0000gn/T/pip-install-ViJMnc/mysql-python/setup.py\", line 17, in &lt;module&gt; metadata, options = get_config() File \"setup_posix.py\", line 43, in get_config libs = mysql_config(\"libs_r\") File \"setup_posix.py\", line 25, in mysql_config raise EnvironmentError(\"%s not found\" % (mysql_config.path,)) EnvironmentError: mysql_config not found ----------------------------------------Command \"python setup.py egg_info\" failed with error code 1 in /private/var/folders/3z/tqw46wwj7xb1d2ftp578x5vm0000gn/T/pip-install-ViJMnc/mysql-python/ 问题原因 mysql_config不存在，原来是系统必须安装mysql客户端 解决方法 brew install mysql 123456⇒ brew install mysqlUpdating Homebrew...==&gt; Downloading https://homebrew.bintray.com/bottles/mysql-5.7.22.high_sierra.bottle.tar.gz######################################################################## 100.0%==&gt; Pouring mysql-5.7.22.high_sierra.bottle.tar.gz... 问题3 连接mysql服务器失败 错误信息 Authentication plugin 'caching_sha2_password' cannot be loaded 1234567891011121314&gt;&gt;&gt; engine = create_engine('mysql+mysqldb://root:123456@127.0.0.1/beta_monitor')&gt;&gt;&gt; connection = engine.connect()Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; File \"/Users/bqi/venv/beta-monitor/lib/python2.7/site-packages/sqlalchemy/engine/base.py\", line 2102, in connect return self._connection_cls(self, **kwargs)... File \"/Users/bqi/venv/beta-monitor/lib/python2.7/site-packages/MySQLdb/__init__.py\", line 81, in Connect return Connection(*args, **kwargs) File \"/Users/bqi/venv/beta-monitor/lib/python2.7/site-packages/MySQLdb/connections.py\", line 193, in __init__ super(Connection, self).__init__(*args, **kwargs2)sqlalchemy.exc.OperationalError: (_mysql_exceptions.OperationalError) (2059, \"Authentication plugin 'caching_sha2_password' cannot be loaded: dlopen(/usr/local/Cellar/mysql/5.7.22/lib/plugin/caching_sha2_password.so, 2): image not found\") (Background on this error at: http://sqlalche.me/e/e3q8) 问题原因 上网查了一通，似乎说从某个版本开始，mysql用了一种认证方式导致问题。根据解决方法看，更换了认证方式就可以了 解决方法 用docker启动mysql时增加参数 --default-authentication-plugin=mysql_native_password 12~|⇒ docker run -p 3306:3306 -d -e MYSQL_ROOT_PASSWORD=123456 -e MYSQL_DATABASE=beta_monitor -e MYSQL_USER=test -e MYSQL_PASSWORD=123456 mysql --default-authentication-plugin=mysql_native_passwordd8f5e623dc595df19b9d6cce52780381b625c1565622f5867f2ad3aeafdca499 如果安装在服务器上，则在my.cnf中修改相关配置 测试 本地连接测试 123456789101112131415161718~|⇒ mysql -utest -p123456 -h 127.0.0.1mysql: [Warning] Using a password on the command line interface can be insecure.ERROR 2013 (HY000): Lost connection to MySQL server at &apos;reading initial communication packet&apos;, system error: 0~|⇒ mysql -utest -p -h 127.0.0.1Enter password:Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 8Server version: 8.0.12 MySQL Community Server - GPLCopyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\\h&apos; for help. Type &apos;\\c&apos; to clear the current input statement.mysql&gt; quit 通过connection = engine.connect()无异常 作为一个暗夜精灵玩家，强烈谴责希尔瓦纳斯烧了我老家的卑劣行径，暴雪怎么洗也没用"},{"title":"美签记录","permalink":"http://blog.simble.xyz/post/fbe9394a.html","text":"记第一次拿到十年美签 时间 提交DS160: 2018-07-08 预约面签时间: 2018-07-27 前面排队人太多 Issued: 2018-07-30 护照已从领事馆那边收回，目前正在安排运送: 2018-07-30 收到护照: 2018-08-02 持续时间 面签持续时间：1分钟不到 AP持续时间：2个工作日 Issued到送到苏州：3个工作日 面签问题 你去美国干什么？ 要去美国多久？ 你是做什么工作的？ 你们公司是做什么的？ 你是一个人去么？ “你通过了！”"},{"title":"Openstack学习笔记1","permalink":"http://blog.simble.xyz/post/9b5fb1d1.html","text":"该系列用来记录本人使用Openstack的一些笔记和心得 Openstack一些理解 服务说明 keystone - 必选 认证服务，是其他所有服务的基础 glance - 必选 镜像服务，用来存储镜像文件，如iso, vmdk, qcow2等 nova - 必选 计算服务，包括计算，调度，管理，api等，是openstack的核心服务 neutron - 必选 网络服务 horizon - 可选（推荐选择） 管理界面，图形化管理界面，方便使用 cinder - 可选 块存储服务，用来创建虚拟机的磁盘 一些理解 openstack可以理解为插件型的，灵活就体现于此。 计算服务，控制节点可以只做计算的调度，管理，也可以在控制节点上启动计算服务 同样，可以在任意一个节点上起块存储服务 关于网络 刚开始的时候很疑惑，为什么每个计算节点上都要配置一个额外的网卡，并将其连在一起 需要说明的是： 虚拟机的第一个网口是openstack的各个节点用来通信的网卡 第二个网口是用来实现在openstack上启动的实例彼此通信的（即东西向流量） 也可以不用第二个网口，这时，两个实例可以通过基于第一个网口之间建立的隧道进行通信 如果要让实例可以访问外网，则需要为其分配专门的访问外网的网口 openstack并不直接管理网口或网桥，要么通过linux bridge，或者是openvSwitch 在openstack的UI上看到的网口，都是代号 测试环境准备 两台虚拟机（当然也可以all in one） 硬件准备 每台虚拟机应包含 虚机可上网 另外包含一张额外的网卡 内存尽量多（8G） CPU尽量多（8 cpus） controller硬盘稍微大一些 基本准备 Ubuntu16.04 + Q版本（Queens） 配置NTP 安装Openstack基础包 准备仓库 12# apt install software-properties-common# add-apt-repository cloud-archive:queens 安装openstack包 123$ apt update$ apt upgrade #经实际测试，如果用apt dist-upgrade可能会出现问题，安装的不是最新的包$ apt install python-openstackclient controller controller上需要安装的东西最多 依赖 数据库: MariaDB 消息队列: RabbitMQ 服务及身份认证: Memcached etcd openstack服务 认证服务: keystone 镜像服务: glance 计算服务（指计算的调度，管理，api等服务）: nova 网络服务（网络管理，调度等服务）: neutron 管理界面: horizon 块存储服务（可选）: cinder compute 计算节点相对来说简单了很多 openstack服务 计算服务（专指计算服务）: nova-compute 网络服务: neutron 可在controller上同样起计算服务 详细安装过程可参看官方文档"},{"title":"使用markdown写PPT","permalink":"http://blog.simble.xyz/post/4f712e63.html","text":"自打接触Markdown以来，深深的爱上了这种格式的书写，先是弄网站，然后又弄电子书。最近要给新员工培训，要准备4个PPT，于是就想有没有直接用md来写PPT的呢？上网一搜，还真有。 有Reveal.js, LandSlide, GitPitch等，但也许是我研究的不深吧，gitpitch是在线的，LandSlide要用pip来安装。Reveal.js倒是之前见team里的大神有写过，但现在好像变成了slides了，也是在线的。最终，主角出场，我选择了Marp。 优点 书写简单 提供两套模版 可实时预览 只需要一个md文件即可，不需要其他诸如yaml之类的东西 支持Mac，Win和Linux 缺点 没有动画 只能导出为pdf格式，不能生成PPT格式 仅支持基础的markdown语法 虽然有着上面的缺点，然而，对于写技术类PPT，我觉得够用了 用法 Marp提供两套主题，都说专治选择障碍。特别是默认主题，简直是选择障碍患者的福音。 下载完安装后，两个例子中已经有比较基本的说明了。但有一些东西还是很容易被忽略。 设置页码 页码是可以随时开启，随时关闭的。 1&lt;!-- page_number: true --&gt; 在任意页输入以上代码就可以开启，但如果你要在哪页关闭显示页码，可以用false 背景设置 因为只有两套模版，所以，想要好看点，还是需要一些背景图片的。刚开始我只知道用![bg](aaa.png)的方式引入背景，但后来发现背景图片其实是和背景主题叠加了的。 后来细细看了文档才发现，原来可以用开关开启，还可以设置背景的大小 1![bg original 70%](aaa.png) 使用了original之后，背景就变成了纯图片，而70%则可以指定图片大小。 当然，更有趣的是，可以用多个背景并排的方式来完成部分植入。 比如： 则是用如下代码完成 123456![bg 450% original](robot2.png)![bg]()![bg]()![bg]()![bg]()![bg 450% original](robot2.png) 还有一个有意思的是，如果不设置图片的比例，那么会按照扩充满整个屏幕来设置，但如果设置比例，如上面的代码，则100%是整个页面宽度/6之后的图片大小。 当然，灵活运用这一个特性，也能带来很多意想不到的效果。 emoji 值得一提的是Marp的emoji表情选择的很是我喜欢的那种，而且可以按照段落来调整大小 比如： 1234# :cat:## :fish:### :tiger:#### :bird: 表情的大小会随着段落比较而变化。还是很不错的。 另外，似乎软件的作者正在开发新的软件，很是期待 🐱"},{"title":"Ubuntu16.04安装Open vSwitch","permalink":"http://blog.simble.xyz/post/b2c6ed30.html","text":"环境准备 获取安装包 1git clone https://github.com/openvswitch/ovs.git 安装必要的依赖 1apt install autoconf libtool make libssl-dev libcap-ng-dev 安装 当使用源代码时，需要自己创建configure脚本 1$ ./boot.sh 配置并开启内核模块 1$ ./configure --prefix=/usr --localstatedir=/var --sysconfdir=/etc --with-linux=/lib/modules/$(uname -r)/build 安装 12$ make$ make install 安装内核模块 该步骤可能会报错，请参看后文 1$ make modules_install 官方安装文档中提到，你有可能之前已经安装了ovs的模块，为了确保使用的是你刚才编译的，则需要在/etc/depmod.d/中添加如下内容 1234567$ config_file=&quot;/etc/depmod.d/openvswitch.conf&quot;$ for module in datapath/linux/*.ko; do modname=&quot;$(basename $&#123;module&#125;)&quot; echo &quot;override $&#123;modname%.ko&#125; * extra&quot; &gt;&gt; &quot;$config_file&quot; echo &quot;override $&#123;modname%.ko&#125; * weak-updates&quot; &gt;&gt; &quot;$config_file&quot; done$ depmod -a 加载内核模块 1$ /sbin/modprobe openvswitch 验证 123456789$ /sbin/lsmod | grep openvswitchopenvswitch 303104 0tunnel6 16384 1 openvswitchnf_nat_ipv6 16384 1 openvswitchnf_defrag_ipv6 36864 2 openvswitch,nf_conntrack_ipv6nf_nat_ipv4 16384 2 openvswitch,iptable_natnf_nat 28672 6 nf_nat_redirect,openvswitch,nf_nat_ipv4,nf_nat_ipv6,xt_nat,nf_nat_masquerade_ipv4nf_conntrack 106496 11 xt_CT,openvswitch,nf_nat,nf_nat_ipv4,nf_nat_ipv6,xt_conntrack,nf_nat_masquerade_ipv4,nf_conntrack_netlink,xt_connmark,nf_conntrack_ipv4,nf_conntrack_ipv6libcrc32c 16384 2 raid456,openvswitch 启动服务 官方说明中提到有一个ovs-ctl的命令，然而，我安装完之后并没有这个命令。可能官方文档比较老了吧。 创建必要的目录 123$ mkdir -p /etc/openvswitch$ mkdir -p /var/log/openvswitch$ mkdir -p /var/run/openvswitch 配置ovsdb-server 从源文件目录创建conf.db 12ovs$ ovsdb-tool create /etc/openvswitch/conf.db \\ vswitchd/vswitch.ovsschema 配置 123456ovsdb-server --remote=punix:/var/run/openvswitch/db.sock \\ --remote=db:Open_vSwitch,Open_vSwitch,manager_options \\ --private-key=db:Open_vSwitch,SSL,private_key \\ --certificate=db:Open_vSwitch,SSL,certificate \\ --bootstrap-ca-cert=db:Open_vSwitch,SSL,ca_cert \\ --pidfile --detach --log-file 初始化数据库 1ovs-vsctl --no-wait init 启动ovs进程 1ovs-vswitchd --pidfile --detach --log-file 验证 创建bridge 1$ ovs-vsctl add-br br0 为br0添加接口 1$ ovs-vsctl add-port br0 eth1 查看配置 1$ ovs-vsctl show 问题及解决 安装内核模块时出现如下错误，忽略即可（我还以为很严重，搜了一阵，发现不用管） 123456789101112131415161718192021222324252627282930313233343536373839$ make modules_installcd datapath/linux &amp;&amp; make modules_installmake[1]: Entering directory &apos;/root/ovs/datapath/linux&apos;make -C /lib/modules/4.4.0-127-generic/build M=/root/ovs/datapath/linux modules_installmake[2]: Entering directory &apos;/usr/src/linux-headers-4.4.0-127-generic&apos; INSTALL /root/ovs/datapath/linux/openvswitch.koAt main.c:222:- SSL error:02001002:system library:fopen:No such file or directory: bss_file.c:175- SSL error:2006D080:BIO routines:BIO_new_file:no such file: bss_file.c:178sign-file: certs/signing_key.pem: No such file or directory INSTALL /root/ovs/datapath/linux/vport-geneve.koAt main.c:222:- SSL error:02001002:system library:fopen:No such file or directory: bss_file.c:175- SSL error:2006D080:BIO routines:BIO_new_file:no such file: bss_file.c:178sign-file: certs/signing_key.pem: No such file or directory INSTALL /root/ovs/datapath/linux/vport-gre.koAt main.c:222:- SSL error:02001002:system library:fopen:No such file or directory: bss_file.c:175- SSL error:2006D080:BIO routines:BIO_new_file:no such file: bss_file.c:178sign-file: certs/signing_key.pem: No such file or directory INSTALL /root/ovs/datapath/linux/vport-lisp.koAt main.c:222:- SSL error:02001002:system library:fopen:No such file or directory: bss_file.c:175- SSL error:2006D080:BIO routines:BIO_new_file:no such file: bss_file.c:178sign-file: certs/signing_key.pem: No such file or directory INSTALL /root/ovs/datapath/linux/vport-stt.koAt main.c:222:- SSL error:02001002:system library:fopen:No such file or directory: bss_file.c:175- SSL error:2006D080:BIO routines:BIO_new_file:no such file: bss_file.c:178sign-file: certs/signing_key.pem: No such file or directory INSTALL /root/ovs/datapath/linux/vport-vxlan.koAt main.c:222:- SSL error:02001002:system library:fopen:No such file or directory: bss_file.c:175- SSL error:2006D080:BIO routines:BIO_new_file:no such file: bss_file.c:178sign-file: certs/signing_key.pem: No such file or directory DEPMOD 4.4.0-127-genericmake[2]: Leaving directory &apos;/usr/src/linux-headers-4.4.0-127-generic&apos;depmod `sed -n &apos;s/#define UTS_RELEASE &quot;\\([^&quot;]*\\)&quot;/\\1/p&apos; /lib/modules/4.4.0-127-generic/build/include/generated/utsrelease.h`make[1]: Leaving directory &apos;/root/ovs/datapath/linux&apos;"},{"title":"gitbook+gitlab发布私有的图书仓库","permalink":"http://blog.simble.xyz/post/51748f3c.html","text":"近期带着一些同事学习python，开始用markdown的格式写了很多练习题，一直有想法将其发布成一本电子书在公司内部分享。但由于有些内容可能涉及公司相关，无法直接对外发布。而公司内部自建了一个gitlab服务器，一直用于托管一些不是很重要的代码。所以，便有了将markdown托管到gitlab上，然后使用gitbook在内部进行发布。 另一方面，由于最近痴迷于Docker，所以，也同样将gitbook打包成docker image，用于快速发布。这篇文章主要用于记录整个操作过程。 gitbook+docker 本地安装 可参看gitbook的官方文档 环境需求 gitbook需要nodejs环境 NodeJS (v4.0.0 and above is recommended) 安装gitbook-cli 1npm install gitbook-cli -g 创建一本书 当使用gitlab时，可跳过此步骤 创建存放书籍的目录 1$ mkdir mybook 初始化 1$ gitbook init 初始化完成后，将生成SUMMARY.md和README.md 插件安装 如果没有生成book.json，可自行创建 12345678910111213&#123;&quot;root&quot;: &quot;./&quot;,&quot;title&quot;: &quot;mybook&quot;,&quot;head_title&quot;: &quot;My first book&quot;,&quot;description&quot;: &quot;test with gitbook&quot;,&quot;author&quot;: &quot;myname&quot;,&quot;output.name&quot;: &quot;practice&quot;,&quot;gitbook&quot;: &quot;&gt;= 3.0.0&quot;,&quot;language&quot;: &quot;zh-hans&quot;,&quot;plugins&quot;: [ ] 将插件在plugins字段中进行声明后，执行以下命令进行安装 1$ gitbook install 预览和发布自己的书 1$ gitbook serve 随后可登陆http://localhost:4000来预览自己的书 docker镜像 其实在docker hub上搜索便可以得到gitbook的镜像，但本着学习的态度，还是自己动手练习制作自己的docker镜像。 由于nodejs有官方提供的docker镜像，所以，一切变得很简单 Dockerfile 12345678910FROM node:8-alpineMAINTAINER Bo Qi &lt;simble1986@gmail.com&gt;RUN npm install gitbook-cli -g &amp;&amp; npm install &amp;&amp; gitbook installWORKDIR /bookEXPOSE 4000 35729CMD gitbook install &amp;&amp; gitbook serve 编译docker镜像 1$ docker build --tag mygitbook . 书的结构 当使用gitbook init后会在当前目录生成两个文件：README.md和SUMMARY.md. 其中，README.md用来对本书进行一些说明 而SUMMARY.md则用来创建目录结构。 SUMMARY.md 1234567891011121314151617# Summary* [介绍](README.md)* [Git使用](gitSetup.md)* [Python基础](part1/README.md) * [练习1-列表](part1/1.md) * [练习2-字典](part1/2.md) * [练习3-数据结构嵌套](part1/3.md) * [练习4-运算符](part1/4.md) * [练习5-逻辑控制](part1/5.md) * [练习6-异常处理](part1/6.md) * [练习7-函数](part1/7.md) * [练习8-文件操作](part1/8.md) * [轻松一刻-猜数字游戏](part1/happy1.md) * [练习9-类.1](part1/9.md) * [练习10-类.2](part1/10.md) * [练习11-类的继承](part1/11.md) book.json 需要自己创建book.json 12345678910111213141516171819202122232425262728293031323334353637&#123; \"root\": \"./\", \"title\": \"练习python\", \"head_title\": \"通过练习的方式来学习python\", \"description\": \"通过小练习一点一点熟悉python\", \"author\": \"myname(simble1986@gmail.com)\", \"output.name\": \"通过练习学脚本\", \"gitbook\": \"3.2.3\", \"language\": \"zh-hans\", \"links\" : &#123; \"sidebar\" : &#123; \"Home\" : \"http://www.simble.site\" &#125; &#125;, \"plugins\": [ \"autotheme\", \"prism\", \"prism-themes\", \"-highlight\", \"-search\", \"search-pro\", \"emphasize\", \"splitter\", \"tbfed-pagefooter\", \"toggle-chapters\", \"codeblock-filename\", \"ace\", \"simple-page-toc\", \"edit-link\", \"copy-code-button\", \"alerts\", \"anchor-navigation-ex\", \"theme-comscore\" ]&#125; git-lab归档 在gitlab上创建自己的project并归档 在服务器上用docker启动预览 从gitlab上clone书的结构 启动docker并挂载gitbook的目录到docker中 1$ docker run -d -p 80:4000 -v /mybook:/book mygitbook 插件及说明 非常感谢Zhangjikai的插件说明文档，然后我发现Zhangjikai和我一样使用了Hexo搭建了自己的blog，并且同样适用了Next的主题"},{"title":"python virtenv环境搭建","permalink":"http://blog.simble.xyz/post/4926e490.html","text":"近期由于组内测试框架更新频繁，且由于框架采用了插件形式，安装包很多。当使用同一台服务器来安装时，可能会导致生产环境破坏。为此，必须采用virtenv方式。 virtualenv的环境建立并不复杂，但每次都需要去查一番。特此记录 安装 安装virtualenv python2.7 1pip install virtualenv python3 1pip3 install virtualenv 使用 创建工作目录 12root@vm1:/home/test# mkdir myprojectroot@vm1:/home/test# cd myproject 创建独立的python运行环境 1234root@vm1:/home/test# virtualenv venvNew python executable in /home/test/myproject/venv/bin/pythonInstalling setuptools, pip, wheel...done.root@vm1:/home/test# 引用新的环境变量 12root@vm1:/home/test# source venv/bin/activate(venv)root@vm1:/home/test# 开始使用 1(venv)root@vm1:/home/test# pip install docker 退出当前venv环境 12(venv)root@vm1:/home/test# deactivateroot@vm1:/home/test# 一个小问题 git clone时遇到server certificate verification failed. CAfile: /etc/ssl/certs/ca-certificates.crt CRLfile: none的解决方法 1export GIT_SSL_NO_VERIFY=1"},{"title":"Docker API一种连接到PTY的交互方法","permalink":"http://blog.simble.xyz/post/b18ab8ad.html","text":"折腾了一周多，终于搞定了在docker的python API下，当执行exec_run时，如何连接container的PTY。 当弄明白之后，才发现原来是那么简单。之前几乎搜遍了google和百度，都没有找到相关的文章 前言 这两年，docker的发展如火如荼，作为网络测试，我们也在尝试着将docker引入测试中，来更多的模拟真实用户，并实现自动化。 Pexpect是一个非常强大且好用的工具，当需要与设备和PC连接时，基本上都会用到。而之前都直接使用spawn一个命令行来进行连接 本篇博文将介绍一种使用pexpect的fdspawn，通过socket方式连接到container的方法，以便与远程的container进行交互 自动化思路 client通过python API连接到docker 创建一个container并保持运行 使用exec_run()新建一个连接，运行/bin/bash，并开启socket方式 使用pexpect的fdspawn连接exec_run()返回的socket 环境准备 Docker开启remote API 参见之前博文 Docker Client 安装必要的包 12$ pip install docker$ pip install pexpect 开始使用 创建container 123&gt;&gt;&gt; import docker&gt;&gt;&gt; client=docker.DockerClient(base_url='tcp://10.0.0.10:1234')&gt;&gt;&gt; c1 = client.containers.run(\"ubuntu\", detach=True, tty=True) 连接container 123456&gt;&gt;&gt; res = c1.exec_run(\"/bin/bash\", socket=True, stdin=True, tty=True)&gt;&gt;&gt; resExecResult(exit_code=None, output=&lt;socket object, fd=15, family=1, type=1, protocol=0&gt;)&gt;&gt;&gt; sock = res.output&gt;&gt;&gt; sock&lt;socket object, fd=15, family=1, type=1, protocol=0&gt; 使用pexpect连接 1234567891011&gt;&gt;&gt; import pexpect.fdpexpect&gt;&gt;&gt; session=pexpect.fdpexpect.fdspawn(sock.fileno(),timeout=10)&gt;&gt;&gt; &gt;&gt;&gt; session.send(\"ls\\n\")3&gt;&gt;&gt; session.expect(\"#\")0&gt;&gt;&gt; session.before' ls\\r\\n\\x1b[0m\\x1b[01;34mbin\\x1b[0m \\x1b[01;34mdev\\x1b[0m \\x1b[01;34mhome\\x1b[0m \\x1b[01;34mlib64\\x1b[0m \\x1b[01;34mmnt\\x1b[0m \\x1b[01;34mproc\\x1b[0m \\x1b[01;34mrun\\x1b[0m \\x1b[01;34msrv\\x1b[0m \\x1b[30;42mtmp\\x1b[0m \\x1b[01;34mvar\\x1b[0m\\r\\n\\x1b[01;34mboot\\x1b[0m \\x1b[01;34metc\\x1b[0m \\x1b[01;34mlib\\x1b[0m \\x1b[01;34mmedia\\x1b[0m \\x1b[01;34mopt\\x1b[0m \\x1b[01;34mroot\\x1b[0m \\x1b[01;34msbin\\x1b[0m \\x1b[01;34msys\\x1b[0m \\x1b[01;34musr\\x1b[0m\\r\\n\\x1b]0;root@6a097ddbe55d: /\\x07root@6a097ddbe55d:/'&gt;&gt;&gt; session.after'#'&gt;&gt;&gt; 注意事项 在使用exec_run()执行开启命令时，需要指定stdin=True，否则，pexpect的send()将无法将命令发送至container 同样，在使用exec_run()时，需要指定tty=True，否则，将没有命令行提示符，无法进行匹配"},{"title":"dockerpty使用","permalink":"http://blog.simble.xyz/post/1c49a9aa.html","text":"本篇博客将介绍在使用docker API时，如何监管container的PTY实现交互 问题引入 docker官方已经提供了API用来管理client，container，image，network等，基本的操作覆盖了docker CLI相关功能，但docker的API现在只能使用exec_run来执行一条命令，中间无法进行交互，希望能有一个类似于-it的方式来完成交互操作。 经过几天的学习和测试，发现其实docker的containers.run()和containers.exec_run()都是可以设置stdin=True, tty=True。但开启这些之后，将返回一个socket，需要自己来进行处理。 google大法后，找到了一个dockerpty的python lib，可以完成这件事情 环境搭建 根据dockerpty的github上提到的安装过程，只需要pip install dockerpty即可完成安装。 但源码已经有两年没有更新了，该版本无法在新的docker API上正常工作 fork了工程后，对其中的代码涉及到的docker API进行更新后，测试可以正常工作，最新的代码已经上传到git上simble1986/dockerpty 依赖 原有的project上提到依赖的docker api为docker-py&gt;=0.3.2，但docker的python API已经更新 安装docker API 1pip install docker 安装步骤 获取源码 1git clone https://github.com/simble1986/dockerpty.git 安装 123$ pip uninstall dockerpty$ cd dockerpty$ python setup.py install 相关API 参看docker官方API文档，以下主要对container相关参数加以说明 tty (bool) – Allocate a pseudo-TTY. stdin_open (bool) – Keep STDIN open even if not attached. 基本使用 连接client 1234567root@slt-docker:/home/bqi# pythonPython 2.7.12 (default, Dec 4 2017, 14:50:18)[GCC 5.4.0 20160609] on linux2Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.&gt;&gt;&gt; import docker&gt;&gt;&gt; import dockerpty&gt;&gt;&gt; client=docker.from_env() 注： 支持远程API 创建container 1&gt;&gt;&gt; test1 = client.containers.create(\"ubuntu\",\"/bin/bash\",tty=True,stdin_open=True) 使用dockerpty 123456&gt;&gt;&gt; dockerpty.start(client,test1)root@d6ddcf619602:/# lsbin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr varroot@d6ddcf619602:/# exitexit&gt;&gt;&gt;"},{"title":"Docker中无密码apt安装mysql","permalink":"http://blog.simble.xyz/post/70cf198a.html","text":"问题出现 Linux在第一次安装有些软件时会有交互的输入的需求，比如mysql在首次安装时需要设置root的密码。这在正常配置过程中没什么问题，但在使用DockerFile创建docker镜像时，则遇到了麻烦。 解决思路 如果将安装好的mysql-server使用apt remove从系统中卸载后，再次重新安装，则不再需要输入密码。另外当安装完一些软件后，可以使用dpkg-config来重新配置。 这样，就可以在安装软件前先对系统做好相关配置。接下来，就是需要获取软件的必要配置项 获取软件必要配置项 下载软件包 可以通过网上搜索方式下载相关的deb包，但众所周知，Linux的软件包版本多，很多情况下并不知道需要安装哪个版本。但可以使用apt来下载相应的软件包 1$ apt-get -d install -y mysql-server 使用apt-get的-d参数，将只会下载，不进行安装。下载完毕后，软件包位于/var/cache/apt/archives目录下。 获取配置项 进入软件包的存放目录，然后执行 1dpkg-preconfigure mysql-server-5.1_5.1.49-3_amd64.deb 使用debconfig-show来查看相应的配置项 12345$ debconf-show mysql-server mysql-server/root_password: (password omitted) mysql-server/root_password_again: (password omitted) mysql-server/error_setting_password: ... 可以看到，有两项是必须配置的：mysql-server/root_password和mysql-server/root_password_again 预配置 创建配置文件 在合适的目录下创建一个文件，例如mysql-passwd，输入以下内容 12debconf mysql-server/root_password password 123456debconf mysql-server/root_password_again password 123456 加载配置 1$ debconf-set-selections mysql-passwd 测试结果 再次安装mysql-server，将不再需要输入密码 1$ apt-get install -y mysql-server DockerFile处理 DockerFile应当尽量避免不需要的操作，所以，获取配置项的操作可以提前在实验环境中完成。 可将获取的命令行保存为本地文件，使用时copy过去。 12COPY ./mysql-passwd /tmp/mysql-passwdRUN apt-get update &amp;&amp; debconf-set-selections /tmp/mysql-passwd &amp;&amp; apt-get install -yqq mysql-server &amp;&amp; rm -rf /var/lib/apt/lists/* 或者是在DockerFile中直接生成，这时则可以将mysql passwd设置为参数 12RUN echo debconf mysql-server/root_password password 123456 &gt; /tmp/mysql-passwd &amp;&amp; echo debconf mysql-server/root_password_again password 123456 &gt;&gt; /tmp/mysql-passwdRUN apt-get update &amp;&amp; debconf-set-selections /tmp/mysql-passwd &amp;&amp; apt-get install -yqq mysql-server &amp;&amp; rm -rf /var/lib/apt/lists/*"},{"title":"Hexo+Next上开启Valine评论系统","permalink":"http://blog.simble.xyz/post/a072bce6.html","text":"朋友之前问怎么没有开评论系统，倒不是不想开，而是刚开始建站的时候浏览了好多博文，似乎很多原来的接口都在hexo不太好用了。特别是很多博文都是两年前写的，当然，大多数其他功能都没问题 简介 Valine Valine是一款基于Leancloud的快速、简洁且高效的无后端评论系统。 Leancloud 我的理解，Leancloud相当于是一个数据托管平台，可以帮助应用存储相关数据。Valine主要用到的是其中的数据存储——comments 环境说明 使用了最新版的Hexo以及最新版的Next主题 获取AppID 注册Leancloud 访问Leancloud，点击免费试用就会跳转到注册/登陆页面。当前支持通过Github，Weibo以及QQ进行注册 注册完后需要验证邮箱 创建应用 访问控制台，在控制台中创建新应用 获取应用Key 点击新创建的应用——设置——应用Key，保存页面上的App ID以及App Key以备后续使用 配置Valine 在最新版的Next主题中，已经合入了Valine的配置代码，使得配置起来非常快捷。访问Hexo中使用Valine，点击merged，会跳转到Next主题的merge历史 检查相关文件 可以再次检查并确认主题配置文件_config.xml，layout/_macro/post.swig和layout/_third-party/comments/valine.swig是否都已经合入了相关代码 配置AppKey 编辑主题配置文件_config.xml中的valine部分内容 12345678910valine: enable: true appid: $Your APP ID # your leancloud application appid appkey: $Your APP Key # your leancloud application appkey notify: false # mail notifier , https://github.com/xCss/Valine/wiki verify: false # Verification code placeholder: 随便说些什么吧 # comment box placeholder avatar: mm # gravatar style guest_info: nick,mail,link # custom comment header pageSize: 10 # pagination size 重新生成页面 执行命令，重新生成并部署 1$ hexo clean &amp;&amp; hexo g &amp;&amp; hexo d 到这里，查看页面已经可以看到评论系统 创建留言页面 可以为站点创建一个单独的留言板页面 创建页面 1$ hexo new page guestbook 配置主题 修改主题配置文件 在主题配置文件_config.xml的menu字段新增guestbook字段 12345678910menu: home: / || home tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive #schedule: /schedule/ || calendar #sitemap: /sitemap.xml || sitemap #commonweal: /404/ || heartbeat about: /about/ || user guestbook: /guestbook/ || comments 可以访问fontawesome选择自己喜欢的图标来作为留言板的图标 本地化处理 编辑对应语言的配置文件themes/next/languages/zh-CN.yml，在menu中增加guestbook的中文 1234567891011menu: home: 首页 archives: 归档 categories: 分类 tags: 标签 about: 关于 search: 搜索 schedule: 日程表 sitemap: 站点地图 commonweal: 公益 404 guestbook: 留言 编辑页面 编辑之前生成的guestbook页面 123456---title: 留言板date: 2018-03-26 23:36:19comments: true---&lt;center&gt;既然来了，就是一种缘分，留下点什么吧:cat:&lt;/center&gt; 重新部署之后就可以看到留言板了 😃 清除测试留言 为了确保留言功能已经正常工作，都会测试一下。测试完毕后，可以通过Leancloud的控制台清除测试数据 点击myblog——存储——Comments，即可查看当前留言，选中测试时的留言，删除即可"},{"title":"在Ubuntu 16.04上开启Docker的Remote API","permalink":"http://blog.simble.xyz/post/3e41aa59.html","text":"由于自动化的考虑，需要用docker的remote API，尝试了多种方法，最终才找到了可行的方法 可行的方法 编辑/lib/systemd/system/docker.service 1$ vim /lib/systemd/system/docker.service 修改ExecStart的参数 1ExecStart=/usr/bin/dockerd -H fd:// -H tcp://0.0.0.0:2375 随后执行service docker restart时会提示Warning: docker.service changed on disk. Run 'systemctl daemon-reload' to reload units.则表示配置成功 1systemctl daemon-reload 重启docker服务 1service docker restart 测试是否成功 12$ curl http://localhost:2375/containers/json[&#123;\"Id\":\"30c8e35f1292421d11f6b09385a4fc980d6abaca591d0f52b18dbad8e4f5be04\",\"Names\":[\"/clever_murdock\"],\"Image\":\"portainer/portainer\",\"ImageID\":\"sha256:a8f2aeb34cf69178be1d152759fb17ccff7915faf750c82cd7d1851b12ec7b37\",\"Command\":\"/portainer\",\"Created\":1520845664,\"Ports\":[&#123;\"IP\":\"0.0.0.0\",\"PrivatePort\":9000,\"PublicPort\":9000,\"Type\":\"tcp\"&#125;],\"Labels\":&#123;&#125;,\"State\":\"running\",\"Status\":\"Up 13 minutes\",\"HostConfig\":&#123;\"NetworkMode\":\"default\"&#125;,\"NetworkSettings\":&#123;\"Networks\":&#123;\"bridge\":&#123;\"IPAMConfig\":null,\"Links\":null,\"Aliases\":null,\"NetworkID\":\"78fa057306e70838bab1e18359c86bd8eff7de2285c351784ad951cd7a73f8d1\",\"EndpointID\":\"e99ca98169320155c8833a8746be7d0e1c8d98186c75fba9d9bf2486367a4e00\",\"Gateway\":\"172.17.0.1\",\"IPAddress\":\"172.17.0.2\",\"IPPrefixLen\":16,\"IPv6Gateway\":\"\",\"GlobalIPv6Address\":\"\",\"GlobalIPv6PrefixLen\":0,\"MacAddress\":\"02:42:ac:11:00:02\",\"DriverOpts\":null&#125;&#125;&#125;,\"Mounts\":[&#123;\"Type\":\"bind\",\"Source\":\"/opt/portainer\",\"Destination\":\"/data\",\"Mode\":\"\",\"RW\":true,\"Propagation\":\"rprivate\"&#125;,&#123;\"Type\":\"bind\",\"Source\":\"/var/run/docker.sock\",\"Destination\":\"/var/run/docker.sock\",\"Mode\":\"\",\"RW\":true,\"Propagation\":\"rprivate\"&#125;]&#125;] 不可行的方法 同时列出在Ubuntu上不可行的方法 修改/etc/default/docker中的DOCKER_OPTS 1DOCKER_OPTS='-H fd:// -H tcp://0.0.0.0:2375' 修改/etc/init/docker.conf中的DOCKER_OPTS 网上有人说在Ubuntu14.04上可以生效 123# modify these in /etc/default/$UPSTART_JOB (/etc/default/docker) DOCKERD=/usr/bin/dockerd DOCKER_OPTS='-H unix:///var/run/docker.sock -H tcp://0.0.0.0:2375'"},{"title":"使用hexo搭建自己的博客——创建站点","permalink":"http://blog.simble.xyz/post/80ca9385.html","text":"安装及配置 简介 Hexo是一个快速、简介且高效的博客框架，可以使用Markdown解析文章并生成网站 说明 站点配置文件 $site_dir/_config.xml 主题配置文件$site_dir/theme/$theme_dir/_config 安装hexo 当环境准备好git以及nodejs后便可以安装hexo 1npm install hexo-cli -g 建站 安装Hexo后，创建站点文件存放的文件夹，如blog，然后执行 123$ hexo init blog$ cd blog$ npm install 完成后，blog目录结构 12345678.├── _config.yml├── package.json├── scaffolds├── source| ├── _drafts| └── _posts└── themes 配置 修改_config.yml 123456title: your site tilesubtitle: description: descriptionauthor: your namelanguage: zh-Hanstimezone: Asia/Shanghai 常用命令 清除缓存文件及已生成的静态文件 1$ hexo clean 启动服务器 1$ hexo server 默认状况下，服务器将使用4000端口。可以通过hexo server -p 80来将server绑定至80端口 部署网站 可将网站按照_config.yml中的配置直接部署于github等（后续看心情写步骤） 1$ hexo deploy 主题修改 可访问hexo官方主题库选择自己喜欢的主题，并下载至theme目录下 修改_config.xml中的theme字段 1theme: landscape 写作 有两种方法创建文章 创建文章 hexo命令行方式 1$ hexo new &lt;layout&gt; title layout为模版 命令执行后，默认文章将位于source/_post/目录下，文件内容为： 12345---title: testdate: 2018-03-25 23:10:04tags:--- 直接创建文件 12$ cd source/_post/$ touch myfirstblog.md 此时文件内容为空，需要自己添加相关内容 创建标签页 配置确认 确认站点配置文件中有以下内容 1tag_dir: tags 确认主题配置文件中tags打开 123menu: 主页: / || home 标签: /tags/ || tags 创建标签页 1$ hexo new page tags 修改tags/index.md中的type为&quot;tags&quot; 12345---title: Tagclouddate: 2018-03-23 01:18:00type: \"tags\"--- 创建分类页面 与创建标签页相似 确认配置 站点配置文件中有category_dir: categories 主题配置文件中的分类: /categories/ || th开启 创建分类也 1$ hexo new page categories"},{"title":"docker基本操作（一）","permalink":"http://blog.simble.xyz/post/493f285a.html","text":"本文作为docker使用笔记供小伙伴们参考 准备工作 安装最新版的docker-ce，会将自动命令行补齐安装在/usr/share/bash-completion/completions/docker目录 为了方便操作，在ubuntu上打开docker命令行自动补齐功能 编辑/etc/bash.bashrc文件，查找completion段，将该段内容前的#删除即可 12345678# enable bash completion in interactive shellsif ! shopt -oq posix; then if [ -f /usr/share/bash-completion/bash_completion ]; then . /usr/share/bash-completion/bash_completion elif [ -f /etc/bash_completion ]; then . /etc/bash_completion fifi image操作 build image 1$ docker build -t mytest:latest . 查看image 12345$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEnginx &lt;none&gt; 73acd1f0cfad 8 days ago 109MBmongo 3 5b1317f8158f 8 days ago 366MB$ dockeer image ls 删除image 12$ docker image rm &lt;id/name&gt;$ docker rmi &lt;id/name&gt; 从docker hub上获取image 国内访问docker hub较慢，可使用阿里云的docker镜像服务 1$ docker pull nginx continer相关 创建docker 1$ docker run -d --name mytest -p 80:80 mynginx 常用参数说明 –rm 当运行结束（当CMD或entrypoint或docker run命令行指定的命令运行结束时，容器停止）时自动删除docker -it 重定向docker终端 -d 在后台执行 -e 添加运行时的参数，常被用于docker CMD执行时增加参数 docker支持长id和短id方式索引，亦可通过名称进行索引 stop/start/restart容器 通过docker stop可以停止运行的容器，也可以使用docker kill来快速停止一个容器 docker start会保留容器的第一次启动时的所有参数 docker restart可以重启容器 可以在启动容器时设置–restart来自动重启容器 删除容器 可以使用docker ps列出当前正在运行的容器 容器停止运行不代表容器已经被删除，可以使用docker ps -a 使用docker rm来删除一个容器 使用docker rmi则会删除docker的image 查看及操作 查看容器状态 123$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES30c8e35f1292 portainer/portainer \"/portainer\" 10 days ago Up 2 days 0.0.0.0:9000-&gt;9000/tcp clever_murdock 使用docker ps -a查看所有容器（包含休眠状态） 1234$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES30c8e35f1292 portainer/portainer \"/portainer\" 10 days ago Up 2 days 0.0.0.0:9000-&gt;9000/tcp clever_murdockef58155f9957 registry:2 \"/entrypoint.sh /etc…\" 2 days ago Exited (137) 41 hours ago sltregistry 连接到容器的终端 尽量使用exec方法，attach连入后可查看当前容器命令运行的日志，但不当的操作容易使运行中的容器退出 使用attach 1$ docker attach &lt;id/name&gt; 使用exec 1$ docker exec -it &lt;id/name&gt; /bin/sh 查看容器运行的日志 容器以-d参数运行时，可以使用docker logs查看运行过程中的日志 123$ docker logs mytest_haproxy[WARNING] 079/100555 (1) : [haproxy.main()] Cannot raise FD limit to 200000011, limit is 1048576.[WARNING] 079/100555 (1) : [haproxy.main()] FD limit (1048576) too low for maxconn=100000000/maxsock=200000011. Please raise 'ulimit-n' to 200000011 or more to avoid any trouble."},{"title":"在树莓派上部署nodejs","permalink":"http://blog.simble.xyz/post/4fa2931e.html","text":"终于有时间在树莓派上部署一个自己的小博客了，平时用惯了ubuntu，到了树莓派上，发现并没有那么简单。首先就是apt install nodejs，安装完没有npm。不过，经过一番折腾，便有了现在的小站点 安装nodejs nodejs可以使用源码编译和二进制包来安装。考虑到树莓派的处理能力，要编译一个nodejs，太过耗时。直接选用官网提供的二进制包完成安装 安装包获取 官网上ARM的bin包有3个，分别是v6，v7和v8。而树莓派的版本是v7 12$ uname -aLinux raspberrypi 4.9.80-v7+ #1098 SMP Fri Mar 9 19:11:42 GMT 2018 armv7l GNU/Linux 获取安装包 1$ wget https://nodejs.org/dist/v8.10.0/node-v8.10.0-linux-armv7l.tar.xz 安装nodejs 解压缩 1234567$ tar -Jxv -f node-v8.10.0-linux-armv7l.tar.xznode-v8.10.0-linux-armv7l/node-v8.10.0-linux-armv7l/README.mdnode-v8.10.0-linux-armv7l/bin/node-v8.10.0-linux-armv7l/bin/nodenode-v8.10.0-linux-armv7l/bin/npm... 可根据个人喜好重命名文件夹，此处重命名为node，分别验证版本信息 1234pi@raspberrypi:~/node $ ./bin/node -vv8.10.0pi@raspberrypi:~/node $ ./bin/npm -v5.6.0 配置node和npm为全局命令 12pi@raspberrypi:~/node $ sudo ln /home/pi/node/bin/node /usr/local/bin/nodepi@raspberrypi:~/node $ sudo ln -s /home/pi/node/lib/node_modules/npm/bin/npm /usr/local/bin/npm 此时执行npm会报错 123456pi@raspberrypi:~/node $ npm -vmodule.js:471 throw err; ^Error: Cannot find module '/usr/local/bin/node_modules/npm/bin/npm-cli.js' 需要修改/usr/local/bin/目录下的npm文件，将$basedir替换为绝对路径，此处为/home/pi/node/ 12345678910111213141516171819202122232425262728293031323334#!/bin/sh(set -o igncr) 2&gt;/dev/null &amp;&amp; set -o igncr; # cygwin encoding fixbasedir=`dirname \"$0\"`case `uname` in *CYGWIN*) basedir=`cygpath -w \"$basedir\"`;;esacNODE_EXE=\"/home/pi/node/bin/node.exe\"if ! [ -x \"$NODE_EXE\" ]; then NODE_EXE=nodefiNPM_CLI_JS=\"/home/pi/node/lib/node_modules/npm/bin/npm-cli.js\"case `uname` in *MINGW*) NPM_PREFIX=`\"$NODE_EXE\" \"$NPM_CLI_JS\" prefix -g` NPM_PREFIX_NPM_CLI_JS=\"$NPM_PREFIX/node_modules/npm/bin/npm-cli.js\" if [ -f \"$NPM_PREFIX_NPM_CLI_JS\" ]; then NPM_CLI_JS=\"$NPM_PREFIX_NPM_CLI_JS\" fi ;; *CYGWIN*) NPM_PREFIX=`\"$NODE_EXE\" \"$NPM_CLI_JS\" prefix -g` NPM_PREFIX_NPM_CLI_JS=\"$NPM_PREFIX/node_modules/npm/bin/npm-cli.js\" if [ -f \"$NPM_PREFIX_NPM_CLI_JS\" ]; then NPM_CLI_JS=\"$NPM_PREFIX_NPM_CLI_JS\" fi ;;esac\"$NODE_EXE\" \"$NPM_CLI_JS\" \"$@\" 再次验证，npm已经可以正常工作"},{"title":"Hello World","permalink":"http://blog.simble.xyz/post/4a17b156.html","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new \"My New Post\" More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment"}]}