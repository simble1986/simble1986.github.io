{"meta":{"title":"Simble的小站","subtitle":"晒晒狗 vs 练练手","description":"记录一些技术的或者旅行的东东","author":"Simble","url":"http://www.isimble.com","root":"/"},"pages":[{"title":"categories","date":"2018-03-22T17:18:58.000Z","updated":"2018-03-30T02:37:58.000Z","comments":false,"path":"categories/index.html","permalink":"http://www.isimble.com/categories/index.html","excerpt":"","text":""},{"title":"About","date":"2018-03-22T17:18:17.000Z","updated":"2018-03-30T02:37:58.000Z","comments":true,"path":"about/index.html","permalink":"http://www.isimble.com/about/index.html","excerpt":"","text":"走不完的旅程，就有说不完的故事 关于博主 已过而立之年的网络测试工程师一名，从事网络安全6年有余。现居苏州 理想是走遍全世界 可以很文艺，也可以很技术 文艺的时候，弹弹琴，拍拍照，旅旅游（且算作是文艺吧） 技术的时候，可以熬夜鼓捣树莓派，可以把游戏扔在一边折腾docker，可以一次又一次的折腾网站玩儿，还可以…… 先等会儿，我先去收拾一下我家布丁，丫的又再咬我的拖鞋 我的马蜂窝 关于博客 应该会包含三部分内容 一些技术笔记 旅行随笔/游记 晒晒狗 关于布丁 出生于2014年12月12日，可能是购物送的吧，哈哈。其实是前同事家的母金毛生的，专门挑了最活波的一条。也跟着我从北京来到了苏州"},{"title":"留言板","date":"2018-03-26T15:36:19.000Z","updated":"2018-03-30T02:37:58.000Z","comments":true,"path":"guestbook/index.html","permalink":"http://www.isimble.com/guestbook/index.html","excerpt":"","text":"既然来了，就是一种缘分，留下点什么吧:cat:"},{"title":"schedule","date":"2019-11-18T05:11:16.000Z","updated":"2019-11-18T13:11:56.065Z","comments":true,"path":"schedule/index.html","permalink":"http://www.isimble.com/schedule/index.html","excerpt":"","text":""},{"title":"Tagcloud","date":"2018-03-22T17:18:00.000Z","updated":"2018-03-30T02:37:58.000Z","comments":false,"path":"tags/index.html","permalink":"http://www.isimble.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"K8s学习笔记——深入理解容器镜像","slug":"kubernetesLearning03","date":"2020-06-23T04:25:55.000Z","updated":"2020-06-23T04:33:00.853Z","comments":true,"path":"2020/06/23/kubernetesLearning03/","link":"","permalink":"http://www.isimble.com/2020/06/23/kubernetesLearning03/","excerpt":"学习极客时间上的《深入剖析Kubernetes》 秉持眼过千遍不如手过一遍的原则. 对应章节：07 | 白话容器基础（三）：深入理解容器镜像","text":"学习极客时间上的《深入剖析Kubernetes》 秉持眼过千遍不如手过一遍的原则. 对应章节：07 | 白话容器基础（三）：深入理解容器镜像 以系统调用方式创建namespace实验 ns.c 12345678910111213141516171819202122232425262728293031#define _GNU_SOURCE#include &lt;sys/mount.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/wait.h&gt;#include &lt;stdio.h&gt;#include &lt;sched.h&gt;#include &lt;signal.h&gt;#include &lt;unistd.h&gt;#define STACK_SIZE (1024 * 1024)static char container_stack[STACK_SIZE];char* const container_args[] = &#123; \"/bin/bash\", NULL&#125;;int container_main(void* arg)&#123; printf(\"Container - inside the container!\\n\"); execv(container_args[0], container_args); printf(\"Something's wrong!\\n\"); return 1;&#125;int main()&#123; printf(\"Parent - start a container!\\n\"); int container_pid = clone(container_main, container_stack+STACK_SIZE, CLONE_NEWNS | SIGCHLD , NULL); waitpid(container_pid, NULL, 0); printf(\"Parent - container stopped!\\n\"); return 0;&#125; build ns并进入ns 12345678910$ gcc -o ns ns.c$ ./nsParent - start a container!Container - inside the container!$ ls /tmpsystemd-private-8bac4934482d484d879054051ff48730-systemd-resolved.service-54yGbQ vmware-root_563-4281712267systemd-private-8bac4934482d484d879054051ff48730-systemd-timesyncd.service-ti2Lns$ exitexitParent - container stopped! 查看进程(在另外一个窗口中执行) 123456789101112$ ps -aux...root 12217 0.0 0.0 5532 720 pts/3 S 06:41 0:00 ./nsroot 12218 0.0 0.3 20312 3980 pts/3 S+ 06:41 0:00 /bin/bash...$ pstree -gsystemd(1)─┬─VGAuthService(562) ├─sshd(1144)─┬─sshd(2307)───bash(2394) │ ├─sshd(2448)───bash(2529) │ ├─sshd(11698)───bash(11784)───pstree(12265) │ └─sshd(11845)───bash(11927)───ns(12217)───bash(12218) ... 由上面的操作可见，即使开启了namespace，容器进程看到的文件系统与宿主机一样 重新挂载目录实验 修改代码 修改ns.c的container_main函数，新创建文件ns_new.c 注：因为是在虚拟机上实验，根目录类型默认是shared，所以，需要先重新挂载根目录 12345678910int container_main(void* arg)&#123; printf(\"Container - inside the container!\\n\"); // 如果你的机器的根目录的挂载类型是shared，那必须先重新挂载根目录 mount(\"\", \"/\", NULL, MS_PRIVATE, \"\"); mount(\"none\", \"/tmp\", \"tmpfs\", 0, \"\"); execv(container_args[0], container_args); printf(\"Something's wrong!\\n\"); return 1;&#125; 编译及测试效果 在容器内 12345678910111213141516$ gcc -o ns_new ns_new.c$ ./ns_newParent - start a container!Container - inside the container!$ ls /tmp$ mount -l | grep tmpfsudev on /dev type devtmpfs (rw,nosuid,relatime,size=473204k,nr_inodes=118301,mode=755)tmpfs on /run type tmpfs (rw,nosuid,noexec,relatime,size=100928k,mode=755)tmpfs on /dev/shm type tmpfs (rw,nosuid,nodev)tmpfs on /run/lock type tmpfs (rw,nosuid,nodev,noexec,relatime,size=5120k)tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)tmpfs on /run/user/0 type tmpfs (rw,nosuid,nodev,relatime,size=100924k,mode=700)none on /tmp type tmpfs (rw,relatime)$ exitexitParent - container stopped! 在宿主机上 1234567$ mount -l | grep tmpfsudev on /dev type devtmpfs (rw,nosuid,relatime,size=473204k,nr_inodes=118301,mode=755)tmpfs on /run type tmpfs (rw,nosuid,noexec,relatime,size=100928k,mode=755)tmpfs on /dev/shm type tmpfs (rw,nosuid,nodev)tmpfs on /run/lock type tmpfs (rw,nosuid,nodev,noexec,relatime,size=5120k)tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)tmpfs on /run/user/0 type tmpfs (rw,nosuid,nodev,relatime,size=100924k,mode=700) chroot实验 准备 创建一个test目录并准备未见 1234567891011121314151617$ mkdir test$ mkdir -p test/&#123;bin,lib64,lib&#125;# 拷贝bash和ls命令$ cp -v /bin/&#123;bash,ls&#125; test/bin/'/bin/bash' -&gt; 'test/bin/bash''/bin/ls' -&gt; 'test/bin/ls'# 拷贝lib$ list=\"$(ldd /bin/ls | egrep -o '/lib.*\\.[0-9]')\"$ mkdir test/lib/x86_64-linux-gnu$ for i in $list; do cp -v \"$i\" \"test$&#123;i&#125;\"; done'/lib/x86_64-linux-gnu/libselinux.so.1' -&gt; 'test/lib/x86_64-linux-gnu/libselinux.so.1''/lib/x86_64-linux-gnu/libc.so.6' -&gt; 'test/lib/x86_64-linux-gnu/libc.so.6''/lib/x86_64-linux-gnu/libpcre.so.3' -&gt; 'test/lib/x86_64-linux-gnu/libpcre.so.3''/lib/x86_64-linux-gnu/libdl.so.2' -&gt; 'test/lib/x86_64-linux-gnu/libdl.so.2''/lib64/ld-linux-x86-64.so.2' -&gt; 'test/lib64/ld-linux-x86-64.so.2''/lib/x86_64-linux-gnu/libpthread.so.0' -&gt; 'test/lib/x86_64-linux-gnu/libpthread.so.0'$ cp /lib/x86_64-linux-gnu/libtinfo.so.5 test/lib/x86_64-linux-gnu/ chroot 123$ chroot test /bin/bashbash-4.4# lsbin lib lib64 以busybox的镜像为例，同样可以chroot 123$ chroot busybox /bin/sh/ # lsbin dev etc home root tmp usr var UnionFS实验 12345678910111213$ mkdir A$ mkdir B$ mkdi^C$ touch A/t1.txt$ touch A/t2.txt$ touch B/t2.txt$ touch B/t3.txt$ mkdir C$ mount -t aufs -o dirs=./A:./B none ./C$ ls Ct1.txt t2.txt t3.txt$ mount -l | grep aufsnone on /root/bqi/C type aufs (rw,relatime,si=f9e234d74656f278) 此时，可以尝试修改A/t1.txt, A/t2.txt, B/t2.txt, B/t3.txt 12345678910$ echo 'This is a test' &gt; A/t1.txt$ cat C/t1.txtThis is a test$ echo 'This is a test2' &gt; A/t2.txt$ cat C/t2.txtThis is a test2$ cat B/t2.txt$ echo 'This is test2 for B' &gt; B/t2.txt$ cat C/t2.txtThis is a test2 docker image解析 123456789101112131415161718192021$ docker inspect ubuntu... \"GraphDriver\": &#123; \"Data\": &#123; \"LowerDir\": \"/var/lib/docker/overlay2/823e415d4256d05fb0101af4dcc42a4389d44cf6467972d654e93e0cc575cd9b/diff:/var/lib/docker/overlay2/37d3e588905fae55c8a0481e9cda7be36177af874631abb15724c893887e260b/diff:/var/lib/docker/overlay2/40d198d6f624e455800254766eb6a7190ce02442fc48f02f6f16f72105cefd0d/diff\", \"MergedDir\": \"/var/lib/docker/overlay2/17bd5da0cda20e8ecd1d4955d25f49609ff0d7aa72fe45a0388a357fcc5b625f/merged\", \"UpperDir\": \"/var/lib/docker/overlay2/17bd5da0cda20e8ecd1d4955d25f49609ff0d7aa72fe45a0388a357fcc5b625f/diff\", \"WorkDir\": \"/var/lib/docker/overlay2/17bd5da0cda20e8ecd1d4955d25f49609ff0d7aa72fe45a0388a357fcc5b625f/work\" &#125;, \"Name\": \"overlay2\" &#125;, \"RootFS\": &#123; \"Type\": \"layers\", \"Layers\": [ \"sha256:7789f1a3d4e9258fbe5469a8d657deb6aba168d86967063e9b80ac3e1154333f\", \"sha256:9e53fd4895597d04f8871a68caea4c686011e1fbd0be32e57e89ada2ea5c24c4\", \"sha256:2a19bd70fcd4ce7fd73b37b1b2c710f8065817a9db821ff839fe0b4b4560e643\", \"sha256:8891751e0a1733c5c214d17ad2b0040deccbdea0acebb963679735964d516ac2\" ] &#125;,... 你会看到，Ubuntu镜像，在我的环境里面是4层 overlay挂载方式 先启动一个container 12345678910111213141516$ docker run -it -d ubuntu$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESa274e38c218a ubuntu \"/bin/bash\" 17 minutes ago Up 17 minutes musing_knuth$ docker inspect a274e38c218a... \"GraphDriver\": &#123; \"Data\": &#123; \"LowerDir\": \"/var/lib/docker/overlay2/0c3ff90aba26b4197b2293789f75d4e3db7a9213601b8d222b1f0c413c7115b2-init/diff:/var/lib/docker/overlay2/17bd5da0cda20e8ecd1d4955d25f49609ff0d7aa72fe45a0388a357fcc5b625f/diff:/var/lib/docker/overlay2/823e415d4256d05fb0101af4dcc42a4389d44cf6467972d654e93e0cc575cd9b/diff:/var/lib/docker/overlay2/37d3e588905fae55c8a0481e9cda7be36177af874631abb15724c893887e260b/diff:/var/lib/docker/overlay2/40d198d6f624e455800254766eb6a7190ce02442fc48f02f6f16f72105cefd0d/diff\", \"MergedDir\": \"/var/lib/docker/overlay2/0c3ff90aba26b4197b2293789f75d4e3db7a9213601b8d222b1f0c413c7115b2/merged\", \"UpperDir\": \"/var/lib/docker/overlay2/0c3ff90aba26b4197b2293789f75d4e3db7a9213601b8d222b1f0c413c7115b2/diff\", \"WorkDir\": \"/var/lib/docker/overlay2/0c3ff90aba26b4197b2293789f75d4e3db7a9213601b8d222b1f0c413c7115b2/work\" &#125;, \"Name\": \"overlay2\" &#125;,... 查看系统挂载表 12$ cat /proc/mounts | grep overlayoverlay /var/lib/docker/overlay2/0c3ff90aba26b4197b2293789f75d4e3db7a9213601b8d222b1f0c413c7115b2/merged overlay rw,relatime,lowerdir=/var/lib/docker/overlay2/l/7ZDIR6KYXXF6RMDW3JCBEQUGMH:/var/lib/docker/overlay2/l/TH3OYMS4POUF3S22QNB7UJPORG:/var/lib/docker/overlay2/l/BJILDL5W6H6U7LGVSUUS2QUTGO:/var/lib/docker/overlay2/l/6F6BVIETKMGL5QLJIOCP6CONB3:/var/lib/docker/overlay2/l/P5BANYVEKZJYYPT4M6IFNLAR7Z,upperdir=/var/lib/docker/overlay2/0c3ff90aba26b4197b2293789f75d4e3db7a9213601b8d222b1f0c413c7115b2/diff,workdir=/var/lib/docker/overlay2/0c3ff90aba26b4197b2293789f75d4e3db7a9213601b8d222b1f0c413c7115b2/work 0 0 可以看到，lowerdir由5个目录共同挂载而成，分别是 7ZDIR6KYXXF6RMDW3JCBEQUGMH TH3OYMS4POUF3S22QNB7UJPORG BJILDL5W6H6U7LGVSUUS2QUTGO 6F6BVIETKMGL5QLJIOCP6CONB3 P5BANYVEKZJYYPT4M6IFNLAR7Z 查看overlay2目录下的文件 12345678910111213141516$ ls /var/lib/docker/overlay2/l/ -ltotal 56lrwxrwxrwx 1 root root 72 May 14 07:09 2L7W765NSNAZAJUW324PTRY6AF -&gt; ../6bd794b03d6772755f61a55ff28f0f20caf1541192c57030b1c0d92e4d3134fa/difflrwxrwxrwx 1 root root 72 May 13 07:09 2VHUX6G37XVXLON33KHDZBOVBH -&gt; ../2787c91d4cd57511162a5b17a1ad9cca5204e57b541127223dd11b8c084710bb/difflrwxrwxrwx 1 root root 72 Jun 11 05:51 6F6BVIETKMGL5QLJIOCP6CONB3 -&gt; ../37d3e588905fae55c8a0481e9cda7be36177af874631abb15724c893887e260b/difflrwxrwxrwx 1 root root 72 May 13 07:24 6P3MYX3ANRVWUUGIBBTJYPGRLP -&gt; ../6d18d5f8aed3820e7500e1f70b3c5d896b90c109977a1097e957667a6b0f48f3/difflrwxrwxrwx 1 root root 77 Jun 11 07:53 7ZDIR6KYXXF6RMDW3JCBEQUGMH -&gt; ../0c3ff90aba26b4197b2293789f75d4e3db7a9213601b8d222b1f0c413c7115b2-init/difflrwxrwxrwx 1 root root 72 Jun 11 05:51 BJILDL5W6H6U7LGVSUUS2QUTGO -&gt; ../823e415d4256d05fb0101af4dcc42a4389d44cf6467972d654e93e0cc575cd9b/difflrwxrwxrwx 1 root root 72 May 13 07:24 D26SOVMOFLZLRVVBCVXJVRYAE2 -&gt; ../a74c293f4eb20bef383865bbba97f84a51fd0d894ced280dc1cfe6021be3ae77/difflrwxrwxrwx 1 root root 72 Jun 11 07:53 EY46SF3NEYTFL4QOXUZT5YHMA3 -&gt; ../0c3ff90aba26b4197b2293789f75d4e3db7a9213601b8d222b1f0c413c7115b2/difflrwxrwxrwx 1 root root 72 May 13 07:30 IXP5XYXVUASXI2FOX5UKMYW2JN -&gt; ../c58e315ff14dda2b6ec7f75a3a0a8099dfe269604a0acecf6ecf026c6b56de63/difflrwxrwxrwx 1 root root 72 May 14 07:18 N7NRQKX4E62F2JKRQ5JCI3BSSH -&gt; ../d6fed7e45abcf9e0055b9de876a81a5347cdcf364736b0f50053630f8f189e30/difflrwxrwxrwx 1 root root 72 May 13 06:16 NJCD6YK72MQFTUSJUMGOEJL4WM -&gt; ../bf1fb537d794b4460c81ae39fc45c3230c22b47e4509a35c282ca15727fe81ac/difflrwxrwxrwx 1 root root 72 Jun 11 05:51 P5BANYVEKZJYYPT4M6IFNLAR7Z -&gt; ../40d198d6f624e455800254766eb6a7190ce02442fc48f02f6f16f72105cefd0d/difflrwxrwxrwx 1 root root 72 Jun 11 05:51 TH3OYMS4POUF3S22QNB7UJPORG -&gt; ../17bd5da0cda20e8ecd1d4955d25f49609ff0d7aa72fe45a0388a357fcc5b625f/difflrwxrwxrwx 1 root root 72 May 13 07:24 YQB24YJNNQSDHU2IDBA2L4LCX4 -&gt; ../c3bee923bb1d5cd56503c976bc8353a6a579698186536f6023524b84373a6834/diff 做个对比 容器ID为a274e38，对应的DIR的ID是0c3ff90 lowerdir=/var/lib/docker/overlay2/l/7ZDIR6KYXXF6RMDW3JCBEQUGMH， 实际上指向了0c3ff90aba26b4197b2293789f75d4e3db7a9213601b8d222b1f0c413c7115b2-init/diff 12$ ls /var/lib/docker/overlay2/0c3ff90aba26b4197b2293789f75d4e3db7a9213601b8d222b1f0c413c7115b2-init/diff/dev etc /var/lib/docker/overlay2/l/TH3OYMS4POUF3S22QNB7UJPORG 实际上指向了17bd5da0cda20e8ecd1d4955d25f49609ff0d7aa72fe45a0388a357fcc5b625f/diff 12$ ls /var/lib/docker/overlay2/17bd5da0cda20e8ecd1d4955d25f49609ff0d7aa72fe45a0388a357fcc5b625f/diff/run 而17bd5da0cda20e8ecd1d4955d25f49609ff0d7aa72fe45a0388a357fcc5b625f/diff实际上是ubuntu镜像的UpperDir 12$ ls /var/lib/docker/overlay2/17bd5da0cda20e8ecd1d4955d25f49609ff0d7aa72fe45a0388a357fcc5b625f/diff/run /var/lib/docker/overlay2/l/BJILDL5W6H6U7LGVSUUS2QUTGO实际上是823e415d4256d05fb0101af4dcc42a4389d44cf6467972d654e93e0cc575cd9b/diff 而823e415d4256d05fb0101af4dcc42a4389d44cf6467972d654e93e0cc575cd9b/diff实际上是ubuntu镜像的LowerDir 12$ ls /var/lib/docker/overlay2/823e415d4256d05fb0101af4dcc42a4389d44cf6467972d654e93e0cc575cd9b/diff/etc usr var /var/lib/docker/overlay2/l/6F6BVIETKMGL5QLJIOCP6CONB3实际上是37d3e588905fae55c8a0481e9cda7be36177af874631abb15724c893887e260b/diff 12$ ls /var/lib/docker/overlay2/37d3e588905fae55c8a0481e9cda7be36177af874631abb15724c893887e260b/diff/var /var/lib/docker/overlay2/l/P5BANYVEKZJYYPT4M6IFNLAR7Z实际上是40d198d6f624e455800254766eb6a7190ce02442fc48f02f6f16f72105cefd0d/diff 12$ ls /var/lib/docker/overlay2/40d198d6f624e455800254766eb6a7190ce02442fc48f02f6f16f72105cefd0d/diff/bin boot dev etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin srv sys tmp usr var 小结 容器的镜像即rootfs是按照一层一层的组合起来的。 启动容器进程时，将多个增量的rootfs联合挂载成一个完整的rootfs 启动容器时，会只读模式挂载一个init层，以及一个可写的层","categories":[{"name":"k8s","slug":"k8s","permalink":"http://www.isimble.com/categories/k8s/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://www.isimble.com/tags/docker/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://www.isimble.com/tags/Kubernetes/"}]},{"title":"K8s学习笔记——限制与隔离","slug":"kubernetesLearning02","date":"2020-06-17T08:08:05.000Z","updated":"2020-06-17T08:18:47.113Z","comments":true,"path":"2020/06/17/kubernetesLearning02/","link":"","permalink":"http://www.isimble.com/2020/06/17/kubernetesLearning02/","excerpt":"学习极客时间上的《深入剖析Kubernetes》 秉持眼过千遍不如手过一遍的原则. 对应章节：06 | 白话容器基础（二）：隔离与限制","text":"学习极客时间上的《深入剖析Kubernetes》 秉持眼过千遍不如手过一遍的原则. 对应章节：06 | 白话容器基础（二）：隔离与限制 隔离与限制 cgroup的mount点 12345678910111213$ mount -t cgroupcgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,name=systemd)cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)cgroup on /sys/fs/cgroup/rdma type cgroup (rw,nosuid,nodev,noexec,relatime,rdma)cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls,net_prio)cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpu,cpuacct)cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset,clone_children)cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer) 查看CPU相关信息 123456789$ cd /sys/fs/cgroup$ ls cpucgroup.clone_children cpuacct.stat cpuacct.usage_percpu cpuacct.usage_sys cpu.cfs_quota_us docker system.slicecgroup.procs cpuacct.usage cpuacct.usage_percpu_sys cpuacct.usage_user cpu.shares notify_on_release taskscgroup.sane_behavior cpuacct.usage_all cpuacct.usage_percpu_user cpu.cfs_period_us cpu.stat release_agent user.slice$ cat cpu/cpu.cfs_period_us100000$ cat cpu/cpu.cfs_quota_us-1 在CPU目录下创建一个container目录 123456$ mkdir container$ ls container/cgroup.clone_children cpuacct.usage_all cpuacct.usage_sys cpu.sharescgroup.procs cpuacct.usage_percpu cpuacct.usage_user cpu.statcpuacct.stat cpuacct.usage_percpu_sys cpu.cfs_period_us notify_on_releasecpuacct.usage cpuacct.usage_percpu_user cpu.cfs_quota_us tasks 会看到创建目录后，会自动创建一堆文件 用一个while循环检测CPU使用状况 1234567891011$ while : ; do : ; done &amp;[1] 16508$ toptop - 09:03:06 up 3:32, 4 users, load average: 0.28, 0.07, 0.02Tasks: 105 total, 2 running, 61 sleeping, 0 stopped, 0 zombie%Cpu(s): 99.7 us, 0.3 sy, 0.0 ni, 0.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem : 1009256 total, 208232 free, 220168 used, 580856 buff/cacheKiB Swap: 2018300 total, 2008256 free, 10044 used. 647692 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND16508 root 20 0 21640 3180 1216 R 99.7 0.3 0:17.88 bash 在有的虚拟机上，你会看到CPU使用率不是99%，可能是50%，25%等，你可以思考一下这是为什么 使用container的cgroup对while循环进行资源限制 1234567891011$ echo 20000 &gt; container/cpu.cfs_quota_us$ echo 16508 &gt; container/tasks$ toptop - 09:06:24 up 3:35, 4 users, load average: 0.97, 0.52, 0.21Tasks: 105 total, 2 running, 61 sleeping, 0 stopped, 0 zombie%Cpu(s): 20.0 us, 0.0 sy, 0.0 ni, 79.7 id, 0.0 wa, 0.0 hi, 0.3 si, 0.0 stKiB Mem : 1009256 total, 208636 free, 219736 used, 580884 buff/cacheKiB Swap: 2018300 total, 2008256 free, 10044 used. 648120 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND16508 root 20 0 21640 3180 1216 R 20.6 0.3 3:31.08 bash 起一个容器看看 12345678910$ docker run -it --cpu-period=100000 --cpu-quota=20000 ubuntu /bin/bashroot@26cbffdcb5bf:/# cat /sys/fs/cgroup/cpu/cpu.cfs_period_us100000root@26cbffdcb5bf:/# cat /sys/fs/cgroup/cpu/cpu.cfs_quota_us20000# 在宿主机上$ cat /sys/fs/cgroup/cpu/docker/26cbffdcb5bf2f9d9ecdc9207f4211c8f5b3cfbc39d83c77ed4666db3ca0bac3/cpu.cfs_period_us100000$ cat /sys/fs/cgroup/cpu/docker/26cbffdcb5bf2f9d9ecdc9207f4211c8f5b3cfbc39d83c77ed4666db3ca0bac3/cpu.cfs_quota_us20000 top对比 12345678910111213141516171819202122232425262728293031323334353637383940# 容器中root@26cbffdcb5bf:/# while : ; do : ; done &amp;[1] 11root@26cbffdcb5bf:/# toptop - 09:10:34 up 3:39, 0 users, load average: 0.01, 0.21, 0.15Tasks: 3 total, 2 running, 1 sleeping, 0 stopped, 0 zombie%Cpu(s): 20.3 us, 0.0 sy, 0.0 ni, 79.7 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stMiB Mem : 985.6 total, 163.9 free, 251.5 used, 570.2 buff/cacheMiB Swap: 1971.0 total, 1961.2 free, 9.8 used. 595.9 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 11 root 20 0 4224 560 0 R 20.3 0.1 0:01.21 bash 1 root 20 0 4224 3504 2944 S 0.0 0.3 0:00.19 bash 12 root 20 0 6080 3244 2740 R 0.0 0.3 0:00.00 top # 宿主机上$ toptop - 09:11:36 up 3:40, 5 users, load average: 0.10, 0.20, 0.15Tasks: 111 total, 2 running, 66 sleeping, 0 stopped, 0 zombie%Cpu(s): 20.0 us, 0.3 sy, 0.0 ni, 79.7 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem : 1009256 total, 167472 free, 257600 used, 584184 buff/cacheKiB Swap: 2018300 total, 2008256 free, 10044 used. 610176 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND16755 root 20 0 4224 560 0 R 19.9 0.1 0:13.62 bash$ pstree -gsystemd(1)─┬─VGAuthService(562) ├─accounts-daemon(866)─┬─&#123;accounts-daemon&#125;(866) │ └─&#123;accounts-daemon&#125;(866) ├─atd(928) ├─containerd(1055)─├─containerd-shim(16557)─┬─bash(16585)───bash(16755) │ │ ├─&#123;containerd-shim&#125;(16557) │ │ ├─&#123;containerd-shim&#125;(16557) │ │ ├─&#123;containerd-shim&#125;(16557) │ │ ├─&#123;containerd-shim&#125;(16557) │ │ ├─&#123;containerd-shim&#125;(16557) │ │ ├─&#123;containerd-shim&#125;(16557) │ │ ├─&#123;containerd-shim&#125;(16557) │ │ ├─&#123;containerd-shim&#125;(16557) │ │ └─&#123;containerd-shim&#125;(16557) 可以看到容器内和容器外部看到的是一样的 总结 主要使用了cgroup对一个进程的CPU使用率进行了限制，通过了解进程的CPU使用率限制机制，了解docker通过cgroup对相关资源使用的限制","categories":[{"name":"k8s","slug":"k8s","permalink":"http://www.isimble.com/categories/k8s/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://www.isimble.com/tags/docker/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://www.isimble.com/tags/Kubernetes/"}]},{"title":"K8s学习笔记——container之于进程","slug":"kubernetesLearning01","date":"2020-06-17T05:35:07.000Z","updated":"2020-06-17T05:41:20.476Z","comments":true,"path":"2020/06/17/kubernetesLearning01/","link":"","permalink":"http://www.isimble.com/2020/06/17/kubernetesLearning01/","excerpt":"学习极客时间上的《深入剖析Kubernetes》 秉持眼过千遍不如手过一遍的原则. 对应章节：05 | 白话容器基础（一）：从进程说开去","text":"学习极客时间上的《深入剖析Kubernetes》 秉持眼过千遍不如手过一遍的原则. 对应章节：05 | 白话容器基础（一）：从进程说开去 操作 start一个container 1$ docker run -it -d busybox 查看进程 12345$ ps -aux...root 2817 0.0 0.2 107700 2296 ? Sl 05:42 0:00 containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runroot 2851 0.0 0.0 1308 4 pts/0 Ss+ 05:42 0:00 sh... 查看进程树 1234567891011121314151617$ pstree -gsystemd(1)─┬─VGAuthService(562) ├─accounts-daemon(866)─┬─&#123;accounts-daemon&#125;(866) │ └─&#123;accounts-daemon&#125;(866) ├─atd(928) ├─containerd(1055)─┬─containerd-shim(2817)─┬─sh(2851) │ │ ├─&#123;containerd-shim&#125;(2817) │ │ ├─&#123;containerd-shim&#125;(2817) │ │ ├─&#123;containerd-shim&#125;(2817) │ │ ├─&#123;containerd-shim&#125;(2817) │ │ ├─&#123;containerd-shim&#125;(2817) │ │ ├─&#123;containerd-shim&#125;(2817) │ │ ├─&#123;containerd-shim&#125;(2817) │ │ ├─&#123;containerd-shim&#125;(2817) │ │ ├─&#123;containerd-shim&#125;(2817) │ │ └─&#123;containerd-shim&#125;(2817) .... 查看容器内进程 1234$ psPID USER TIME COMMAND 1 root 0:00 sh 6 root 0:00 ps 分别查看两个进程的namespace 1234567891011121314151617181920/proc/2817/ns# ls -ltotal 0lrwxrwxrwx 1 root root 0 Jun 11 05:43 cgroup -&gt; 'cgroup:[4026531835]'lrwxrwxrwx 1 root root 0 Jun 11 05:43 ipc -&gt; 'ipc:[4026531839]'lrwxrwxrwx 1 root root 0 Jun 11 05:43 mnt -&gt; 'mnt:[4026531840]'lrwxrwxrwx 1 root root 0 Jun 11 05:43 net -&gt; 'net:[4026531993]'lrwxrwxrwx 1 root root 0 Jun 11 05:43 pid -&gt; 'pid:[4026531836]'lrwxrwxrwx 1 root root 0 Jun 11 06:16 pid_for_children -&gt; 'pid:[4026531836]'lrwxrwxrwx 1 root root 0 Jun 11 05:43 user -&gt; 'user:[4026531837]'lrwxrwxrwx 1 root root 0 Jun 11 05:43 uts -&gt; 'uts:[4026531838]/proc/2851/ns# ls -ltotal 0lrwxrwxrwx 1 root root 0 Jun 11 05:43 cgroup -&gt; 'cgroup:[4026531835]'lrwxrwxrwx 1 root root 0 Jun 11 05:43 ipc -&gt; 'ipc:[4026532571]'lrwxrwxrwx 1 root root 0 Jun 11 05:43 mnt -&gt; 'mnt:[4026532569]'lrwxrwxrwx 1 root root 0 Jun 11 05:42 net -&gt; 'net:[4026532574]'lrwxrwxrwx 1 root root 0 Jun 11 05:43 pid -&gt; 'pid:[4026532572]'lrwxrwxrwx 1 root root 0 Jun 11 06:16 pid_for_children -&gt; 'pid:[4026532572]'lrwxrwxrwx 1 root root 0 Jun 11 05:43 user -&gt; 'user:[4026531837]'lrwxrwxrwx 1 root root 0 Jun 11 05:43 uts -&gt; 'uts:[4026532570]' 总结 启动一个docker容器后，会看到启动了一个2817的进程，这个进程是1055的子进程 而因为busybox容器启动后，启动了sh，其实际上是2817的子进程2851 而在容器中，能看到1号进程是sh 通过/proc下可以看到，进程2817和进程2851的ns下，cgroup是都是4026531835 而很明显，每个container都会创建ipc, mnt, net, pid, pid_for_children, user, uts这些namespace","categories":[{"name":"k8s","slug":"k8s","permalink":"http://www.isimble.com/categories/k8s/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://www.isimble.com/tags/docker/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://www.isimble.com/tags/Kubernetes/"}]},{"title":"国内源安装k8s——Ubuntu","slug":"ubuntu-install-k8s-aliyun","date":"2020-05-19T09:16:31.000Z","updated":"2020-05-19T09:26:18.171Z","comments":true,"path":"2020/05/19/ubuntu-install-k8s-aliyun/","link":"","permalink":"http://www.isimble.com/2020/05/19/ubuntu-install-k8s-aliyun/","excerpt":"阿里云源 Ubuntu20.04 master + slave","text":"阿里云源 Ubuntu20.04 master + slave 安装docker 分别在两个node上安装docker-ce 123456$ apt-get update $ apt-get -y install apt-transport-https ca-certificates curl software-properties-common$ curl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -$ add-apt-repository \"deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable\"$ apt update$ apt-get -y install docker-ce 安装kubeadm，kubectl，kubelet 分别在两个node上安装 1234567$ apt-get update &amp;&amp; apt-get install -y apt-transport-https$ curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - $ cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.listdeb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial mainEOF$ apt-get update$ apt-get install -y kubelet kubeadm kubectl 初始化master节点 1$ kubeadm init --pod-network-cidr=172.172.0.0/16 --image-repository registry.aliyuncs.com/google_containers 安装成功后，会有如下信息： 12345678910111213141516Your Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/Then you can join any number of worker nodes by running the following on each as root:kubeadm join 10.160.18.180:6443 --token 5xxosi.3du1z15pevcvnyyx \\ --discovery-token-ca-cert-hash sha256:4cc4977482e04ac0ca845bf3520a6a5fa8a0cf6ac8233e734a47e0250c259f73 根据提示，执行 123$ mkdir -p $HOME/.kube$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config$ sudo chown $(id -u):$(id -g) $HOME/.kube/config 安装flannel 参考： https://github.com/coreos/flannel 1$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 安装dashboard 参考：https://github.com/kubernetes/dashboard 1$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml 修改dashboard配置 修改spec中的type为NodePort 1234567891011121314$ kubectl -n kubernetes-dashboard edit service kubernetes-dashboard.......spec: clusterIP: 10.101.212.193 externalTrafficPolicy: Cluster ports: - nodePort: 32609 port: 443 protocol: TCP targetPort: 8443 selector: k8s-app: kubernetes-dashboard sessionAffinity: None type: NodePort 修改成功后，查看port信息 123$ kubectl -n kubernetes-dashboard get service kubernetes-dashboardNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes-dashboard NodePort 10.101.212.193 &lt;none&gt; 443:32609/TCP 27m 现在，可以通过https://&lt;master-ip&gt;:&lt;NodePort&gt;（这里的port是32609）来访问dashboard了 虽然页面已经展示出来了，但需要使用token或Kubeconfig才能访问 创建sample-user 创建服务账号 新建dashboard-adminuser.yaml并写入： 12345apiVersion: v1kind: ServiceAccountmetadata: name: admin-user namespace: kubernetes-dashboard 执行： 1$ kubectl apply -f dashboard-adminuser.yaml 创建ClusterRoleBinding 新建cluster-role-binding.yaml并写入： 123456789101112apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: admin-userroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard 执行： 1$ kubetcl apply -f cluster-role-binding.yaml 获取token 1234567891011121314$ kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '&#123;print $1&#125;')Name: admin-user-token-jmggpNamespace: kubernetes-dashboardLabels: &lt;none&gt;Annotations: kubernetes.io/service-account.name: admin-user kubernetes.io/service-account.uid: 58210c16-0fac-438c-8867-d0a3e7b950b9Type: kubernetes.io/service-account-tokenData====ca.crt: 1025 bytesnamespace: 20 bytestoken: eyJhbGciOiJSUzI1NiIsImtpZCI6IlhTSnlXMUhXTlNnUmd4MlVMTzdtbm14YVdiSzNUdjk4UnVoZ3RRbUFXZGsifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLWptZ2dwIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI1ODIxMGMxNi0wZmFjLTQzOGMtODg2Ny1kMGEzZTdiOTUwYjkiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.F4TKNO_6Guu-vcLUtELUOhRI2dGMcZ3V1et2evono_a6f-TvCR9c4pbyYCnRdCG6_MumTmyE5W1g3zHioVnb5TgnGwfmAfIWLltwwLEOxOdLfO7oqM8zrYfzZnIH16SoOZQYMU7xIk5MhE5WN265n8Q2kpDMraf0L06_nqNy1pq8h9eaX0QIntosl4fmf9KVew0geLCKbknEwpnzGGfSCcKLLgE7a45ACWwStJiL29t69gcKJ6ze33MXpA5_irk2nKkavXbKEk7ejapgYK66nOxJnDKgbNVDcBP47xHrPjGeeupB6bw6uUMWxA6z4kJUTVRepk6yTMGVDPzB9Muicw 现在，可以使用token登录Dashboard了 slave节点加入集群 12$ kubeadm join 10.160.18.180:6443 --token 5xxosi.3du1z15pevcvnyyx \\ --discovery-token-ca-cert-hash sha256:4cc4977482e04ac0ca845bf3520a6a5fa8a0cf6ac8233e734a47e0250c259f73 问题及解决方案 docker cgroup driver问题 问题日志 1[WARNING IsDockerSystemdCheck]: detected \"cgroupfs\" as the Docker cgroup driver. The recommended driver is \"systemd\". Please follow the guide at https://kubernetes.io/docs/setup/cri/ 解决方法 在/etc/docker/下创建daemon.json 12345cat &gt; /etc/docker/daemon.json &lt;&lt;EOF&#123; \"exec-opts\": [\"native.cgroupdriver=systemd\"]&#125;EOF 重启docker进程 12$ systemctl restart docker$ systemctl status docker swap问题 问题日志 1[ERROR Swap]: running with swap on is not supported. Please disable swap 解决方法 12$ swapoff -a# 在所有node上执行 但这只是暂时关闭了swap，重启node后，就会再次打开。需要修改/etc/fstab，在swap那行加上# 1#UUID=4eeb5155-41f9-4478-a420-2beb4290a721 none swap sw 0 0 Node处于NotReady状态 node处于NotReady状态的原因有很多。可以一步一步处理 1234$ kubectl get nodesNAME STATUS ROLES AGE VERSIONnode1 Ready master 4h56m v1.18.2node2 NotReady &lt;none&gt; 4h6m v1.18.2 先查看错误原因： 123456789101112$ kubectl get pod -n kube-systemNAME READY STATUS RESTARTS AGEcoredns-7ff77c879f-2k7rw 1/1 Running 1 4h47mcoredns-7ff77c879f-q76jr 1/1 Running 1 4h47metcd-node1 1/1 Running 2 4h47mkube-apiserver-node1 1/1 Running 2 4h47mkube-controller-manager-node1 1/1 Running 2 4h47mkube-flannel-ds-amd64-2jn8n 0/1 Init:ImagePullBackOff 0 3h49mkube-flannel-ds-amd64-ftpxl 1/1 Running 1 3h49mkube-proxy-5q8wp 1/1 Running 2 4h47mkube-proxy-wfcjq 0/1 ContainerCreating 0 5m46skube-scheduler-node1 1/1 Running 2 4h47m k8s有些服务会在各个节点上启动，比如这里的proxy，flannel。 1234567891011$ kubectl describe pod -n kube-system kube-flannel-ds-amd64-2jn8n.....Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Pulling 6m51s kubelet, node2 Pulling image \"quay.io/coreos/flannel:v0.12.0-amd64\" Warning Failed 5m48s kubelet, node2 Failed to pull image \"quay.io/coreos/flannel:v0.12.0-amd64\": rpc error: code = Unknown desc = Error response from daemon: Get https://quay.io/v2/coreos/flannel/manifests/v0.12.0-amd64: received unexpected HTTP status: 500 Internal Server Error Warning Failed 5m48s kubelet, node2 Error: ErrImagePull Normal BackOff 5m47s kubelet, node2 Back-off pulling image \"quay.io/coreos/flannel:v0.12.0-amd64\" Warning Failed 5m47s kubelet, node2 Error: ImagePullBackOff Normal Pulling 5m36s (x2 over 5m51s) kubelet, node2 Pulling image \"quay.io/coreos/flannel:v0.12.0-amd64\" 最常见的是ImagePull失败。比如master node上镜像拉取正常，而其他节点拉取失败。 解决方法 1. 在slave节点上手工拉取镜像 1$ docker pull quay.io/coreos/flannel:v0.12.0-amd64 2. 将master节点上的镜像导入slave节点 查看master节点上的镜像 123456789101112(master)$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEkubernetesui/dashboard v2.0.0 8b32422733b3 3 weeks ago 222MBregistry.aliyuncs.com/google_containers/kube-proxy v1.18.2 0d40868643c6 4 weeks ago 117MBregistry.aliyuncs.com/google_containers/kube-scheduler v1.18.2 a3099161e137 4 weeks ago 95.3MBregistry.aliyuncs.com/google_containers/kube-apiserver v1.18.2 6ed75ad404bd 4 weeks ago 173MBregistry.aliyuncs.com/google_containers/kube-controller-manager v1.18.2 ace0a8c17ba9 4 weeks ago 162MBkubernetesui/metrics-scraper v1.0.4 86262685d9ab 7 weeks ago 36.9MBquay.io/coreos/flannel v0.12.0-amd64 4e9f801d2217 2 months ago 52.8MBregistry.aliyuncs.com/google_containers/pause 3.2 80d28bedfe5d 3 months ago 683kBregistry.aliyuncs.com/google_containers/coredns 1.6.7 67da37a9a360 3 months ago 43.8MBregistry.aliyuncs.com/google_containers/etcd 3.4.3-0 303ce5db0e90 6 months ago 288MB 将镜像导出为文件 1(master)$ docker save quay.io/coreos/flannel &gt; flannel.tar 将文件传输到slave节点上 slave节点上导入镜像 1234567(slave)$ docker load &lt; flannel.tar256a7af3acb1: Loading layer [==================================================&gt;] 5.844MB/5.844MBd572e5d9d39b: Loading layer [==================================================&gt;] 10.37MB/10.37MB57c10be5852f: Loading layer [==================================================&gt;] 2.249MB/2.249MB7412f8eefb77: Loading layer [==================================================&gt;] 35.26MB/35.26MB05116c9ff7bf: Loading layer [==================================================&gt;] 5.12kB/5.12kBLoaded image: quay.io/coreos/flannel:v0.12.0-amd64 Now， all is OK","categories":[{"name":"k8s","slug":"k8s","permalink":"http://www.isimble.com/categories/k8s/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://www.isimble.com/tags/docker/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://www.isimble.com/tags/Kubernetes/"}]},{"title":"Ubuntu安装IPSAN","slug":"setup-ipsan-ubuntu","date":"2020-04-17T09:00:27.000Z","updated":"2020-04-17T09:08:44.200Z","comments":true,"path":"2020/04/17/setup-ipsan-ubuntu/","link":"","permalink":"http://www.isimble.com/2020/04/17/setup-ipsan-ubuntu/","excerpt":"参考文章 IPSAN的定义就不讲了，主要讲一下搭建环境。IPSAN主要包括两种node： initiator：可以理解为客户端 target：可以理解为服务器端 实际上也不存在客户端服务器端的理解。target端主要是把磁盘share出来给其他设备用的。initiator相当于用来映射target端的磁盘到本地的。","text":"参考文章 IPSAN的定义就不讲了，主要讲一下搭建环境。IPSAN主要包括两种node： initiator：可以理解为客户端 target：可以理解为服务器端 实际上也不存在客户端服务器端的理解。target端主要是把磁盘share出来给其他设备用的。initiator相当于用来映射target端的磁盘到本地的。 安装及配置target端 服务器准备(ip: 192.168.0.100) 服务器需要有两块磁盘，如果使用虚拟机环境，则特别简单 第一块硬盘用来安装系统，大小40G就够了 第二块硬盘用来通过IPSAN来share，按需来创建，最小2G 安装软件包 在Ubuntu上安装target端很简单，主要是安装tgt软件包 12345# 升级软件包$ apt update -y$ apt upgrade -y# 安装tgt$ apt install tgt -y 默认状况下，安装完毕后，tgt服务就应该已经正常运行了。当然，可以确认是否已经运行正常 1$ systemctl status tgt 配置 安装完成后，一般情况是没有这个文件的，需要自己创建一个。创建配置文件/etc/tgt/conf.d/iscsi.conf 123456&lt;target abc.id123.testsite.xyz:lun1&gt; backing-store /dev/sdb initiator-address 192.168.0.0/16 incominguser test password outgoinguser test password&lt;/target&gt; 配置说明 target后面为节点名称，随便写 backing-store，即将要share出去的磁盘 initiator-address，即可以允许连接的IP地址或地址范围 incominguser，入的用户名密码（我的理解是写权限） outgoinguser，出的用户名密码（我的理解是读权限） 检查配置 1$ systemctl restart tgt 查看状态 12345678910111213141516171819202122232425262728293031323334353637383940$ tgtadm --mode target --op showTarget abc.id123.testsite.xyz:lun1 System information: Driver: iscsi State: ready I_T nexus information: LUN information: LUN: 0 Type: controller SCSI ID: IET 00010000 SCSI SN: beaf10 Size: 0 MB, Block size: 1 Online: Yes Removable media: No Prevent removal: No Readonly: No SWP: No Thin-provisioning: No Backing store type: null Backing store path: None Backing store flags: LUN: 1 Type: disk SCSI ID: IET 00010001 SCSI SN: beaf11 Size: 2146 MB, Block size: 512 Online: Yes Removable media: No Prevent removal: No Readonly: No SWP: No Thin-provisioning: No Backing store type: rdwr Backing store path: /dev/sdb Backing store flags: Account information: test test (outgoing) ACL information: 192.168.0.0/16 安装及配置initiator 如果你只是想创建一个IPSAN共享存储出去，那么不需要安装initiator。本例则同样通过另外一台Ubuntu服务器来连接target端 安装及配置initiator(192.168.0.200) 安装open-iscsi软件包 1$ apt-get install open-iscsi -y 探测target端 12$ iscsiadm -m discovery -t st -p 192.168.0.100192.168.0.100:3260,1 abc.id123.testsite.xyz:lun1 修改配置文件 探测完成后，将会在/etc/iscsi/nodes/目录和/etc/iscsi/send_targets/目录下看到对应的target 12345678$ ls -l /etc/iscsi/nodes/abc.id123.testsite.xyz\\:lun1/192.168.0.100\\,3260\\,1/ total 4-rw------- 1 root root 1840 Nov 8 13:17 default$ ls -l /etc/iscsi/send_targets/192.168.0.100,3260/total 8lrwxrwxrwx 1 root root 66 Nov 8 13:17 abc.id123.testsite.xyz:lun1,192.168.0.100,3260,1,default -&gt; /etc/iscsi/nodes/abc.id123.testsite.xyz:lun1/192.168.0.100,3260,1-rw------- 1 root root 547 Nov 8 13:17 st_config 修改default文件，增加CHAP的配置（认证相关） vim /etc/iscsi/nodes/abc.id123.testsite.xyz\\:lun1/192.168.0.100\\,3260\\,1/default，新增如下内容 123456node.session.auth.authmethod = CHAPnode.session.auth.username = testnode.session.auth.password = passwordnode.session.auth.username_in = testnode.session.auth.password_in = passwordnode.startup = automatic 验证 重启服务 1$ systemctl restart open-iscsi 检查是否映射完成 12345678$ lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda 8:0 0 40G 0 disk├─sda1 8:1 0 512M 0 part /boot/efi└─sda2 8:2 0 39.5G 0 part /sdb 8:16 0 120G 0 disk└─sdb1 8:17 0 120G 0 partsr0 11:0 1 1024M 0 rom 其中的sdb即为通过IPSAN连接的磁盘。可以以本地磁盘的方式进行访问 更多内容 可以参看这篇文章来创建多台target，创建LVM并share出去一组成IPSAN存储设备组","categories":[{"name":"cloud","slug":"cloud","permalink":"http://www.isimble.com/categories/cloud/"}],"tags":[{"name":"network","slug":"network","permalink":"http://www.isimble.com/tags/network/"}]},{"title":"Openstack学习 —— 跨主机虚拟机访问2","slug":"openstack-cross-host-vm-2","date":"2019-11-27T06:47:24.000Z","updated":"2019-11-27T06:51:27.597Z","comments":true,"path":"2019/11/27/openstack-cross-host-vm-2/","link":"","permalink":"http://www.isimble.com/2019/11/27/openstack-cross-host-vm-2/","excerpt":"上篇文章梳理了分别在两个node上创建了VM后，底层的Linux系统上的namespace、linux bridge以及ovs中发生的事情。 本文将来着重关注两个node上的VM相互访问的流量通路。特别是令人头疼的ovs流表以及两个node是如何通过VXLAN网络将两台虚拟机连在了一起的。","text":"上篇文章梳理了分别在两个node上创建了VM后，底层的Linux系统上的namespace、linux bridge以及ovs中发生的事情。 本文将来着重关注两个node上的VM相互访问的流量通路。特别是令人头疼的ovs流表以及两个node是如何通过VXLAN网络将两台虚拟机连在了一起的。 br-tun 在前面收集信息时，你或许已经关注到了linux bridge和ovs中还有其他变化，那么，来看看 br-tun ovs 12345678910111213141516171819(node0)$ ovs-vsctl show... Bridge br-tun ... Port \"vxlan-ac0a000b\" Interface \"vxlan-ac0a000b\" type: vxlan options: &#123;df_default=\"true\", in_key=flow, local_ip=\"172.10.0.10\", out_key=flow, remote_ip=\"172.10.0.11\"&#125; ... (node1)$ ovs-vsctl show... Bridge br-tun ... Port \"vxlan-ac0a000a\" Interface \"vxlan-ac0a000a\" type: vxlan options: &#123;df_default=\"true\", in_key=flow, local_ip=\"172.10.0.11\", out_key=flow, remote_ip=\"172.10.0.10\"&#125; ... 当环境搭建完毕后，并不会立即创建这个接口，而是当创建了虚拟机后，会创建VXLAN的VTEP接口。 流表 node0 br-int 1234567891011121314$ ovs-ofctl dump-flows br-int table=0, ..., priority=65535,vlan_tci=0x0fff/0x1fff actions=drop table=0, ..., priority=10,icmp6,in_port=\"qvo02407769-97\",icmp_type=136 actions=resubmit(,24) table=0, ..., priority=10,arp,in_port=\"qvo02407769-97\" actions=resubmit(,24) table=0, ..., priority=2,in_port=\"int-br-provider\" actions=drop table=0, ..., priority=2,in_port=\"int-br-ext\" actions=drop table=0, ..., priority=9,in_port=\"qvo02407769-97\" actions=resubmit(,25) table=0, ..., priority=0 actions=resubmit(,60) table=23, ..., priority=0 actions=drop table=24, ..., priority=2,icmp6,in_port=\"qvo02407769-97\",icmp_type=136,nd_target=fe80::f816:3eff:fead:669e actions=resubmit(,60) table=24, ..., priority=2,arp,in_port=\"qvo02407769-97\",arp_spa=200.0.0.219 actions=resubmit(,25) table=24, ..., priority=0 actions=drop table=25, ..., priority=2,in_port=\"qvo02407769-97\",dl_src=fa:16:3e:ad:66:9e actions=resubmit(,60) table=60, ..., priority=3 actions=NORMAL table 0 从qvo02407769-97送入的icmp，arp报文，送往table 24 从qvo02407769-97送入其他报文送往table 25 其他报文送table 60 table 24 从qvo02407769-97送入的报文，送往table 60。fe80::f816:3eff:fead:669e是vm-1的IPv6地址 从qvo02407769-97送入的arp报文，arp源地址为200.0.0.219即vm-1来的报文，送往table 25 table 25 从qvo02407769-97送入的报文，源mac为fa:16:3e:ad:66:9e即vm-1来的报文，送往table 60 table 60 正常转发 综上，从vm-1来的报文以及其他报文，都将正常在br-int上转发 br-tun 12345678910111213141516$ ovs-ofctl dump-flows br-tun table=0, ..., priority=1,in_port=\"patch-int\" actions=resubmit(,2) table=0, ..., priority=1,in_port=\"vxlan-ac0a000b\" actions=resubmit(,4) table=0, ..., priority=0 actions=drop table=2, ..., priority=0,dl_dst=00:00:00:00:00:00/01:00:00:00:00:00 actions=resubmit(,20) table=2, ..., priority=0,dl_dst=01:00:00:00:00:00/01:00:00:00:00:00 actions=resubmit(,22) table=3, ..., priority=0 actions=drop table=4, ..., priority=1,tun_id=0xf actions=mod_vlan_vid:1,resubmit(,10) table=4, ..., priority=0 actions=drop table=6, ..., priority=0 actions=drop table=10, ..., priority=1 actions=learn(table=20,hard_timeout=300,priority=1,cookie=0x9c915752f14d2544,NXM_OF_VLAN_TCI[0..11],NXM_OF_ETH_DST[]=NXM_OF_ETH_SRC[],load:0-&gt;NXM_OF_VLAN_TCI[],load:NXM_NX_TUN_ID[]-&gt;NXM_NX_TUN_ID[],output:OXM_OF_IN_PORT[]),output:\"patch-int\" table=20, ..., priority=2,dl_vlan=1,dl_dst=fa:16:3e:f5:ca:f5 actions=strip_vlan,load:0xf-&gt;NXM_NX_TUN_ID[],output:\"vxlan-ac0a000b\" table=20, ..., hard_timeout=300, priority=1,vlan_tci=0x0001/0x0fff,dl_dst=fa:16:3e:f5:ca:f5 actions=load:0-&gt;NXM_OF_VLAN_TCI[],load:0xf-&gt;NXM_NX_TUN_ID[],output:\"vxlan-ac0a000b\" table=20, ..., priority=0 actions=resubmit(,22) table=22, ..., priority=1,dl_vlan=1 actions=strip_vlan,load:0xf-&gt;NXM_NX_TUN_ID[],output:\"vxlan-ac0a000b\" table=22, ..., priority=0 actions=drop table 0 从br-int来的报文（即需要从本节点送出的报文），送到table 2 从vxlan-ac0a000b送来的报文（即从其他节点送到node 0的），送到table 4 table 2： 将要送出本节点的报文 送往table 20 送往table 22 table 4： 从其他节点送到本节点的报文 tun_id为15（0xf）的报文，打上vlan标签1，送往table 10 即从vxlan的tunnel出来的报文，如果tunnel id是15，则报文设置vlan为1 而qvo02407769-97（与vm-1相连）和tap17a89323-fd（DHCP的tap接口）的vlan tag正是1 table 10 learn(table=20,hard_timeout=300,priority=1,cookie=0x9c915752f14d2544,NXM_OF_VLAN_TCI[0…11],NXM_OF_ETH_DST[]=NXM_OF_ETH_SRC[],load:0-&gt;NXM_OF_VLAN_TCI[],load:NXM_NX_TUN_ID[]-&gt;NXM_NX_TUN_ID[],output:OXM_OF_IN_PORT[]),output:“patch-int” 这个action是如此的复杂，但不看learn()中的内容，报文最终被送往了br-int learn则是在table20中增加对于回程报文的转发规则 table 20 送往table 22 table 22 VLAN tag为1的报文，去掉VLAN，设置tun_id为15（0xf）后，送往vxlan-ac0a000b 综上： 从vm-1送来的报文，携带VLAN tag为1，去掉VLAN tag，设置tun_id为15，从vxlan接口送出 从其他节点送来的报文，如果tun_id为15，设置VLAN tag为1，送往br-int 总结 完整的分析了node0上的流表，node1上的流表内容基本相似，就不再展开。 至此，跨节点的虚拟机相互访问的实验及分析正式完结。你会发现 每一个port在连接到虚拟机的时候，都创建了一个网桥 每个虚拟机连在br-int上的接口，都按照subnet分配了一个VLAN tag，且每个节点的并不相同 当虚拟机的报文要送出/进入当前节点时，会有VLAN tag和VXLAN的tun_id相互转换 现在，思考一下： node1上的vm-1是如何通过DHCP获取IP地址的？ 为什么虚拟机不直接连在br-int上，而是要通过一个linux bridge连接到br-int上呢？","categories":[{"name":"cloud","slug":"cloud","permalink":"http://www.isimble.com/categories/cloud/"}],"tags":[{"name":"openstack","slug":"openstack","permalink":"http://www.isimble.com/tags/openstack/"}]},{"title":"Openstack学习 —— 跨主机虚拟机访问1","slug":"openstack-cross-host-vm-1","date":"2019-11-22T07:44:38.000Z","updated":"2019-11-22T08:57:27.572Z","comments":true,"path":"2019/11/22/openstack-cross-host-vm-1/","link":"","permalink":"http://www.isimble.com/2019/11/22/openstack-cross-host-vm-1/","excerpt":"前面已经有一篇博文讲解了同主机内的虚拟机通信，本篇则主要关注与跨主机虚拟机访问的详细解析。 创建网络 - 创建虚拟机实例 - 创建路由器 在上一篇文章中已经详细介绍了创建网络后主机上发生的事情。接下来，我们就创建两个虚拟机（nova自动调度到了两个节点上），然后来看看网络部分又发生了哪些事情。","text":"前面已经有一篇博文讲解了同主机内的虚拟机通信，本篇则主要关注与跨主机虚拟机访问的详细解析。 创建网络 - 创建虚拟机实例 - 创建路由器 在上一篇文章中已经详细介绍了创建网络后主机上发生的事情。接下来，我们就创建两个虚拟机（nova自动调度到了两个节点上），然后来看看网络部分又发生了哪些事情。 注：openstack及ovs等CLI输出内容较多，文章中仅列出关注的部分内容 创建虚拟机 在创建虚拟机之前，需要创建规格（flavor）和上传镜像（image） 本文使用cirros作为测试镜像 12345678$ openstack server create --flavor small --image cirros --network net1 --min 2 --max 2 vm$ openstack server list+--------------------------------------+------+--------+------------------+--------+--------+| ID | Name | Status | Networks | Image | Flavor |+--------------------------------------+------+--------+------------------+--------+--------+| ebb1b8fa-6457-4e80-8a49-14093622ce5d | vm-2 | ACTIVE | net1=200.0.0.219 | cirros | small || fb619b6c-9954-4dbd-8b1e-461c1c3c4c7d | vm-1 | ACTIVE | net1=200.0.0.238 | cirros | small |+--------------------------------------+------+--------+------------------+--------+--------+ 发生了什么？ Compute 还是有必要简单介绍一下nova相关的操作 1234567891011121314151617181920$ openstack server show vm-1+-------------------------------------+--------------------------------------+| Field | Value |+-------------------------------------+--------------------------------------+| OS-EXT-SRV-ATTR:hypervisor_hostname | node1 || OS-EXT-SRV-ATTR:instance_name | instance-00000039 || addresses | net1=200.0.0.238 || id | fb619b6c-9954-4dbd-8b1e-461c1c3c4c7d || name | vm-1 |+-------------------------------------+--------------------------------------+$ openstack server show vm-2+-------------------------------------+--------------------------------------+| Field | Value |+-------------------------------------+--------------------------------------+| OS-EXT-SRV-ATTR:hypervisor_hostname | node0 || OS-EXT-SRV-ATTR:instance_name | instance-0000003a || addresses | net1=200.0.0.219 || id | ebb1b8fa-6457-4e80-8a49-14093622ce5d || name | vm-2 |+-------------------------------------+--------------------------------------+ 从实例的信息中看到，分别在node1上创建了vm-1：instance-00000039，在node0上创建了vm-2：instance-0000003a。不妨分别在两个node上使用virsh list看看： 1234(node0)$ virsh list Id Name State----------------------------------- 21 instance-0000003a running 1234(node1)$ virsh list Id Name State----------------------------------- 11 instance-00000039 running Network 12345678$ openstack port list+--------------------------------------+------+-------------------+----------------------------------------------------------------------------+--------+| ID | Name | MAC Address | Fixed IP Addresses | Status |+--------------------------------------+------+-------------------+----------------------------------------------------------------------------+--------+| 02407769-97c6-45ae-8fba-ea464020951c | | fa:16:3e:ad:66:9e | ip_address='200.0.0.219', subnet_id='5875f007-7b0f-45af-b78f-82527e5d9a90' | ACTIVE || 17a89323-fd0a-44cc-a525-fcf8d2efb836 | | fa:16:3e:f6:db:c9 | ip_address='200.0.0.2', subnet_id='5875f007-7b0f-45af-b78f-82527e5d9a90' | ACTIVE || f4c391da-2295-479a-95c5-c7759132f2ea | | fa:16:3e:f5:ca:f5 | ip_address='200.0.0.238', subnet_id='5875f007-7b0f-45af-b78f-82527e5d9a90' | ACTIVE |+--------------------------------------+------+-------------------+----------------------------------------------------------------------------+--------+ 先记住两个port对应的ID： ID IP Instance Node 02407769-97c6-45ae-8fba-ea464020951c 200.0.0.219 vm-1 node0 f4c391da-2295-479a-95c5-c7759132f2ea 200.0.0.238 vm-2 node1 node0 Linux bridge 1234$ brctl showbridge name bridge id STP enabled interfacesqbr02407769-97 8000.02989dedfae5 no qvb02407769-97 tap02407769-97 ovs 12345678$ ovs-vsctl show... Bridge br-int ... Port \"qvo02407769-97\" tag: 1 Interface \"qvo02407769-97\" ... 别忘了留意tag: 1，后面分析时才会用到 MAC地址 123456789$ ip link show161: qbr02407769-97: ... link/ether 02:98:9d:ed:fa:e5 brd ff:ff:ff:ff:ff:ff162: qvo02407769-97@qvb02407769-97: ... link/ether 2a:16:2b:d1:8d:a3 brd ff:ff:ff:ff:ff:ff163: qvb02407769-97@qvo02407769-97: ... link/ether 02:98:9d:ed:fa:e5 brd ff:ff:ff:ff:ff:ff164: tap02407769-97: ... link/ether fe:16:3e:ad:66:9e brd ff:ff:ff:ff:ff:ff vm-1 可以使用virsh console xxx连入虚拟机的console进行操作 1234567891011121314151617$ virsh console instance-0000003aConnected to domain instance-0000003aEscape character is ^]login as 'cirros' user. default password: 'gocubsgo'. use 'sudo' for root.vm-2 login: cirrosPassword:$ ifconfigeth0 Link encap:Ethernet HWaddr FA:16:3E:AD:66:9E inet addr:200.0.0.219 Bcast:200.0.0.255 Mask:255.255.255.0 inet6 addr: fe80::f816:3eff:fead:669e/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1450 Metric:1 RX packets:94 errors:0 dropped:0 overruns:0 frame:0 TX packets:122 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:9192 (8.9 KiB) TX bytes:10571 (10.3 KiB) ... 从上面我们可以看到，分别创建了： 一个linux网桥qbr02407769-97 一组接口对： qvb02407769-97@qvo02407769-97 一个tap接口：tap02407769-97 你或许已经发现了tap02407769-97接口与instance-0000003a的eth0的关系，是的，他们MAC地址相同，也即表示他们实际上就是同一个网络接口。 连接关系整理 将上一篇文章中的dhcp的namespace也合入整个连接关系后： node1 同样收集node1上的各种信息 linux bridge 1234$ brctl showbridge name bridge id STP enabled interfacesqbrf4c391da-22 8000.aa5d4b2ddf79 no qvbf4c391da-22 tapf4c391da-22 ovs 12345678$ ovs-vsctl show... Bridge br-int ... Port \"qvof4c391da-22\" tag: 6 Interface \"qvof4c391da-22\" ... 别忘了留意tag: 6，你也可以思考一下，为什么同一个subnet，node0上是1，node1上是6？ 连接关系 到现在，分布在两个node上的VM与br-int之间的通路已经清晰起来 小结 至此，两台虚拟机已经创建完毕。再加上实验环境介绍中的连接图： 不妨先从vm-1访问一下vm-2，先思考一下两个虚拟机是如何访问的。下一篇文章再继续探讨流量是如何从一个节点的虚拟机经历千山万水到达另外一个节点虚拟机上的。","categories":[{"name":"cloud","slug":"cloud","permalink":"http://www.isimble.com/categories/cloud/"}],"tags":[{"name":"openstack","slug":"openstack","permalink":"http://www.isimble.com/tags/openstack/"}]},{"title":"Openstack学习 —— 创建网络及子网","slug":"openstack-create-network","date":"2019-11-21T00:22:03.000Z","updated":"2019-11-21T08:23:35.866Z","comments":true,"path":"2019/11/21/openstack-create-network/","link":"","permalink":"http://www.isimble.com/2019/11/21/openstack-create-network/","excerpt":"搭建完测试环境后，拉起来个虚拟机看看，操作步骤 创建网络 - 创建虚拟机实例 - 创建路由器 本篇主要关注创建网络时主机上的行为","text":"搭建完测试环境后，拉起来个虚拟机看看，操作步骤 创建网络 - 创建虚拟机实例 - 创建路由器 本篇主要关注创建网络时主机上的行为 注：openstack CLI以及ovs命令行输出信息较多，仅提取关键信息 创建网络及子网 123456789101112131415161718$ openstack network create net1+---------------------------+-------------------------------------| Field | Value +---------------------------+-------------------------------------| id | 2caecd1e-bda7-4a4e-baec-3d192f52aa3b| provider:network_type | vxlan | provider:segmentation_id | 15 +---------------------------+-------------------------------------$ openstack subnet create --network net1 --subnet-range 200.0.0.0/24 sub1+-------------------+-------------------------------------| Field | Value +-------------------+-------------------------------------| allocation_pools | 200.0.0.2-200.0.0.254 | cidr | 200.0.0.0/24 | gateway_ip | 200.0.0.1 | id | 5875f007-7b0f-45af-b78f-82527e5d9a90| network_id | 2caecd1e-bda7-4a4e-baec-3d192f52aa3b+-------------------+------------------------------------- 发生了什么？ 1. openstack net1的id为2caecd1e-bda7-4a4e-baec-3d192f52aa3b sub1的id为5875f007-7b0f-45af-b78f-82527e5d9a90 123456$ openstack port list+--------------------------------------+------+-------------------+--------------------------------------------------------------------------+--------+| ID | Name | MAC Address | Fixed IP Addresses | Status |+--------------------------------------+------+-------------------+--------------------------------------------------------------------------+--------+| 17a89323-fd0a-44cc-a525-fcf8d2efb836 | | fa:16:3e:f6:db:c9 | ip_address='200.0.0.2', subnet_id='5875f007-7b0f-45af-b78f-82527e5d9a90' | ACTIVE |+--------------------------------------+------+-------------------+--------------------------------------------------------------------------+--------+ 在sub1下创建了一个port ID：17a89323-fd0a-44cc-a525-fcf8d2efb836 IP：200.0.0.2 2. ovs 123456789$ ovs-vsctl show... Bridge br-int ... Port \"tap17a89323-fd\" tag: 1 Interface \"tap17a89323-fd\" type: internal ... ovs在br-int下创建了一个port tap17a89323-fd, tag为 1 3. Linux namespace 123456789101112$ ip netnsqdhcp-2caecd1e-bda7-4a4e-baec-3d192f52aa3b (id: 0)$ ip netns exec qdhcp-2caecd1e-bda7-4a4e-baec-3d192f52aa3b ifconfig...tap17a89323-fd: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1450 inet 169.254.169.254 netmask 255.255.0.0 broadcast 169.254.255.255 inet6 fe80::f816:3eff:fef6:dbc9 prefixlen 64 scopeid 0x20&lt;link&gt; ether fa:16:3e:f6:db:c9 txqueuelen 1000 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 5 bytes 446 (446.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 创建了一个qdhcp-2caecd1e-bda7-4a4e-baec-3d192f52aa3b的namespace ovs的br-int上的 tap17a89323-fd的port被放在了这个namespace中。 留意一下接口的IP地址及MAC地址 综上 openstack上创建network并创建一个子网之后，体现为： 创建了一个dhcp的namespace ovs在br-int上创建了dhcp的port，并将其置于dhcp的namespace中","categories":[{"name":"cloud","slug":"cloud","permalink":"http://www.isimble.com/categories/cloud/"}],"tags":[{"name":"openstack","slug":"openstack","permalink":"http://www.isimble.com/tags/openstack/"}]},{"title":"Openstack学习 —— 实验环境介绍","slug":"openstack-env","date":"2019-11-20T22:48:16.000Z","updated":"2019-11-21T07:02:21.269Z","comments":true,"path":"2019/11/21/openstack-env/","link":"","permalink":"http://www.isimble.com/2019/11/21/openstack-env/","excerpt":"本文作为个人学习笔记整理归档。如果你在学习openstack的过程中看到了本文，希望也能为你带来更加深入的理解。 CloudMan的每天5分钟系列作为入门书，确实让我了解了不少东西。一边看书，一边实验并且加以理解，才是更好的理解其中实现的原理。但老话说的，自己理解了和能给别人讲清楚，还是有很大差异。本着试着把里面的内容讲清楚的目的，进行一系列实验来验证，从而提升自己对相关内容的理解。","text":"本文作为个人学习笔记整理归档。如果你在学习openstack的过程中看到了本文，希望也能为你带来更加深入的理解。 CloudMan的每天5分钟系列作为入门书，确实让我了解了不少东西。一边看书，一边实验并且加以理解，才是更好的理解其中实现的原理。但老话说的，自己理解了和能给别人讲清楚，还是有很大差异。本着试着把里面的内容讲清楚的目的，进行一系列实验来验证，从而提升自己对相关内容的理解。 环境概览 Openstack的环境搭建有很多方法，这里仅对我当前环境组网加以说明。 版本说明 Openstack： stein（1 controller node + 1 compute node） Host： Ubuntu 18.04 Network driver： openvswitch 节点说明 node0既做为控制节点，也同时为网络节点、存储节点 node1为计算节点 网桥说明 br-tun和br-int以及其连接关系皆由openstack的neutron-openvswitch-agent自动创建 分别在两个node上手工创建了br-provider并将物理接口绑定其中 在node0上创建br-ext作为外部网络网桥并绑定物理接口 在/etc/neutron/plugins/ml2/openvswitch_agent.ini中配置了bridge_mappings = provider:br-provider,external:br-ext后，重启neutron-openvswitch-agent后，系统会自动创建br-int与相关网桥的连接 tunnel相关配置 tunnel_type当前设置为vxlan local_ip配置于br-provider上","categories":[{"name":"cloud","slug":"cloud","permalink":"http://www.isimble.com/categories/cloud/"}],"tags":[{"name":"openstack","slug":"openstack","permalink":"http://www.isimble.com/tags/openstack/"}]},{"title":"Openstack网络——虚拟机通信实验1","slug":"openstack-vm-traffic1","date":"2019-09-02T00:58:49.000Z","updated":"2019-09-02T09:04:03.245Z","comments":true,"path":"2019/09/02/openstack-vm-traffic1/","link":"","permalink":"http://www.isimble.com/2019/09/02/openstack-vm-traffic1/","excerpt":"Openstack中的虚拟机流量通常被分为东西向和南北向。而测试环境中可能会遇到各种流量问题，搞清楚每种流量的通路有助于出现问题后的快速定位。 计划将分多篇博文，掰开来详细分析openstack中的各种流量路径。 本文以最简单的单节点为例，介绍在同子网下，虚拟机之间相互访问的流量路径","text":"Openstack中的虚拟机流量通常被分为东西向和南北向。而测试环境中可能会遇到各种流量问题，搞清楚每种流量的通路有助于出现问题后的快速定位。 计划将分多篇博文，掰开来详细分析openstack中的各种流量路径。 本文以最简单的单节点为例，介绍在同子网下，虚拟机之间相互访问的流量路径 环境说明 Openstack： stein（all in one） Host： Ubuntu 18.04 Network driver： openvswitch 必要准备 外部网络：MyEx 镜像：cirros flavor： small 实验 同子网虚拟机 创建网络 12$ openstack network create net1$ openstack subnet create --network net1 --subnet-range 200.0.0.0/24 sub1 创建虚拟机 12345678$ openstack server create --flavor small --image cirros --network net1 --min 2 --max 2 vm$ openstack server list+--------------------------------------+------+--------+------------------+--------+--------+| ID | Name | Status | Networks | Image | Flavor |+--------------------------------------+------+--------+------------------+--------+--------+| ba3d4162-6a4d-45dc-9b41-0aa2e2ae0c88 | vm-1 | ACTIVE | net1=200.0.0.16 | cirros | small || d349a3a1-fdae-473c-83be-ac02ab5997f3 | vm-2 | ACTIVE | net1=200.0.0.224 | cirros | small |+--------------------------------------+------+--------+------------------+--------+--------+ 分别创建了： vm-1: 200.0.0.16 vm-2: 200.0.0.224 先确认两个虚机的连通性 发生了什么? linux bridge 1234567$ brctl showbridge name bridge id STP enabled interfacesbrq459c374c-d3 8000.da9618bd3c8d no vxlan-6qbra95f52ba-a2 8000.c26ea2427d47 no qvba95f52ba-a2 tapa95f52ba-a2qbrebeae637-5c 8000.e2ef56b21ffb no qvbebeae637-5c tapebeae637-5c 先不关注brq459c374c-d3的网桥 Openstack 先来记录一下openstack上的port信息 123456789$ openstack port list+--------------------------------------+------+-------------------+----------------------------------------------------------------------------+--------+| ID | Name | MAC Address | Fixed IP Addresses | Status |+--------------------------------------+------+-------------------+----------------------------------------------------------------------------+--------+| a333499b-e5ea-4658-9ec4-cc9b1942d660 | | fa:16:3e:c7:8c:10 | ip_address='192.168.0.2', subnet_id='a5b25645-c5df-486b-9a49-9412eebc2e59' | ACTIVE || a95f52ba-a255-46a7-b3ca-2c870de30e2d | | fa:16:3e:53:d5:50 | ip_address='200.0.0.224', subnet_id='690d8c09-5f55-4b0c-b673-aff967fb0765' | ACTIVE || e013286e-b707-4992-b9e2-4c1f77d465b1 | | fa:16:3e:45:00:dc | ip_address='200.0.0.2', subnet_id='690d8c09-5f55-4b0c-b673-aff967fb0765' | ACTIVE || ebeae637-5c92-4a5d-943c-27a7c3abfe1f | | fa:16:3e:17:60:c2 | ip_address='200.0.0.16', subnet_id='690d8c09-5f55-4b0c-b673-aff967fb0765' | ACTIVE |+--------------------------------------+------+-------------------+----------------------------------------------------------------------------+--------+ id 用途 ip a333499b-e5 外部网络MyEx的DHCP port 192.168.0.2 a95f52ba-a2 vm-2 200.0.0.224 e013286e-b7 租户网络net1的DHCP port 200.0.0.2 ebeae637-5c vm-1 200.0.0.16 网络接口 123456789101112131415161718192021222324252627282930$ ip link show...45: qbra95f52ba-a2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP mode DEFAULT group default qlen 1000 link/ether c2:6e:a2:42:7d:47 brd ff:ff:ff:ff:ff:ff# qbra95f52ba-a2: linux bridge，vm-2相关46: qvoa95f52ba-a2@qvba95f52ba-a2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue master ovs-system state UP mode DEFAULT group default qlen 1000 link/ether 72:63:10:8c:b6:b4 brd ff:ff:ff:ff:ff:ff47: qvba95f52ba-a2@qvoa95f52ba-a2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue master qbra95f52ba-a2 state UP mode DEFAULT group default qlen 1000 link/ether c2:6e:a2:42:7d:47 brd ff:ff:ff:ff:ff:ff# qvoa95f52ba-a2和qvba95f52ba-a2：veth pair48: qbrebeae637-5c: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP mode DEFAULT group default qlen 1000 link/ether e2:ef:56:b2:1f:fb brd ff:ff:ff:ff:ff:ff# qbrebeae637-5c： linux bridge，vm-1相关49: qvoebeae637-5c@qvbebeae637-5c: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue master ovs-system state UP mode DEFAULT group default qlen 1000 link/ether 92:19:d7:5f:b7:0c brd ff:ff:ff:ff:ff:ff50: qvbebeae637-5c@qvoebeae637-5c: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue master qbrebeae637-5c state UP mode DEFAULT group default qlen 1000 link/ether e2:ef:56:b2:1f:fb brd ff:ff:ff:ff:ff:ff# qvoebeae637-5c和qvbebeae637-5c：veth pair51: tapa95f52ba-a2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc fq_codel master qbra95f52ba-a2 state UNKNOWN mode DEFAULT group default qlen 1000 link/ether fe:16:3e:53:d5:50 brd ff:ff:ff:ff:ff:ff# tapa95f52ba-a2：tap接口，通过vnet方式与虚拟机vm-2相连52: tapebeae637-5c: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc fq_codel master qbrebeae637-5c state UNKNOWN mode DEFAULT group default qlen 1000 link/ether fe:16:3e:17:60:c2 brd ff:ff:ff:ff:ff:ff# tapebeae637-5c：tap接口，通过vnet方式与虚拟机vm-1相连53: vxlan-6: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue master brq459c374c-d3 state UNKNOWN mode DEFAULT group default qlen 1000 link/ether da:96:18:bd:3c:8d brd ff:ff:ff:ff:ff:ff# overlay接口54: brq459c374c-d3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP mode DEFAULT group default qlen 1000 link/ether da:96:18:bd:3c:8d brd ff:ff:ff:ff:ff:ff# overlay网桥 上面出现了两个id：a95f52ba-a2和ebeae637-5c，分别对应了openstack的两个port的id 而对于同一个id，有4个前缀：qbr,qvb,qvo,tap: 前缀 说明 qbr Linux网桥 qvb 与qvo互为veth pair，qvb置于qbr网桥中 qvo 与qvo互为veth pair，qvo置于br-int的ovs网桥中 tap tap接口，与虚拟机中的网卡组成veth pair namespace 123$ ip netns listqdhcp-459c374c-d347-4c3d-8dca-7dbb6b403f4f (id: 0)qdhcp-04d375a6-7600-4a62-87ff-9d66a41d15b1 (id: 1) 2个namespace的id分别对应了openstack上的两个network的id 1234567$ openstack network list+--------------------------------------+------+--------------------------------------+| ID | Name | Subnets |+--------------------------------------+------+--------------------------------------+| 04d375a6-7600-4a62-87ff-9d66a41d15b1 | MyEx | a5b25645-c5df-486b-9a49-9412eebc2e59 || 459c374c-d347-4c3d-8dca-7dbb6b403f4f | net1 | 690d8c09-5f55-4b0c-b673-aff967fb0765 |+--------------------------------------+------+--------------------------------------+ namespace中： 1234567891011121314151617181920$ ip netns exec qdhcp-459c374c-d347-4c3d-8dca-7dbb6b403f4f ifconfig...tape013286e-b7: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1450 inet 169.254.169.254 netmask 255.255.0.0 broadcast 169.254.255.255 inet6 fe80::f816:3eff:fe45:dc prefixlen 64 scopeid 0x20&lt;link&gt; ether fa:16:3e:45:00:dc txqueuelen 1000 (Ethernet) RX packets 254 bytes 18301 (18.3 KB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 161 bytes 17139 (17.1 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0$ ip netns exec qdhcp-04d375a6-7600-4a62-87ff-9d66a41d15b1 ifconfig...tapa333499b-e5: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 169.254.169.254 netmask 255.255.0.0 broadcast 169.254.255.255 inet6 fe80::f816:3eff:fec7:8c10 prefixlen 64 scopeid 0x20&lt;link&gt; ether fa:16:3e:c7:8c:10 txqueuelen 1000 (Ethernet) RX packets 153628 bytes 12984859 (12.9 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 151174 bytes 14352784 (14.3 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 每个namespace各有一个接口：tape013286e-b7和tapa333499b-e5 这两个接口因为在namespace中，在ip link show的时候看不到。 其id也分别对应了openstack中的port的id，后面会看到这两个接口出现在ovs的br-int中 openvswitch 12345678910111213141516171819202122232425262728293031323334353637$ ovs-vsctl show479f3788-7afb-48a4-accd-eb173f318715 ... Bridge br-int Controller \"tcp:127.0.0.1:6633\" is_connected: true fail_mode: secure Port int-br-ext Interface int-br-ext type: patch options: &#123;peer=phy-br-ext&#125; Port \"tapa333499b-e5\" tag: 2 Interface \"tapa333499b-e5\" type: internal Port \"tape013286e-b7\" tag: 3 Interface \"tape013286e-b7\" type: internal Port \"qvoebeae637-5c\" tag: 3 Interface \"qvoebeae637-5c\" Port \"qvoa95f52ba-a2\" tag: 3 Interface \"qvoa95f52ba-a2\" Port patch-tun Interface patch-tun type: patch options: &#123;peer=patch-int&#125; Port int-br-provider Interface int-br-provider type: patch options: &#123;peer=phy-br-provider&#125; Port br-int Interface br-int type: internal ovs_version: \"2.11.0\" br-int下有4个port：tapa333499b-e5,tape013286e-b7,qvoebeae637-5c,qvoa95f52ba-a2，这4个接口在前面都有提及 port 说明 tapa333499b-e5 MyEx的dhcp相关，位于qdhcp-04d375a6-76...的namespace中 tape013286e-b7 net1的dhcp相关，位于qdhcp-459c374c-d3...的namespace中 qvoebeae637-5c 通过veth peer接口qvbebeae637-5c与vm-1连接 qvoa95f52ba-a2 通过veth peer接口qvoa95f52ba-a2与vm-2连接 ovs流表 为了方便观察，手动删除了cookie和duration字段 123456789101112131415161718192021$ ovs-ofctl dump-flows br-inttable=0, n_packets=0, n_bytes=0, priority=65535,vlan_tci=0x0fff/0x1fff actions=droptable=0, n_packets=0, n_bytes=0, priority=10,icmp6,in_port=\"qvoebeae637-5c\",icmp_type=136 actions=resubmit(,24)table=0, n_packets=0, n_bytes=0, priority=10,icmp6,in_port=\"qvoa95f52ba-a2\",icmp_type=136 actions=resubmit(,24)table=0, n_packets=1073, n_bytes=45066, priority=10,arp,in_port=\"qvoebeae637-5c\" actions=resubmit(,24)table=0, n_packets=1072, n_bytes=45024, priority=10,arp,in_port=\"qvoa95f52ba-a2\" actions=resubmit(,24)table=0, n_packets=3103, n_bytes=926990, priority=2,in_port=\"int-br-provider\" actions=droptable=0, n_packets=21, n_bytes=5096, priority=2,in_port=\"int-br-ext\" actions=droptable=0, n_packets=18494, n_bytes=1811307, priority=9,in_port=\"qvoebeae637-5c\" actions=resubmit(,25)table=0, n_packets=18499, n_bytes=1811728, priority=9,in_port=\"qvoa95f52ba-a2\" actions=resubmit(,25)table=0, n_packets=2654, n_bytes=849079, priority=3,in_port=\"int-br-ext\",vlan_tci=0x0000/0x1fff actions=mod_vlan_vid:2,resubmit(,60)table=0, n_packets=763992, n_bytes=72539656, priority=0 actions=resubmit(,60)table=23, n_packets=0, n_bytes=0, priority=0 actions=droptable=24, n_packets=0, n_bytes=0, priority=2,icmp6,in_port=\"qvoebeae637-5c\",icmp_type=136,nd_target=fe80::f816:3eff:fe17:60c2 actions=resubmit(,60)table=24, n_packets=0, n_bytes=0, priority=2,icmp6,in_port=\"qvoa95f52ba-a2\",icmp_type=136,nd_target=fe80::f816:3eff:fe53:d550 actions=resubmit(,60)table=24, n_packets=1073, n_bytes=45066, priority=2,arp,in_port=\"qvoebeae637-5c\",arp_spa=200.0.0.16 actions=resubmit(,25)table=24, n_packets=1072, n_bytes=45024, priority=2,arp,in_port=\"qvoa95f52ba-a2\",arp_spa=200.0.0.224 actions=resubmit(,25)table=24, n_packets=0, n_bytes=0, priority=0 actions=droptable=25, n_packets=19555, n_bytes=1855533, priority=2,in_port=\"qvoebeae637-5c\",dl_src=fa:16:3e:17:60:c2 actions=resubmit(,60)table=25, n_packets=19560, n_bytes=1855982, priority=2,in_port=\"qvoa95f52ba-a2\",dl_src=fa:16:3e:53:d5:50 actions=resubmit(,60)table=60, n_packets=963419, n_bytes=92070282, priority=3 actions=NORMAL 先忽略icmp6的流表 table=0 in_port为qvoebeae637-5c和qvoa95f52ba-a2的arp报文送往table=24 in_port为qvoebeae637-5c和qvoa95f52ba-a2的其他报文送往table=25 其他报文送往table=60 table=24 in_port为qvoebeae637-5c和qvoa95f52ba-a2的arp报文，arp_spa分别是200.0.0.224和200.0.0.16的报文，送往table=25 table=25 in_port为qvoebeae637-5c和qvoa95f52ba-a2的报文，目的mac分别为fa:16:3e:17:60:c2和fa:16:3e:53:d5:50的报文，送往table=60 table=60 正常转发 通过上面一系列的流表，in_port为qvoebeae637-5c和qvoa95f52ba-a2的报文基本上都会在br-int上正常转发 梳理一下 vm通过vnet与一个tap接口相连 tap接口与qvbxxx接口置于一个linux网桥中 qvbxxx的veth peer置于br-int的ovs网桥中 dhcp服务位于linux namespace中，使用了一个tap接口，而此tap接口同时位于br-int的ovs网桥中 报文跟踪 vm之间互通 分析 根据上面的图我们可以看到，vm-1访问vm-2的流量路径为： tapebeae637-5c qbrebeae637-5c qvbebeae637-5c qvoebeae637-5c br-int qvoa95f52ba-a2 qvba95f52ba-a2 qbra95f52ba-a2 tapa95f52ba-a2 抓包 实际上这个抓包很无聊，因为每个上面看到的报文都是一样的，这里只列举其中一个的结果 1234567$ tcpdump -i qvoebeae637-5ctcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on qvoebeae637-5c, link-type EN10MB (Ethernet), capture size 262144 bytes15:42:19.320342 IP 200.0.0.224 &gt; 200.0.0.16: ICMP echo request, id 48897, seq 20449, length 6415:42:19.321219 IP 200.0.0.16 &gt; 200.0.0.224: ICMP echo reply, id 48897, seq 20449, length 6415:42:20.321525 IP 200.0.0.224 &gt; 200.0.0.16: ICMP echo request, id 48897, seq 20450, length 6415:42:20.322566 IP 200.0.0.16 &gt; 200.0.0.224: ICMP echo reply, id 48897, seq 20450, length 64 vm-1访问dhcp的port 分析 而访问dhcp的通路，前面到达br-int的报文都一样，到了br-int后，会到达对应的namespace中 1. 在vm-1上ping 200.0.0.2 2. 在ovs中抓包 123456$ ovs-tcpdump -i tape013286e-b7tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on ovsmi644603, link-type EN10MB (Ethernet), capture size 262144 bytes16:00:35.469101 IP6 :: &gt; ff02::16: HBH ICMP6, multicast listener report v2, 1 group record(s), length 2816:00:35.552469 IP 200.0.0.16 &gt; 200.0.0.2: ICMP echo request, id 48129, seq 480, length 6416:00:35.552507 IP 200.0.0.2 &gt; 200.0.0.16: ICMP echo reply, id 48129, seq 480, length 64 3. 在namespace中抓包 注意，在namespace中抓包，报文信息可能不会及时打印在屏幕上 12345$ ip netns exec qdhcp-459c374c-d347-4c3d-8dca-7dbb6b403f4f tcpdump -i tape013286e-b7tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on tape013286e-b7, link-type EN10MB (Ethernet), capture size 262144 bytes16:02:00.643387 IP 200.0.0.16 &gt; 200.0.0.2: ICMP echo request, id 48129, seq 565, length 6416:02:00.643461 IP 200.0.0.2 &gt; 200.0.0.16: ICMP echo reply, id 48129, seq 565, length 64 More 你或许已经发现了，在namespace中的接口的IP为169.254.169.254，这个地址是干嘛的？那么200.0.0.2到底在哪儿呢？ 169.254.169.254 在openstack中，你会经常看到这个IP地址，它是metadata service的IP 大多数cloud os实例启动时，都会向这个IP地址发起请求，获取一些信息，如以下实例启动日志中，获取public-keys和user-data 1234567891011121314Starting network...udhcpc (v1.23.2) startedSending discover...Sending select for 200.0.0.224...Lease of 200.0.0.224 obtained, lease time 86400route: SIOCADDRT: File existsWARN: failed: route add -net \"0.0.0.0/0\" gw \"200.0.0.1\"checking http://169.254.169.254/2009-04-04/instance-idfailed 1/20: up 24.66. request failedsuccessful after 2/20 tries: up 37.29. iid=i-00000021failed to get http://169.254.169.254/2009-04-04/meta-data/public-keyswarning: no ec2 metadata for public-keysfailed to get http://169.254.169.254/2009-04-04/user-datawarning: no ec2 metadata for user-data 200.0.0.2在哪儿？ 1234$ ps -aux | grep dnsmasq...nobody 3449 0.0 0.0 53332 2588 ? S 09:49 0:00 dnsmasq --no-hosts --no-resolv --pid-file=/var/lib/neutron/dhcp/459c374c-d347-4c3d-8dca-7dbb6b403f4f/pid --dhcp-hostsfile=/var/lib/neutron/dhcp/459c374c-d347-4c3d-8dca-7dbb6b403f4f/host --addn-hosts=/var/lib/neutron/dhcp/459c374c-d347-4c3d-8dca-7dbb6b403f4f/addn_hosts --dhcp-optsfile=/var/lib/neutron/dhcp/459c374c-d347-4c3d-8dca-7dbb6b403f4f/opts --dhcp-leasefile=/var/lib/neutron/dhcp/459c374c-d347-4c3d-8dca-7dbb6b403f4f/leases --dhcp-match=set:ipxe,175 --dhcp-userclass=set:ipxe6,iPXE --local-service --bind-interfaces --dhcp-range=set:tag0,200.0.0.0,static,255.255.255.0,86400s --dhcp-option-force=option:mtu,1450 --dhcp-lease-max=256 --conf-file= --domain=openstacklocal... --dhcp-leasefile指向了/var/lib/neutron/dhcp/459c374c-d347-4c3d-8dca-7dbb6b403f4f/leases，这个文件记录了dhcp分配的IP地址： 1234$ cat /var/lib/neutron/dhcp/459c374c-d347-4c3d-8dca-7dbb6b403f4f/leases1567498384 fa:16:3e:17:60:c2 200.0.0.16 host-200-0-0-16 01:fa:16:3e:17:60:c21567475691 fa:16:3e:53:d5:50 200.0.0.224 host-200-0-0-224 01:fa:16:3e:53:d5:501567475394 fa:16:3e:45:00:dc 200.0.0.2 host-200-0-0-2 * 200.0.0.2的mac地址是fa:16:3e:45:00:dc，而这个mac正是tape013286e-b7的物理地址 12345678910$ ip netns exec qdhcp-459c374c-d347-4c3d-8dca-7dbb6b403f4f ifconfig...tape013286e-b7: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1450 inet 169.254.169.254 netmask 255.255.0.0 broadcast 169.254.255.255 inet6 fe80::f816:3eff:fe45:dc prefixlen 64 scopeid 0x20&lt;link&gt; ether fa:16:3e:45:00:dc txqueuelen 1000 (Ethernet) RX packets 1639 bytes 130817 (130.8 KB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 1546 bytes 149118 (149.1 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 小结 在单节点状况下，同子网下的虚拟机相互访问，是一个非常简单的路径。 通过tap接口、veth pair、linux网桥、ovs网桥以及ovs流表来实现了流量通路。","categories":[{"name":"openstack","slug":"openstack","permalink":"http://www.isimble.com/categories/openstack/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://www.isimble.com/tags/linux/"},{"name":"openstack","slug":"openstack","permalink":"http://www.isimble.com/tags/openstack/"},{"name":"neutron","slug":"neutron","permalink":"http://www.isimble.com/tags/neutron/"}]},{"title":"Linux虚拟网络之虚拟机","slug":"linux-virtual-vm","date":"2019-08-15T23:06:26.000Z","updated":"2019-08-16T07:15:29.564Z","comments":true,"path":"2019/08/16/linux-virtual-vm/","link":"","permalink":"http://www.isimble.com/2019/08/16/linux-virtual-vm/","excerpt":"本文将通过实验来理解Linux上的虚拟机如何通过tap、bridge、router以及iptables实现相关的网络访问功能。 环境准备 系统： Ubuntu 16.04.6 LTS","text":"本文将通过实验来理解Linux上的虚拟机如何通过tap、bridge、router以及iptables实现相关的网络访问功能。 环境准备 系统： Ubuntu 16.04.6 LTS 实验topo 软件包 1. qemu 由于整套实验环境都是在虚拟机上完成，在没有开启诸如Intel-VTx技术的情况下，使用qemu作为虚拟机模拟器 123$ apt install qemu$ apt install uml-utilities$ apt install bridge-utils 2. 虚拟机镜像 本文使用cirros(当前版本：0.4.0)作为虚拟机镜像 12$ wget http://download.cirros-cloud.net/0.4.0/cirros-0.4.0-x86_64-disk.img$ cp cirros-0.4.0-x86_64-disk.img cirros.img 3. vncserver 使用qemu启动cirros需要用到图形界面，在纯命令行模式启动，需要使用vncserver 1$ apt install tightvncserver 4. 环境检查 先拉起个虚机试试 1$ qemu-system-x86_64 -vnc 10.180.13.232:99 cirros-0.4.0-x86_64-disk.img 5. 测试 用vnc viewer连接当前虚拟机 tap接口使用 1. 创建虚拟机 1(host)$ qemu-system-x86_64 -vnc 10.180.13.232:99 cirros-0.4.0-x86_64-disk.img -netdev tap,id=mynet0,ifname=tap1,script=no,downscript=no -device e1000,netdev=mynet0 创建成功后，可以看到，自动创建了一个tap1的接口，但没有up 123456789(host)$ ifconfig -a...tap1 Link encap:Ethernet HWaddr 7a:d1:ba:68:06:b2 BROADCAST MULTICAST MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)... 2. 测试连通性 配置host的tap1接口IP 1(host)$ ifconfig tap1 192.168.1.1/24 up 配置虚拟机的IP 1(vm)$ sudo ifconfig eth0 192.168.1.2 ping 1234(vm)$ ping 192.168.1.2PING 192.168.1.2 (192.168.1.2) 56(84) bytes of data.64 bytes from 192.168.1.2: icmp_seq=1 ttl=64 time=16.6 ms64 bytes from 192.168.1.2: icmp_seq=2 ttl=64 time=1.51 ms 3. 小结 使用tap类型的netdev创建qemu虚拟机时，会自动创建一个tap接口，而这个tap接口与VM的接口相连。 连接图如下： 虚拟机通信 理解了虚拟机使用tap接口之后，就可以考虑使用网桥方式进行2个甚至多个虚拟机之间通信了。 1. 创建虚拟机 先拉起4个VM，分别指定网络 注意，必须为每个虚机指定MAC地址，否则，拉起的虚机将使用同样的MAC地址 12(host)$ qemu-system-x86_64 -vnc 10.180.13.232:10 cirros.img -netdev tap,id=mynet0,ifname=tap0,script=no,downscript=no -device e1000,netdev=mynet0,mac=52:54:98:76:54:30 &amp;(host)$ qemu-system-x86_64 -vnc 10.180.13.232:11 cirros.img -netdev tap,id=mynet1,ifname=tap1,script=no,downscript=no -device e1000,netdev=mynet1,mac=52:54:98:76:54:31 &amp; 查看Host网卡 123456(host)$ ip link show...46: tap0: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether 42:47:1b:de:8e:07 brd ff:ff:ff:ff:ff:ff47: tap1: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether f2:23:b1:c5:55:d3 brd ff:ff:ff:ff:ff:ff 可以看到随虚拟机启动，创建了4个tap接口 2. 配置IP VM1 1(vm1)$ sudo ifconfig eth0 192.168.1.10 VM2 1(vm2)$ sudo ifconfig eth0 192.168.1.11 3. 创建网桥 Host 123456(host)$ brctl addbr br0(host)$ brctl addif br0 tap0(host)$ brctl addif br0 tap1(host)$ bridge name bridge id STP enabled interfaces br0 8000.42471bde8e07 no tap0 tap1 尝试ping一下，发现还是ping不通。因为对应的tap接口及br0没有up 123(host)$ ifconfig br0 up(host)$ ifconfig tap0 up(host)$ ifconfig tap1 up 现在，可以两个VM相互ping一下了 4. 访问外部PC 虚拟机需要访问外部网络，则需要添加物理接口，如下图 Host 12(host)$ ifconfig ens192 up(host)$ brctl addif br0 ens192 VM1 1234(vm1)$ ping 192.168.1.3PING 192.168.1.3 (192.168.1.3): 56 data bytes64 bytes from 192.168.1.3: seq=0 ttl=64 time=29.700 ms... 抓个包看看 - ens192 12345(host)$ tcpdump -i ens192tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on ens192, link-type EN10MB (Ethernet), capture size 262144 bytes11:15:02.317164 IP 192.168.1.11 &gt; 192.168.1.3: ICMP echo request, id 47873, seq 0, length 6411:15:02.317527 IP 192.168.1.3 &gt; 192.168.1.11: ICMP echo reply, id 47873, seq 0, length 64 5. 小结 本节主要实验了虚拟机通过网桥进行相互通信及访问外网。对应了qemu网络的基于网桥的虚拟网卡 NAT模式 Host需要开启转发功能 临时修改：echo &quot;1&quot; &gt; /proc/sys/net/ipv4/ip_forward NAT模式主要通过iptables来实现。通过将虚拟机发出的报文的源地址转换为物理接口的IP，实现与外网的通信 拓扑图如下： 1. 为网桥配置IP地址 用网桥的br0接口作为192.168.1.0/24网段的网关 1(host)$ ifconfig br0 192.168.1.1/24 测试host与外部网络连通性 需要确保host有通往外部网络的路由 123(host)$ ping 223.5.5.5PING 223.5.5.5 (223.5.5.5) 56(84) bytes of data.64 bytes from 223.5.5.5: icmp_seq=1 ttl=115 time=11.7 ms 2. 配置iptables 将192.168.1.0/24网段转换为发送出去的接口IP MASQUERADE: 用发送数据的网卡上的IP来替换源IP 1(host)$ iptables -t nat -A POSTROUTING -s 192.168.1.0/24 ! -d 192.168.1.0/24 -j MASQUERADE 3. 虚拟机配置默认路由 1(vm1)$ route add -net 0.0.0.0/0 gw 192.168.1.1 4. 测试连通性 1234(vm1)$ ping 223.5.5.5PING 223.5.5.5 (223.5.5.5): 56 data bytes64 bytes from 223.5.5.5: seq=0 ttl=114 time=14.161 ms... 5. 抓包看看 tap1 123456(host)$ tcpdump -i tap0 -ntcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on tap0, link-type EN10MB (Ethernet), capture size 262144 bytes14:33:41.158181 IP 192.168.1.10 &gt; 223.5.5.5: ICMP echo request, id 53761, seq 0, length 6414:33:41.332908 IP 223.5.5.5 &gt; 192.168.1.10: ICMP echo reply, id 53761, seq 0, length 64... tap1接口上体现了虚拟机的请求： 192.168.1.10 - 223.5.5.5 br0 12345$ tcpdump -i br0 -ntcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on br0, link-type EN10MB (Ethernet), capture size 262144 bytes14:35:05.964841 IP 192.168.1.10 &gt; 223.5.5.5: ICMP echo request, id 54017, seq 0, length 6414:35:06.028936 IP 223.5.5.5 &gt; 192.168.1.10: ICMP echo reply, id 54017, seq 0, length 64 网桥接口br0上的报文于tap0相同，既当前并没有修改报文 ens160 12345$ tcpdump -i ens160 -n host 223.5.5.5tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on ens160, link-type EN10MB (Ethernet), capture size 262144 bytes14:36:22.762002 IP 10.180.13.232 &gt; 223.5.5.5: ICMP echo request, id 54273, seq 0, length 6414:36:22.775182 IP 223.5.5.5 &gt; 10.180.13.232: ICMP echo reply, id 54273, seq 0, length 64 可以看到在ens160接口上，IP则变成了10.180.13.232 - 223.5.5.5之间的通信 6. 小结 在本节中，当虚拟机访问外部网络时，从虚拟机内部发出的报文，在主机上通过查路由的方式转发到外部网络中。而在转发出去之前，将源IP地址修改为当前主机的接口IP","categories":[{"name":"network","slug":"network","permalink":"http://www.isimble.com/categories/network/"}],"tags":[{"name":"network","slug":"network","permalink":"http://www.isimble.com/tags/network/"},{"name":"linux","slug":"linux","permalink":"http://www.isimble.com/tags/linux/"}]},{"title":"西班牙申根签证快速攻略","slug":"spain-visa","date":"2019-08-14T17:22:22.000Z","updated":"2019-08-15T01:34:57.170Z","comments":true,"path":"2019/08/15/spain-visa/","link":"","permalink":"http://www.isimble.com/2019/08/15/spain-visa/","excerpt":"适用对象 北京领区 在职人员 旅游签证 2019年 小提醒 签证中心只收现金 据签证中心工作人员介绍，周一和周五人最多，其他时间都还好 只需一张照片，但多带几张总不坏 流水余额取决于旅行时间，最少3W吧","text":"适用对象 北京领区 在职人员 旅游签证 2019年 小提醒 签证中心只收现金 据签证中心工作人员介绍，周一和周五人最多，其他时间都还好 只需一张照片，但多带几张总不坏 流水余额取决于旅行时间，最少3W吧 主要网站 西班牙签证申请中心：https://china.blsspainvisa.com/chinese/index.php 流程 准备资料 预约 前往签证中心 坐等快递 资料清单 官网文件 来源：西班牙签证申请中心官网 获取：首页 -&gt; 签证类型 -&gt; 短期 -&gt; 旅游。或直接点击这里 检查清单（Checklist） 签证申请表 知悉声明 准备文件 相关填写及说明参看后续章节 准备文件均来源于检查清单中 工作相关（必须） 公司营业执照复应件，需盖章 在职证明（中文，英文各一份），可参看在职证明模版 任职公司的地址、电话及传真号码 任职公司签字人的姓名和职务 申请人姓名、职务、收入及工作年限 旅行资料（必须） 完整旅行计划 全部机票预订单 所有住宿证明 旅行保险（投保金额至少30000 欧元或等值的人民币） 身份证明（必须） 护照原件（如有旧护照，一并提供） 整本护照复印件（首页需要复印两张） 户口本原件 户口本所有页复印件 照片 收入类（必须） 工资卡3-6个月的流水，无需存款证明 收入证明（包含于在职证明） 辅助资料（非必须） 房产证及复印件 机动车产权证 预约 在西班牙签证申请中心官网： 申请预约 -&gt; BLS签证中心 -&gt; 网上预约，或直接点击这里 预约表单填写 基本信息 预约申请表 预约确认信 预约完成后，注意保存预约确认信，可打印也可保存在手机上 前往签证中心 地址 签证中心楼下并没有明显的标示 北京市朝阳区新源里16号琨莎中心 1号楼1006室 费用 签证中心当前只收现金 签证费：469 服务费：121 快递费：60 流程 排队 前台检查资料，填写快递单 拿号 递交材料 缴费 录指纹+拍照 回执 离开签证中心时，你手里应该有： 缴费回执（上含受理号） 一张护照首页复印件（上含受理号） 其他文件原件（如户口本、房产证等） 快递单照片（没有原件） 坐等快递 提交完所有资料后，你只需要在家等待签证的快递，是否通过，已经由不得你了。 查询进度 可以登陆西班牙签证申请中心官网，通过受理号在线查询您的申请 在职证明模版 在职证明需要有公司名称、地址、电话并加盖公章 中文 兹证明张三，护照号：E12345678，自xxxx/xx/xx日入职至今，现在我司任职xxx，月收入税前人民币：xxxx元，大写： xxxx。经公司批准，他将于xxxx/xx/xx至xxxx/xx/xx休假。旅行期间的费用由他本人承担。 他的假期期间，我公司将保留他的职务。贵处如需要其他信息，请随时联系我们。如果他的签证申请能顺利通过，我们将非常感激。 英文 Employment Certificate To Spain Visa Application Center: This is to certify that SAN ZHANG(Passport No.: E12345678) works for xxxxxxxx Co., Ltd. Since xxxx/xx/xx till present. He has been allowed for a leave from xxxx/xx/xx to xxxx/xx/xx. His position is xxxxxxxx and average monthly pre-tax salary is xxxxx RMB. All costs relating to his trip will be covered by himself. We will keep his position during his leave. Meanwhile, we guarantee that during this trip he will abide laws of your country and be back as scheduled. If any other information is needed, please feel free to contact us. It will be grateful if his visa is issued successfully. 最后，祝君好运","categories":[{"name":"旅行","slug":"旅行","permalink":"http://www.isimble.com/categories/%E6%97%85%E8%A1%8C/"}],"tags":[{"name":"旅行","slug":"旅行","permalink":"http://www.isimble.com/tags/%E6%97%85%E8%A1%8C/"},{"name":"visa","slug":"visa","permalink":"http://www.isimble.com/tags/visa/"}]},{"title":"Openstack安全组基于ovs的学习笔记","slug":"openstack-security-group-ovs-1","date":"2019-08-08T01:05:40.000Z","updated":"2019-08-12T04:36:36.842Z","comments":true,"path":"2019/08/08/openstack-security-group-ovs-1/","link":"","permalink":"http://www.isimble.com/2019/08/08/openstack-security-group-ovs-1/","excerpt":"本文主要介绍Openstack的安全组在规则及其在ovs的流表中的体现。 实验基于openstack的stein版本，devstack安装，采用了ovs作为虚拟交换机。描述Openflow在Openstack安全组中的应用分析。实验环境以Ubuntu 18.04.3 LTS搭建","text":"本文主要介绍Openstack的安全组在规则及其在ovs的流表中的体现。 实验基于openstack的stein版本，devstack安装，采用了ovs作为虚拟交换机。描述Openflow在Openstack安全组中的应用分析。实验环境以Ubuntu 18.04.3 LTS搭建 Openstack安全组 1. 安全组 在Openstack中，安全组是做哦那个在neutron port上的一组策略，这些策略可以理解为一些防火墙的规则集合。 2. 安全组的实现 Openstack中的安全组的实现有以下集中： ovs + iptables + connection track ovs + openflow + connection track linuxbridge + iptables + connection track 由于本文实验环境基于devstack（stein），因此，仅基于第二种情况进行说明 实验准备 1. 创建项目及用户（非必须） 创建一个test的project和user 123$ openstack project create --domain default --description \"Security Group Test Project\" test$ openstack user create --domain default --password-prompt test$ openstack role add --project test --user test admin 2. 查看test租户的默认安全组下的规则 以test用户登陆openstack的dashboard，在网络 &gt; 安全组 下找到default安全组，查看对应的规则 安全组实验 需要事先创建一个虚拟机示例，默认关联default的安全组 0. 获取一些信息 端口ID 123456$ openstack port list+--------------------------------------+------+-------------------+-----------------------------------------------------------------------------------------------------+--------+| ID | Name | MAC Address | Fixed IP Addresses | Status |+--------------------------------------+------+-------------------+-----------------------------------------------------------------------------------------------------+--------+| 41ca5359-2edc-4d74-8321-5883ecb618c5 | | fa:16:3e:fa:fd:13 | ip_address='192.168.233.49', subnet_id='ef4d1362-24b5-4d01-8748-ffe9cc2ca2e5' | ACTIVE |+--------------------------------------+------+-------------------+-----------------------------------------------------------------------------------------------------+--------+ 创建了一个ID为41ca5359-2edc-4d74-8321-5883ecb618c5的port，分配IP地址为192.168.233.49 接口名称 123456789101112$ ovs-vsctl show... Bridge br-int Controller \"tcp:127.0.0.1:6633\" is_connected: true ... Port \"tap41ca5359-2e\" tag: 3 Interface \"tap41ca5359-2e\" ... 对应的创建了一个tap41ca5359-2e的接口 1. 创建一条ingress的规则 通过Dashboard在default的安全组下创建一条ingress的规则，remote-ip为199.0.0.0/24，协议为tcp，目的端口为80 查看br-int流表 br-int下的流表非常的多，可以使用过滤规则进行匹配 123$ ovs-ofctl dump-flows br-int | grep 199.0.0.0 cookie=0xd9177c011dbdf439, duration=98.493s, table=82, n_packets=0, n_bytes=0, idle_age=98, priority=77,ct_state=+est-rel-rpl,tcp,reg5=0xe,nw_src=199.0.0.0/24,tp_dst=80 actions=output:14 cookie=0xd9177c011dbdf439, duration=98.493s, table=82, n_packets=0, n_bytes=0, idle_age=98, priority=77,ct_state=+new-est,tcp,reg5=0xe,nw_src=199.0.0.0/24,tp_dst=80 actions=ct(commit,zone=NXM_NX_REG6[0..15]),output:14,resubmit(,92) 可以看到，共新增了两条流表 3. 创建一条egress的规则 同样以remote-ip为199.0.0.0/24，协议为tcp，目的端口为80的参数创建egress规则 查看流表看到新增了如下两条： 12cookie=0xd9177c011dbdf439, duration=19.261s, table=72, n_packets=0, n_bytes=0, idle_age=19, priority=77,ct_state=+est-rel-rpl,tcp,reg5=0xe,nw_dst=199.0.0.0/24,tp_dst=80 actions=resubmit(,73)cookie=0xd9177c011dbdf439, duration=19.261s, table=72, n_packets=0, n_bytes=0, idle_age=19, priority=77,ct_state=+new-est,tcp,reg5=0xe,nw_dst=199.0.0.0/24,tp_dst=80 actions=resubmit(,73) 4. 流表解读 4.1 ingress stpe2中的流表，分别指定了tcp协议，nw_src为199.0.0.0/24，tp_dst为80 step2的action，第一条为直接从14的接口送出，第二条为重定向到了92 先来看看ID为14的接口是哪个？不出所料，自然是tap41ca5359-2e的接口 123456789$ ovs-ofctl show br-int... 14(tap41ca5359-2e): addr:fe:16:3e:fa:fd:13 config: 0 state: 0 current: 10MB-FD COPPER speed: 10 Mbps now, 0 Mbps max ... table 92 丢弃 1cookie=0xd9177c011dbdf439, duration=3197934.551s, table=92, n_packets=0, n_bytes=0, priority=0 actions=drop 以上可看到，源地址为199.0.0.0/24，目的端口为80的TCP报文大多数情况将送往tap41ca5359-2e的接口 4.2 egress step3中的流表，则分别指定了tcp协议，nw_dst为199.0.0.0/24，tp_dst为80 step3的action则送往了73的table，而73则非常复杂，又涉及到了81，91，94的table 123456789101112131415cookie=0xd9177c011dbdf439, duration=1305.051s, table=73, n_packets=10, n_bytes=1345, priority=100,reg6=0x3,dl_dst=fa:16:3e:fa:fd:13 actions=load:0xe-&gt;NXM_NX_REG5[],resubmit(,81)cookie=0xd9177c011dbdf439, duration=1305.051s, table=73, n_packets=5, n_bytes=438, priority=90,ct_state=+new-est,ip,reg5=0xe actions=ct(commit,zone=NXM_NX_REG6[0..15]),resubmit(,91)cookie=0xd9177c011dbdf439, duration=1305.051s, table=73, n_packets=0, n_bytes=0, priority=90,ct_state=+new-est,ipv6,reg5=0xe actions=ct(commit,zone=NXM_NX_REG6[0..15]),resubmit(,91)cookie=0xd9177c011dbdf439, duration=3197929.613s, table=73, n_packets=0, n_bytes=0, priority=80,reg5=0x6 actions=resubmit(,94)cookie=0xd9177c011dbdf439, duration=3197929.613s, table=73, n_packets=0, n_bytes=0, priority=80,reg5=0x3 actions=resubmit(,94)cookie=0xd9177c011dbdf439, duration=3197929.613s, table=73, n_packets=0, n_bytes=0, priority=80,reg5=0x4 actions=resubmit(,94)cookie=0xd9177c011dbdf439, duration=3197929.613s, table=73, n_packets=0, n_bytes=0, priority=80,reg5=0x7 actions=resubmit(,94)cookie=0xd9177c011dbdf439, duration=3197929.613s, table=73, n_packets=0, n_bytes=0, priority=80,reg5=0x5 actions=resubmit(,94)cookie=0xd9177c011dbdf439, duration=3197849.617s, table=73, n_packets=5, n_bytes=446, priority=80,reg5=0x9 actions=resubmit(,94)cookie=0xd9177c011dbdf439, duration=3197849.617s, table=73, n_packets=2, n_bytes=180, priority=80,reg5=0x8 actions=resubmit(,94)cookie=0xd9177c011dbdf439, duration=3197817.084s, table=73, n_packets=11, n_bytes=778, priority=80,reg5=0xa actions=resubmit(,94)cookie=0xd9177c011dbdf439, duration=3197815.612s, table=73, n_packets=49080, n_bytes=5791412, priority=80,reg5=0xb actions=resubmit(,94)cookie=0xd9177c011dbdf439, duration=3197815.612s, table=73, n_packets=8, n_bytes=648, priority=80,reg5=0xc actions=resubmit(,94)cookie=0xd9177c011dbdf439, duration=1305.051s, table=73, n_packets=6, n_bytes=1078, priority=80,reg5=0xe actions=resubmit(,94)cookie=0xd9177c011dbdf439, duration=3197934.616s, table=73, n_packets=0, n_bytes=0, priority=0 actions=drop table 91 重定向到了94 1cookie=0xd9177c011dbdf439, duration=3197934.561s, table=91, n_packets=10, n_bytes=876, priority=1 actions=resubmit(,94) table 94 正常转发 1cookie=0xd9177c011dbdf439, duration=3197934.572s, table=94, n_packets=49280, n_bytes=5803120, priority=1 actions=NORMAL table 81 送往tap41ca5359-2e的接口 12345678910cookie=0xd9177c011dbdf439, duration=1305.051s, table=81, n_packets=3, n_bytes=126, priority=100,arp,reg5=0xe actions=output:\"tap41ca5359-2e\"cookie=0xd9177c011dbdf439, duration=1305.051s, table=81, n_packets=0, n_bytes=0, priority=100,icmp6,reg5=0xe,icmp_type=130 actions=output:\"tap41ca5359-2e\"cookie=0xd9177c011dbdf439, duration=1305.051s, table=81, n_packets=0, n_bytes=0, priority=100,icmp6,reg5=0xe,icmp_type=135 actions=output:\"tap41ca5359-2e\"cookie=0xd9177c011dbdf439, duration=1305.051s, table=81, n_packets=0, n_bytes=0, priority=100,icmp6,reg5=0xe,icmp_type=136 actions=output:\"tap41ca5359-2e\"cookie=0xd9177c011dbdf439, duration=1305.051s, table=81, n_packets=2, n_bytes=729, priority=95,udp,reg5=0xe,tp_src=67,tp_dst=68 actions=output:\"tap41ca5359-2e\"cookie=0xd9177c011dbdf439, duration=1305.051s, table=81, n_packets=0, n_bytes=0, priority=95,udp6,reg5=0xe,tp_src=547,tp_dst=546 actions=output:\"tap41ca5359-2e\"cookie=0xd9177c011dbdf439, duration=1305.051s, table=81, n_packets=5, n_bytes=490, priority=90,ct_state=-trk,ip,reg5=0xe actions=ct(table=82,zone=NXM_NX_REG6[0..15])cookie=0xd9177c011dbdf439, duration=1305.051s, table=81, n_packets=0, n_bytes=0, priority=90,ct_state=-trk,ipv6,reg5=0xe actions=ct(table=82,zone=NXM_NX_REG6[0..15])cookie=0xd9177c011dbdf439, duration=1305.051s, table=81, n_packets=0, n_bytes=0, priority=80,ct_state=+trk,reg5=0xe actions=resubmit(,82)cookie=0xd9177c011dbdf439, duration=3197934.606s, table=81, n_packets=0, n_bytes=0, priority=0 actions=drop 由上可看到，送往199.0.0.0/24，目的端口为80的TCP报文大多数情况将正常转发出去","categories":[{"name":"openstack","slug":"openstack","permalink":"http://www.isimble.com/categories/openstack/"}],"tags":[{"name":"ovs","slug":"ovs","permalink":"http://www.isimble.com/tags/ovs/"},{"name":"openstack","slug":"openstack","permalink":"http://www.isimble.com/tags/openstack/"}]},{"title":"Linux虚拟网络学习笔记","slug":"linux-virtual-network-basic","date":"2019-08-07T01:05:40.000Z","updated":"2019-08-12T03:42:30.737Z","comments":true,"path":"2019/08/07/linux-virtual-network-basic/","link":"","permalink":"http://www.isimble.com/2019/08/07/linux-virtual-network-basic/","excerpt":"Linux虚拟网络是近几年虚拟化及容器等技术的基础，掌握了这些基础，可以更深入的理解openstack、docker的网络功能，以及测试过程中的问题定位。 本文通过实验的方式学习这些虚拟网络功能，包括tap设备，veth pair， bridge及router","text":"Linux虚拟网络是近几年虚拟化及容器等技术的基础，掌握了这些基础，可以更深入的理解openstack、docker的网络功能，以及测试过程中的问题定位。 本文通过实验的方式学习这些虚拟网络功能，包括tap设备，veth pair， bridge及router 环境准备 注：本实验基于Ubuntu16.04 12$ apt install uml-utilities$ apt install bridge-utils tap设备 Linux的tun/tap驱动实现了虚拟网卡的功能，tun表示虚拟的是点对点设备，tap表示虚拟的是以太网设备 tap位于网络OSI模型的二层（数据链路层），tun位于网络的三层。 1. 创建tap 12$ tunctl -t tap_test$ ifconfig tap_test 192.168.100.1/24 2. 创建namespace 1$ ip netns add ns_test 3. 迁移网口到namespace 1$ ip link set tap_test netns ns_test 迁移后，对应的ip会没有 4. 进入namespace并查看接口信息 12345678910$ ip netns exec ns_test /bin/bash$ ifconfig tap_test 192.168.50.1/24$ ifconfigtap_test Link encap:Ethernet HWaddr 76:71:70:f2:ac:f6 inet addr:192.168.50.1 Bcast:192.168.50.255 Mask:255.255.255.0 UP BROADCAST MULTICAST MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) veth pair Veth pair是一对虚拟网卡，从一个veth网卡发出的数据包可以直接到达它的peer veth。相当于两个接口之间接着一根网线 1. 创建两个ns 12$ ip netns add ns1$ ip netns add ns2 2. 创建veth pair 1$ ip link add tap1 type veth peer name tap2 3. 迁移网口到namespace 12$ ip link set tap1 netns ns1$ ip link set tap2 netns ns2 4. 分别绑定IP地址 12$ ip netns exec ns1 ifconfig tap1 192.168.40.1/24$ ip netns exec ns2 ifconfig tap2 192.168.40.2/24 5. 测试连通性 1234$ ip netns exec ns1 ping 192.168.40.2PING 192.168.40.2 (192.168.40.2) 56(84) bytes of data.64 bytes from 192.168.40.2: icmp_seq=1 ttl=64 time=0.096 ms64 bytes from 192.168.40.2: icmp_seq=2 ttl=64 time=0.049 ms bridge 两个namespace中的网络可以通过veth pair访问，但3个之间甚至多个互通，veth pair就无法胜任，此时需要用到bridge/switch 下面的实验模拟4个namespace中的接口通过bridge互通 1. 创建veth pair 1234$ ip link add tap1 type veth peer name tap1_peer$ ip link add tap2 type veth peer name tap2_peer$ ip link add tap3 type veth peer name tap3_peer$ ip link add tap4 type veth peer name tap4_peer 2. 创建namespace并迁移tap接口 123456789$ ip netns add ns1$ ip netns add ns2$ ip netns add ns3$ ip netns add ns4# 迁移tap接口$ ip link set tap1 netns ns1$ ip link set tap2 netns ns2$ ip link set tap3 netns ns3$ ip link set tap4 netns ns4 3. 创建bridge 1$ brctl addbr br1 4. 将对应的tap添加到bridge中 1234$ brctl addif br1 tap1_peer$ brctl addif br1 tap2_peer$ brctl addif br1 tap3_peer$ brctl addif br1 tap4_peer 5. 配置IP地址 1234$ ip netns exec ns1 ifconfig tap1 192.168.50.1/24$ ip netns exec ns2 ifconfig tap2 192.168.50.2/24$ ip netns exec ns3 ifconfig tap3 192.168.50.3/24$ ip netns exec ns4 ifconfig tap4 192.168.50.4/24 注： 此时是无法相互访问 6. 设置网桥及对应接口状态为up 12345$ ifconfig br1 up$ ifconfig tap1_peer up$ ifconfig tap2_peer up$ ifconfig tap3_peer up$ ifconfig tap4_peer up 7. 测试连通性 12345678910111213141516$ ip netns exec ns4 ping 192.168.50.1 -c 1PING 192.168.50.1 (192.168.50.1) 56(84) bytes of data.64 bytes from 192.168.50.1: icmp_seq=1 ttl=64 time=0.095 ms--- 192.168.50.1 ping statistics ---1 packets transmitted, 1 received, 0% packet loss, time 0msrtt min/avg/max/mdev = 0.095/0.095/0.095/0.000 ms$ ip netns exec ns4 ping 192.168.50.2 -c 1PING 192.168.50.2 (192.168.50.2) 56(84) bytes of data.64 bytes from 192.168.50.2: icmp_seq=1 ttl=64 time=0.106 ms--- 192.168.50.2 ping statistics ---1 packets transmitted, 1 received, 0% packet loss, time 0msrtt min/avg/max/mdev = 0.106/0.106/0.106/0.000 ms... router 注：Linux默认不开启转发功能，实验前需要先打开 修改/etc/sysctl.conf文件，设置net.ipv4.ip_forward=1，重启生效 临时修改： echo &quot;1&quot; &gt; /proc/sys/net/ipv4/ip_forward 1. 创建veth pair 12$ ip link add tap5 type veth peer name tap5_peer$ ip link add tap6 type veth peer name tap6_peer 2. 创建namespace并迁移tap接口 1234$ ip netns add ns5$ ip netns add ns6$ ip link set tap5 netns ns5$ ip link set tap6 netns ns6 3. 配置IP地址 1234$ ifconfig tap5_peer 192.168.100.1/24$ ifconfig tap6_peer 192.168.200.1/24$ ip netns exec ns5 ifconfig tap5 192.168.100.2/24$ ip netns exec ns6 ifconfig tap6 192.168.200.2/24 4. 为namespace配置路由 12$ ip netns exec ns5 route add -net 192.168.200.0/24 gw 192.168.100.1$ ip netns exec ns6 route add -net 192.168.100.0/24 gw 192.168.200.1 5. 测试 1234$ ip netns exec ns5 ping 192.168.200.2PING 192.168.200.2 (192.168.200.2) 56(84) bytes of data.64 bytes from 192.168.200.2: icmp_seq=1 ttl=63 time=0.077 ms64 bytes from 192.168.200.2: icmp_seq=2 ttl=63 time=0.087 ms tun tun应该是tunnel的缩写，启用了IP层隧道功能 Linux原生支持5种隧道{ ipip | gre | sit | isatap | vti } 忽略上图中的tun1和tun2后，整个topo与router中的一样。那么先按照router章节创建两个ns及配置 1. 创建veth pair并分别迁移到对应的namespace 123456$ ip link add tap1 type veth peer name tap1_peer$ ip link add tap2 type veth peer name tap2_peer$ ip netns add ns1$ ip netns add ns2$ ip link set tap1 netns ns1$ ip link set tap2 netns ns2 2. 配置IP和路由 123456$ ifconfig tap1_peer 192.168.100.1/24$ ifconfig tap2_peer 192.168.200.1/24$ ip netns exec ns1 ifconfig tap1 192.168.100.2/24$ ip netns exec ns2 ifconfig tap2 192.168.200.2/24$ ip netns exec ns1 route add -net 192.168.200.0/24 gw 192.168.100.1$ ip netns exec ns2 route add -net 192.168.100.0/24 gw 192.168.200.1 3. 在ns1中创建tun1 1234567891011121314151617181920$ ip netns exec ns1 ip tunnel add tun1 mode ipip remote 192.168.200.2 local 192.168.100.2 ttl 255$ ip netns exec ns1 ip addr add 192.168.50.10 peer 192.168.60.10 dev tun1$ ip netns exec ns1 ifconfig tun1 up$ ip netns exec ns1 ifconfigtap1 Link encap:Ethernet HWaddr 46:a0:97:02:8c:07 inet addr:192.168.100.2 Bcast:192.168.100.255 Mask:255.255.255.0 inet6 addr: fe80::44a0:97ff:fe02:8c07/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:12 errors:0 dropped:0 overruns:0 frame:0 TX packets:12 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:928 (928.0 B) TX bytes:928 (928.0 B)tun1 Link encap:IPIP Tunnel HWaddr inet addr:192.168.50.10 P-t-P:192.168.60.10 Mask:255.255.255.255 UP POINTOPOINT RUNNING NOARP MTU:1480 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) 4. 在ns2中创建tun2 1234567891011121314151617181920$ ip netns exec ns2 ip tunnel add tun2 mode ipip remote 192.168.100.2 local 192.168.200.2 ttl 255$ ip netns exec ns2 ip addr add 192.168.60.10 peer 192.168.50.10 dev tun2$ ip netns exec ns2 ifconfig tun2 up$ ip netns exec ns2 ifconfigtap2 Link encap:Ethernet HWaddr aa:2e:e9:18:94:95 inet addr:192.168.200.2 Bcast:192.168.200.255 Mask:255.255.255.0 inet6 addr: fe80::a82e:e9ff:fe18:9495/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:12 errors:0 dropped:0 overruns:0 frame:0 TX packets:12 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:928 (928.0 B) TX bytes:928 (928.0 B)tun2 Link encap:IPIP Tunnel HWaddr inet addr:192.168.60.10 P-t-P:192.168.50.10 Mask:255.255.255.255 UP POINTOPOINT RUNNING NOARP MTU:1480 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) 5. 测试tun的连通性 1234$ ip netns exec ns1 ping 192.168.60.10PING 192.168.60.10 (192.168.60.10) 56(84) bytes of data.64 bytes from 192.168.60.10: icmp_seq=1 ttl=64 time=0.333 ms... 6. 抓包看看 123456789101112$ ip netns exec ns2 tcpdump -i tap2tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on tap2, link-type EN10MB (Ethernet), capture size 262144 bytes09:55:19.399964 IP 192.168.100.2 &gt; 192.168.200.2: IP 192.168.50.10 &gt; 192.168.60.10: ICMP echo request, id 17197, seq 1, length 64 (ipip-proto-4)09:55:19.400004 IP 192.168.200.2 &gt; 192.168.100.2: IP 192.168.60.10 &gt; 192.168.50.10: ICMP echo reply, id 17197, seq 1, length 64 (ipip-proto-4)...$ ip netns exec ns2 tcpdump -i tun2tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on tun2, link-type RAW (Raw IP), capture size 262144 bytes09:55:57.735663 IP 192.168.50.10 &gt; 192.168.60.10: ICMP echo request, id 17199, seq 1, length 6409:55:57.735685 IP 192.168.60.10 &gt; 192.168.50.10: ICMP echo reply, id 17199, seq 1, length 64... 注：namespace中抓包可能不会立即打印在屏幕上 7. 查看路由表项 123456$ ip netns exec ns1 route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface192.168.60.10 0.0.0.0 255.255.255.255 UH 0 0 0 tun1192.168.100.0 0.0.0.0 255.255.255.0 U 0 0 0 tap1192.168.200.0 192.168.100.1 255.255.255.0 UG 0 0 0 tap1 可以看到，ns1中自动创建了一条指向192.168.60.10的路由，下一跳是tun1 小结 Openstack的neutron组件正是基于这些Linux虚拟网络功能实现了虚拟机之间的网络通道。 其中，tap、tun、veth pair被用于bridge之间的连接、bridge与vm的连接、bridge与router之间的连接。 而bridge提供二层转发功能，router提供三层转发功能。","categories":[{"name":"network","slug":"network","permalink":"http://www.isimble.com/categories/network/"}],"tags":[{"name":"network","slug":"network","permalink":"http://www.isimble.com/tags/network/"},{"name":"linux","slug":"linux","permalink":"http://www.isimble.com/tags/linux/"}]},{"title":"Docker + ovs，原来如此简单","slug":"docker-ovs-basic","date":"2019-08-06T23:05:29.000Z","updated":"2019-08-07T07:09:31.967Z","comments":true,"path":"2019/08/07/docker-ovs-basic/","link":"","permalink":"http://www.isimble.com/2019/08/07/docker-ovs-basic/","excerpt":"一直没敢下手尝试docker的网络使用ovs，总觉得可能很复杂。所以，人往往都是自己把自己拒之门外的。 我整整折腾了一天，容器内以二层方式访问外部的IP始终不通。最后发现原来是网卡混杂模式惹的祸","text":"一直没敢下手尝试docker的网络使用ovs，总觉得可能很复杂。所以，人往往都是自己把自己拒之门外的。 我整整折腾了一天，容器内以二层方式访问外部的IP始终不通。最后发现原来是网卡混杂模式惹的祸 环境准备 安装docker 在我的其他博文里有国内源安装docker的内容，原样照搬 12345$ sudo apt-get update &amp;&amp; sudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common$ curl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -$ add-apt-repository \"deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable\"$ apt update$ apt-get -y install docker-ce 安装ovs 1$ sudo apt-get install openvswitch-switch 安装ovs-docker 简单介绍一下，ovs-docker实际上是一个shell脚本，封装了ovs和docker的一些操作。 123$ cd /usr/bin$ wget https://raw.githubusercontent.com/openvswitch/ovs/master/utilities/ovs-docker$ chmod a+rwx ovs-docker 实验 两个docker容器经过ovs网桥进行访问及访问外部网络 容器互访 创建ovs网桥 1$ ovs-vsctl add-br br0 创建两个容器 注：实验使用了busybox的镜像，需要提前pull到本地 1docker pull busybox 1$ docker run -it --net=none --privileged=true --name=h1 busybox 在另外一个shell中执行 1$ docker run -it --net=none --privileged=true --name=h2 busybox 查看当前网络 12345678/ # ifconfiglo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) 可以看到当前仅有一个loopback接口 连接容器到网桥 12$ ovs-docker add-port br0 eth0 h1$ ovs-docker add-port br0 eth0 h2 配置IP地址 分别在两个docker容器中配置eth0的IP地址为同一个网段 h1 1$ ifconfig eth0 192.168.1.2 h2 1$ ifconfig eth0 192.168.1.3 测试连通性 1234$ ping 192.168.1.3PING 192.168.1.2 (192.168.1.3): 56 data bytes64 bytes from 192.168.1.3: seq=0 ttl=64 time=1.065 ms64 bytes from 192.168.1.3: seq=1 ttl=64 time=0.139 ms 没错，就是这么简单 访问外部网络 当我解决了混杂模式的问题之后，原来一切都是那么简单 在外部另外一个PC上配置了192.168.1.1 为br0增加物理接口 1$ ovs-vsctl add-port br0 ens192 测试 从容器中ping 192.168.1.1 1234$ ping 192.168.1.1PING 192.168.1.1 (192.168.1.1): 56 data bytes64 bytes from 192.168.1.1: seq=30 ttl=64 time=1.319 ms64 bytes from 192.168.1.1: seq=31 ttl=64 time=0.461 ms 大功告成","categories":[{"name":"docker","slug":"docker","permalink":"http://www.isimble.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://www.isimble.com/tags/docker/"},{"name":"ovs","slug":"ovs","permalink":"http://www.isimble.com/tags/ovs/"}]},{"title":"k8s环境安装之差点儿我就放弃了","slug":"k8s-env-prep","date":"2019-07-18T19:23:45.000Z","updated":"2019-07-19T03:30:28.053Z","comments":true,"path":"2019/07/19/k8s-env-prep/","link":"","permalink":"http://www.isimble.com/2019/07/19/k8s-env-prep/","excerpt":"尝试了好几种方式，想要准备一个k8s的实验集群，都或多或少的遇到了些问题。主要是因为要入门，所以要搭一套环境，但因为还没有入门，搭建过程中遇到问题，就不知如何下手。 在行将放弃之际，kubeasz挽救了我，本文主要记录如何用kubeasz搭建一个allinone的环境","text":"尝试了好几种方式，想要准备一个k8s的实验集群，都或多或少的遇到了些问题。主要是因为要入门，所以要搭一套环境，但因为还没有入门，搭建过程中遇到问题，就不知如何下手。 在行将放弃之际，kubeasz挽救了我，本文主要记录如何用kubeasz搭建一个allinone的环境 环境准备 其实kubeasz的文档写的非常简单，因为本身搭建过程就是很简单，可参看官方文档 一台Linux虚拟机(Ubuntu16.04 server或CentOS 7 Minimal) 内存需要2G 硬盘30G 安装 下载文件 github的说明文档提供的下载方式，我其实都疑惑了，因为有一个${release}，并非开箱即食。可以这样做 1git clone https://github.com/easzlab/kubeasz.git 随后，进入kubeasz目录，使用工具脚本下载 12$ cd kubeasz$ bash tools/easzup -D 安装 下面就是按部就班的运行了，注意，你也不需要提前安装docker，所有的都是自动下载及安装的 容器化运行 kubeasz 1$ bash tools/easzup -S 使用默认配置安装 aio 集群 1$ docker exec -it kubeasz easzctl start-aio 如遇到失败，可再次尝试执行 验证安装 12345$ kubectl version # 验证集群版本 $ kubectl get componentstatus # 验证 scheduler/controller-manager/etcd等组件状态$ kubectl get node # 验证节点就绪 (Ready) 状态$ kubectl get pod --all-namespaces # 验证集群pod状态，默认已安装网络插件、coredns、metrics-server等$ kubectl get svc --all-namespaces # 验证集群服务状态 登陆dashboard 安装完成后，dashboard已经准备好了，只需要获取登陆方式即可。 按照官方文档的说法，登陆dashboard有好几种方式，选择一种即可。如token方式 获取port 按照以上方式安装完成后，默认的port应该是34980，所以，可以访问https://当前IP:34980，即可登陆dashboard 也可以查看dashboard的端口开放状况 12345678kubectl get svc --all-namespacesNAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdefault kubernetes ClusterIP 10.68.0.1 &lt;none&gt; 443/TCP 30mkube-system heapster ClusterIP 10.68.91.148 &lt;none&gt; 80/TCP 114skube-system kube-dns ClusterIP 10.68.0.2 &lt;none&gt; 53/UDP,53/TCP,9153/TCP 2m20skube-system kubernetes-dashboard NodePort 10.68.23.76 &lt;none&gt; 443:34980/TCP 118skube-system metrics-server ClusterIP 10.68.29.183 &lt;none&gt; 443/TCP 2m13skube-system traefik-ingress-service NodePort 10.68.226.239 &lt;none&gt; 80:23456/TCP,8080:38858/TCP 105s 获取token 查看如下文件是否存在 1$ ls /etc/ansible/manifests/dashboard/admin-user-sa-rbac.yaml 获取token 1234567$ kubectl apply -f /etc/ansible/manifests/dashboard/admin-user-sa-rbac.yaml...====ca.crt: 1350 bytesnamespace: 11 bytestoken: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLWRoNjYyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJmZDdjZjc4Mi1hOTM0LTExZTktOTA1Zi0wMDBjMjlhMzc1YmYiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.KXFNuuibxRpF7AJHZBRkGgvpiVl4axfGA0A8haLIR3coX0HHPrQph8sO9GKl7C0KP6YXN-43VIxlpDXy-i_UkimxDJRVS08IW4r39Z4HhD-rKFYafQJROMrFG9lcbn1pZWTLBRGsO3lFCI1DyRDC7W_HSb63mgO2IoiQXGhgdhEWc7OnyNfxovkbtQA5xfsVqWqeWE-Dn4Jeo0lFe630I-iREMXPQvF5I7UNJFMJLCgFV0fG8J7__MYBf8xVnYL3ryaMBwKjQxQVsD3IOGS6TAk7RLZzjevySl-pXE1CcE8fosQQwJsOjOoKn5u1LGK3XUkIJR2rOf3jVMoQYYqe1Q 踩过的坑 node出于NotReady状态 排查思路 查看服务状态及相关错误信息 1$ journalctl -f -u kubelet 最终发现我的内存因为是1G导致运行失败","categories":[{"name":"k8s","slug":"k8s","permalink":"http://www.isimble.com/categories/k8s/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://www.isimble.com/tags/docker/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://www.isimble.com/tags/Kubernetes/"},{"name":"k8s","slug":"k8s","permalink":"http://www.isimble.com/tags/k8s/"}]},{"title":"pyenv使用笔记","slug":"pyenv-notebook","date":"2019-02-05T08:01:10.000Z","updated":"2019-02-05T16:06:30.170Z","comments":true,"path":"2019/02/05/pyenv-notebook/","link":"","permalink":"http://www.isimble.com/2019/02/05/pyenv-notebook/","excerpt":"OSX上的python版本纷繁复杂。 有系统自带的: /System/Library/Frameworks/Python.framework/Versions/ 有后来安装的: /Library/Frameworks/Python.framework/Versions/","text":"OSX上的python版本纷繁复杂。 有系统自带的: /System/Library/Frameworks/Python.framework/Versions/ 有后来安装的: /Library/Frameworks/Python.framework/Versions/ 经常搞不清楚用的是哪里的python。遇到要升级python版本，又提心吊胆的怕把系统的那个给搞挂了。于是，便试着用pyenv来管理版本 关于Pyenv pyenv是Python版本管理工具，能够使你轻松的在多个python版本之间进行切换 pyenv当前在github上有14k的关注量 安装说明参看github上的安装文档 常用命令 使用pyenv commands可以查看所有命令或者通过pyenv -h来查看常用命令 版本管理 查看已安装的python版本 123$ pyenv versions* system (set by /Users/xxx/.pyenv/version) 3.7.0 查看可以安装的版本 123456$ pyenv install -lAvailable versions: 2.1.3 2.2.3 ... stackless-3.5.4 可以看到，可安装的版本有python2, python3, activepython, anaconda, ironpython, jython, pypy, stackless, etc. 安装指定版本 123456$ pyenv install 2.7.15python-build: use openssl from homebrewpython-build: use readline from homebrewInstalling Python-2.7.15...python-build: use readline from homebrewInstalled Python-2.7.15 to /Users/xxx/.pyenv/versions/2.7.15 注：Mac上安装可能会遇到The Python zlib extension was not compiled. Missing the zlib?的错误，可以尝试CFLAGS=&quot;-I$(xcrun --show-sdk-path)/usr/include&quot; pyenv install 2.7.15 更多错误，可查看项目的wiki中的常见问题 安装完毕一个python版本后，需要执行pyenv rehash来更新后，才能看到已安装的版本 优先级 可以为当前目录、当前shell以及全局进行不同的python版本定义 遵循shell&gt;local&gt;global的优先级顺序 设置版本 12$ pyenv global 2.7.15 #设置全局版本，版本信息记录在~/.pyenv/version$ pyenv local 3.7.0 #设置当前目录的python版本，将在当前目录生成.python-version文件 当需要为一个目录设置特定的python版本时，可以先进入该目录下执行pyenv local xxx 当需要为该目录下的一个子目录设置特定的python版本，可进入该子目录设置local 设置完毕后，会在响应的目录下生成.python-version的文件 使用时，将从当前目录开始查找，如果不存在.python-version文件，则向上一级查找，直到根目录为止 如果到跟目录任然没有查找到，则使用global的设置 1234$ pyenv shell 3.6.0 #为当前shell设置python版本，将通过环境变量的方式设置$ env | grep PYENV_VERSIONPYENV_VERSION=2.7.15$ pyenv shell --unset # virtualenv 默认情况下，安装pyenv后会安装pyenv-virtualenv的插件，可以通过pyenv virtualenv创建虚拟环境 查看已有的virtualenv 123$ pyenv virtualenvs 3.7.0/envs/common3 (created from /Users/abc/.pyenv/versions/3.7.0) common3 (created from /Users/abc/.pyenv/versions/3.7.0) 创建virtualenv 12345678910111213141516171819$ pyenv virtualenv 2.7.15 my2.7.15Collecting virtualenv Downloading https://files.pythonhosted.org/packages/8f/f1/c0b069ca6cb44f9681715232e6d3d65c75866dd231c5e4a88e80a46634bb/virtualenv-16.3.0-py2.py3-none-any.whl (2.0MB) 100% |████████████████████████████████| 2.0MB 117kB/sRequirement already satisfied: setuptools&gt;=18.0.0 in ./.pyenv/versions/2.7.15/lib/python2.7/site-packages (from virtualenv) (39.0.1)Installing collected packages: virtualenvSuccessfully installed virtualenv-16.3.0New python executable in /Users/abc/.pyenv/versions/2.7.15/envs/my2.7.15/bin/python2.7Also creating executable in /Users/abc/.pyenv/versions/2.7.15/envs/my2.7.15/bin/pythonInstalling setuptools, pip, wheel...done.Requirement already satisfied: setuptools in /Users/abc/.pyenv/versions/2.7.15/envs/my2.7.15/lib/python2.7/site-packagesRequirement already satisfied: pip in /Users/abc/.pyenv/versions/2.7.15/envs/my2.7.15/lib/python2.7/site-packages$ pyenv virtualenvs 2.7.15/envs/my2.7.15 (created from /Users/abc/.pyenv/versions/2.7.15) 3.7.0/envs/common3 (created from /Users/abc/.pyenv/versions/3.7.0) common3 (created from /Users/abc/.pyenv/versions/3.7.0) my2.7.15 (created from /Users/abc/.pyenv/versions/2.7.15) active &amp; deactive 123$ pyenv activate my2.7.15(my2.7.15) $(my2.7.15) $ pyenv deactivate 删除virtualenv 12$ pyenv uninstall my2.7.15pyenv-virtualenv: remove /Users/abc/.pyenv/versions/2.7.15/envs/my2.7.15? Y Have Fun 😄","categories":[{"name":"python","slug":"python","permalink":"http://www.isimble.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://www.isimble.com/tags/python/"}]},{"title":"使用国内资源（备忘）","slug":"cn_resources","date":"2019-01-14T21:34:00.000Z","updated":"2019-11-18T13:53:40.136Z","comments":true,"path":"2019/01/15/cn_resources/","link":"","permalink":"http://www.isimble.com/2019/01/15/cn_resources/","excerpt":"使用阿里云镜像安装Docker-CE docker.com越来越难访问了","text":"使用阿里云镜像安装Docker-CE docker.com越来越难访问了 安装必要工具 1$ sudo apt-get update sudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common 安装阿里云的GPG证书 1$ curl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add - 添加软件源 1$ add-apt-repository \"deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable\" 拉取更新 1$ apt update 安装docker-ce 1$ apt-get -y install docker-ce 使用阿里云的pip源 安装pip 12$ curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py$ python get-pip.py 修改pip源为阿里云（永久修改） 在用户根目录创建 .pip/pip.conf 12345[global]index-url = http://mirrors.aliyun.com/pypi/simple[install]trusted-host=mirrors.aliyun.com MAC使用清华homebrew源 可直接访问清华大学开源软件镜像站获取 替换现有的git 1234567$ cd \"$(brew --repo)\"$ git remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/brew.git$ cd \"$(brew --repo)/Library/Taps/homebrew/homebrew-core\"$ git remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/homebrew-core.git$ brew update","categories":[{"name":"tech","slug":"tech","permalink":"http://www.isimble.com/categories/tech/"}],"tags":[{"name":"python","slug":"python","permalink":"http://www.isimble.com/tags/python/"},{"name":"docker","slug":"docker","permalink":"http://www.isimble.com/tags/docker/"},{"name":"Linux","slug":"Linux","permalink":"http://www.isimble.com/tags/Linux/"},{"name":"Mac","slug":"Mac","permalink":"http://www.isimble.com/tags/Mac/"}]},{"title":"Ubuntu16.04 VPP环境搭建","slug":"vpp-setup","date":"2018-11-15T00:31:50.000Z","updated":"2018-11-15T08:42:11.503Z","comments":true,"path":"2018/11/15/vpp-setup/","link":"","permalink":"http://www.isimble.com/2018/11/15/vpp-setup/","excerpt":"环境 环境准备 VmWare虚拟环境 Host需求：2cpu，4G内存，3块网卡 Ubuntu16.04 拓扑 搭建典型的c2s拓扑","text":"环境 环境准备 VmWare虚拟环境 Host需求：2cpu，4G内存，3块网卡 Ubuntu16.04 拓扑 搭建典型的c2s拓扑 通过vsphere创建两个虚拟交换机 host1的第二个接口与vpp的第二个接口连在同一个交换机上 host2的第二个接口与vpp的第三个接口连在同一个交换机上 VPP安装先决条件 Ubuntu安装git（git默认安装） Ubuntu安装dpdk，并绑定PCI的另外两个接口到dpdk 安装 官方文档提供了多种安装方式，并且推荐使用vagrant包，由于这里使用的是虚拟环境，则使用直接安装的方式 获取源码 1$ git clone https://gerrit.fd.io/r/vpp 安装vpp 1$ cd vpp Step1 可以先执行一下make查看可以执行哪些操作 Step2 - make 12345678$ make build-release...by executing \"make install-dep\"Makefile:262: recipe for target '/root/vpp/build-root/.deps.ok' failedmake: *** [/root/vpp/build-root/.deps.ok] Error 1 如果遇到以上错误，则意味着缺少依赖包 1$ make install-dep 依赖包安装完成后，再次执行make build-release Step3 - Build deb包 1$ make pkg-deb Step4 - 安装VPP packages 1$ dpkg -i /vpp/build-root/*.deb 中间可能会遇到vpp-api-python的错误，使用apt安装后重新安装packages 1$ apt install vpp-api-python 配置 123mkdir -p /etc/vppcp ./build-root/deb/debian/vpp/etc/vpp/startup.conf /etc/vpp/cp ./build-root/deb/debian/vpp/etc/sysctl.d/80-vpp.conf /etc/sysctl.d/ 修改/etc/vpp/startup.conf 123456789101112131415161718192021222324252627282930313233343536unix &#123; nodaemon log /var/log/vpp/vpp.log full-coredump #cli-listen /run/vpp/cli.sock cli-listen 0.0.0.0:5002 gid vpp&#125;api-trace &#123; on&#125;api-segment &#123; gid vpp&#125;socksvr &#123; default&#125;cpu &#123; main-core 0 workers 2&#125;dpdk &#123; dev 0000:0b:00.0 &#123;num-rx-queues 2&#125; dev 0000:13:00.0 &#123;num-rx-queues 2&#125; num-mbufs 128000 socket-mem 1024,1024 &#125;plugins &#123; path /root/vpp/build-root/install-vpp-native/vpp/lib/vpp_plugins&#125; 启动VPP 1$ service vpp start 查看状态 123456789101112131415161718192021222324$ systemctl status vpp.service● vpp.service - vector packet processing engine Loaded: loaded (/lib/systemd/system/vpp.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2018-11-15 16:00:09 CST; 4min 52s ago Process: 19959 ExecStopPost=/bin/rm -f /dev/shm/db /dev/shm/global_vm /dev/shm/vpe-api (code=exited, status=0/SUCCESS) Process: 19985 ExecStartPre=/sbin/modprobe uio_pci_generic (code=exited, status=0/SUCCESS) Process: 19981 ExecStartPre=/bin/rm -f /dev/shm/db /dev/shm/global_vm /dev/shm/vpe-api (code=exited, status=0/SUCCESS) Main PID: 19989 (vpp_main) Tasks: 6 Memory: 55.9M CPU: 9min 46.890s CGroup: /system.slice/vpp.service └─19989 /usr/bin/vpp -c /etc/vpp/startup.confNov 15 16:00:09 fdio /usr/bin/vpp[19989]: load_one_vat_plugin:67: Loaded plugin: nat_test_plugin.soNov 15 16:00:09 fdio /usr/bin/vpp[19989]: load_one_vat_plugin:67: Loaded plugin: lb_test_plugin.soNov 15 16:00:09 fdio /usr/bin/vpp[19989]: load_one_vat_plugin:67: Loaded plugin: gtpu_test_plugin.soNov 15 16:00:09 fdio /usr/bin/vpp[19989]: load_one_vat_plugin:67: Loaded plugin: avf_test_plugin.soNov 15 16:00:09 fdio /usr/bin/vpp[19989]: load_one_vat_plugin:67: Loaded plugin: acl_test_plugin.soNov 15 16:00:09 fdio /usr/bin/vpp[19989]: load_one_vat_plugin:67: Loaded plugin: memif_test_plugin.soNov 15 16:00:09 fdio /usr/bin/vpp[19989]: load_one_vat_plugin:67: Loaded plugin: stn_test_plugin.soNov 15 16:00:09 fdio vpp[19989]: /usr/bin/vpp[19989]: dpdk: EAL init args: -c 7 -n 4 --huge-dir /run/vpp/hugepages --file-prefix vpp -w 0000:0b:00.0 -w 0000:13:00.0 --master-lcorNov 15 16:00:09 fdio /usr/bin/vpp[19989]: dpdk: EAL init args: -c 7 -n 4 --huge-dir /run/vpp/hugepages --file-prefix vpp -w 0000:0b:00.0 -w 0000:13:00.0 --master-lcore 0 --socketNov 15 16:00:10 fdio vnet[19989]: dpdk_ipsec_process:1015: not enough DPDK crypto resources, default to OpenSSL 虽然有警告信息，但似乎不影响使用 测试 根据startup.conf中的配置不同，选择不同的连接vpp方式 登陆VPP CLI 123456789101112131415$ telnet 127.0.0.1 5002Trying 127.0.0.1...Connected to 127.0.0.1.Escape character is '^]'. _______ _ _ _____ ___ __/ __/ _ \\ (_)__ | | / / _ \\/ _ \\ _/ _// // / / / _ \\ | |/ / ___/ ___/ /_/ /____(_)_/\\___/ |___/_/ /_/vpp# show interface Name Idx State MTU (L3/IP4/IP6/MPLS) Counter CountGigabitEthernet13/0/0 2 down 9000/0/0/0GigabitEthernetb/0/0 1 down 9000/0/0/0local0 0 down 0/0/0/0vpp# 开启端口 1234567891011121314vpp# set interface state GigabitEthernet13/0/0 upvpp# set interface state GigabitEthernetb/0/0 upvpp# show interface Name Idx State MTU (L3/IP4/IP6/MPLS) Counter CountGigabitEthernet13/0/0 2 up 9000/0/0/0 rx packets 1 rx bytes 60 drops 1 ip4 1GigabitEthernetb/0/0 1 up 9000/0/0/0 rx packets 4 rx bytes 240 drops 4 ip4 1local0 0 down 0/0/0/0vpp# 配置为switch模式 12345vpp# set interface l2 bridge GigabitEthernet13/0/0 1vpp# set interface l2 bridge GigabitEthernetb/0/0 1vpp# show bridge-domain BD-ID Index BSN Age(min) Learning U-Forwrd UU-Flood Flooding ARP-Term BVI-Intf 1 1 0 off on on flood on off N/A 测试连通性 c2s Host1: 192.168.0.2/24 Host2: 192.168.0.3/24 通过host1 ping host2 配置loopback口测试连通性 123456789101112131415vpp# create loopback interfaceloop0vpp# set interface ip address loop0 192.168.0.1/24vpp# set interface state loop0 upvpp# set interface l2 bridge loop0 1 bvivpp# show bridge-domain BD-ID Index BSN Age(min) Learning U-Forwrd UU-Flood Flooding ARP-Term BVI-Intf 1 1 0 off on on flood on off loop0vpp# ping 192.168.0.2116 bytes from 192.168.0.2: icmp_seq=2 ttl=64 time=.8704 ms116 bytes from 192.168.0.2: icmp_seq=3 ttl=64 time=.2564 ms116 bytes from 192.168.0.2: icmp_seq=4 ttl=64 time=.2653 ms116 bytes from 192.168.0.2: icmp_seq=5 ttl=64 time=.3413 msStatistics: 5 sent, 4 received, 20% packet loss set interface l2 bridge loop0 1 bvi中的bvi意味着这个接口将用来接收、发送以及转发该bridge domain的报文","categories":[{"name":"network","slug":"network","permalink":"http://www.isimble.com/categories/network/"}],"tags":[{"name":"network","slug":"network","permalink":"http://www.isimble.com/tags/network/"},{"name":"vpp","slug":"vpp","permalink":"http://www.isimble.com/tags/vpp/"}]},{"title":"Ubuntu16.04上安装DPDK","slug":"dpdk-setup","date":"2018-11-14T21:47:28.000Z","updated":"2018-11-15T05:55:46.719Z","comments":true,"path":"2018/11/15/dpdk-setup/","link":"","permalink":"http://www.isimble.com/2018/11/15/dpdk-setup/","excerpt":"DPDK安装 DPDK（Data Plane Development Kit）是一个用来进行包数据处理加速的软件库 从git获取源码 1$ git clone git://dpdk.org/dpdk","text":"DPDK安装 DPDK（Data Plane Development Kit）是一个用来进行包数据处理加速的软件库 从git获取源码 1$ git clone git://dpdk.org/dpdk 创建环境变量 1234$ cd ~/dpdk$ export RTE_SDK=`pwd`$ export DESTDIR=`pwd`$ export RTE_TARGET=x86_64-default-linuxapp-gcc 因为这些环境变量总是会用到，可以将其放入一个文件，如env.source，使用source env.source 123456$ cd ~/dpdk$ more env.sourceexport RTE_SDK=`pwd`export DESTDIR=`pwd`export RTE_TARGET=x86_64-default-linuxapp-gcc$ source env.source 开始安装 使用dpdk-setup.sh脚本进行安装 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061./usertools/dpdk-setup.sh------------------------------------------------------------------------------ RTE_SDK exported as /root/dpdk---------------------------------------------------------------------------------------------------------------------------------------- Step 1: Select the DPDK environment to build----------------------------------------------------------[1] arm64-armv8a-linuxapp-clang[2] arm64-armv8a-linuxapp-gcc[3] arm64-dpaa2-linuxapp-gcc[4] arm64-dpaa-linuxapp-gcc[5] arm64-stingray-linuxapp-gcc[6] arm64-thunderx-linuxapp-gcc[7] arm64-xgene1-linuxapp-gcc[8] arm-armv7a-linuxapp-gcc[9] i686-native-linuxapp-gcc[10] i686-native-linuxapp-icc[11] ppc_64-power8-linuxapp-gcc[12] x86_64-native-bsdapp-clang[13] x86_64-native-bsdapp-gcc[14] x86_64-native-linuxapp-clang[15] x86_64-native-linuxapp-gcc[16] x86_64-native-linuxapp-icc[17] x86_x32-native-linuxapp-gcc---------------------------------------------------------- Step 2: Setup linuxapp environment----------------------------------------------------------[18] Insert IGB UIO module[19] Insert VFIO module[20] Insert KNI module[21] Setup hugepage mappings for non-NUMA systems[22] Setup hugepage mappings for NUMA systems[23] Display current Ethernet/Crypto device settings[24] Bind Ethernet/Crypto device to IGB UIO module[25] Bind Ethernet/Crypto device to VFIO module[26] Setup VFIO permissions---------------------------------------------------------- Step 3: Run test application for linuxapp environment----------------------------------------------------------[27] Run test application ($RTE_TARGET/app/test)[28] Run testpmd application in interactive mode ($RTE_TARGET/app/testpmd)---------------------------------------------------------- Step 4: Other tools----------------------------------------------------------[29] List hugepage info from /proc/meminfo---------------------------------------------------------- Step 5: Uninstall and system cleanup----------------------------------------------------------[30] Unbind devices from IGB UIO or VFIO driver[31] Remove IGB UIO module[32] Remove VFIO module[33] Remove KNI module[34] Remove hugepage mappings[35] Exit ScriptOption: Step1 根据自己环境选择相应的build，如我是64位的Intel架构的环境，则选择**[15]** 12345678...Installation in /root/dpdk/ complete------------------------------------------------------------------------------ RTE_TARGET exported as x86_64-native-linuxapp-gcc------------------------------------------------------------------------------Press enter to continue ... Step2 选择**[18]**来家在哪里igb_uio模块 选择**[21]**来创建Hugepage，这里我输入了128 选择**[24]**来绑定PCI网卡 123456789101112131415161718192021222324252627282930Option: 21Removing currently reserved hugepagesUnmounting /mnt/huge and removing directory Input the number of 2048kB hugepages Example: to have 128MB of hugepages available in a 2MB huge page system, enter '64' to reserve 64 * 2MB pagesNumber of pages: 128Reserving hugepagesCreating /mnt/huge and mounting as hugetlbfsPress enter to continue .........Option: 24......Other Compress devices======================&lt;none&gt;Enter PCI address of device to bind to IGB UIO driver: 0b:00.0Network devices using DPDK-compatible driver============================================0000:0b:00.0 'VMXNET3 Ethernet Controller 07b0' drv=igb_uio unused=vmxnet3,uio_pci_generic0000:13:00.0 'VMXNET3 Ethernet Controller 07b0' drv=igb_uio unused=vmxnet3,uio_pci_generic 确认需要使用的PCI网卡为drv=igb_uio则说明绑定成功 Step3 - 测试 123456789101112131415161718192021Option: 27 Enter hex bitmask of cores to execute test app on Example: to execute app on cores 0 to 7, enter 0xffbitmask: 0x3Launching appsudo: x86_64-default-linuxapp-gcc/app/test: command not foundPress enter to continue ......Option: 28 Enter hex bitmask of cores to execute testpmd app on Example: to execute app on cores 0 to 7, enter 0xffbitmask: 0x3Launching appsudo: x86_64-default-linuxapp-gcc/app/testpmd: command not found 执行27和28都有可能报错，提示command not found，此时，可以先退出安装脚本 进入/dpdk/x86_64-native-linuxapp-gcc/app目录，会看到testpmd存在于目录下，运行测试，正常状况时，会如下显示 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859$ ./testpmdEAL: Detected 8 lcore(s)EAL: Detected 1 NUMA nodesEAL: Multi-process socket /var/run/dpdk/rte/mp_socketEAL: Probing VFIO support...EAL: PCI device 0000:03:00.0 on NUMA socket -1EAL: Invalid NUMA socket, default to 0EAL: probe driver: 15ad:7b0 net_vmxnet3EAL: PCI device 0000:0b:00.0 on NUMA socket -1EAL: Invalid NUMA socket, default to 0EAL: probe driver: 15ad:7b0 net_vmxnet3EAL: PCI device 0000:13:00.0 on NUMA socket -1EAL: Invalid NUMA socket, default to 0EAL: probe driver: 15ad:7b0 net_vmxnet3testpmd: create a new mbuf pool &lt;mbuf_pool_socket_0&gt;: n=203456, size=2176, socket=0testpmd: preferred mempool ops selected: ring_mp_mcConfiguring Port 0 (socket 0)...... TX queue: 0 TX desc=0 - TX free threshold=0 TX threshold registers: pthresh=0 hthresh=0 wthresh=0 TX offloads=0x0 - TX RS bit threshold=0Press enter to exitTelling cores to stop...Waiting for lcores to finish... ---------------------- Forward statistics for port 0 ---------------------- RX-packets: 57 RX-dropped: 0 RX-total: 57 TX-packets: 57 TX-dropped: 0 TX-total: 57 ---------------------------------------------------------------------------- ---------------------- Forward statistics for port 1 ---------------------- RX-packets: 57 RX-dropped: 0 RX-total: 57 TX-packets: 57 TX-dropped: 0 TX-total: 57 ---------------------------------------------------------------------------- +++++++++++++++ Accumulated forward statistics for all ports+++++++++++++++ RX-packets: 114 RX-dropped: 0 RX-total: 114 TX-packets: 114 TX-dropped: 0 TX-total: 114 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++Done.Shutting down port 0...Stopping ports...DoneClosing ports...DoneShutting down port 1...Stopping ports...DoneClosing ports...DoneBye... HugePage问题解决 问题 当运行测试时或者testpmd，可能会遇到如下问题 12345678910111213141516$ ./testpmdEAL: Detected 8 lcore(s)EAL: Detected 1 NUMA nodesEAL: Multi-process socket /var/run/dpdk/rte/mp_socketEAL: No free hugepages reported in hugepages-2048kBEAL: No free hugepages reported in hugepages-2048kBEAL: FATAL: Cannot get hugepage information.EAL: Cannot get hugepage information.PANIC in main():Cannot init EAL5: [./testpmd(_start+0x29) [0x498829]]4: [/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0) [0x7f8a0fee0830]]3: [./testpmd(main+0xc48) [0x48f528]]2: [./testpmd(__rte_panic+0xbb) [0x47eb09]]1: [./testpmd(rte_dump_stack+0x2b) [0x5c8a1b]]Aborted (core dumped) 问题原因及解决 这说明Hugepage不够用，可以先查看系统内存状况 1234567$ cat /proc/meminfo | grep HugeAnonHugePages: 0 kBHugePages_Total: 665HugePages_Free: 0HugePages_Rsvd: 0HugePages_Surp: 537Hugepagesize: 2048 kB 很显然，这里总共有665，太小了，不够用，需要修改系统相关内容 12345678$ echo 2048 &gt; /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages$ cat /proc/meminfo | grep HugeAnonHugePages: 0 kBHugePages_Total: 2048HugePages_Free: 1080HugePages_Rsvd: 0HugePages_Surp: 0Hugepagesize: 2048 kB 注 如果系统重启或者重新编译，则该值会被重新刷新为默认值，需要重新设置","categories":[{"name":"network","slug":"network","permalink":"http://www.isimble.com/categories/network/"}],"tags":[{"name":"dpdk","slug":"dpdk","permalink":"http://www.isimble.com/tags/dpdk/"},{"name":"network","slug":"network","permalink":"http://www.isimble.com/tags/network/"}]},{"title":"OVS学习笔记——常用命令练习","slug":"ovs-study2","date":"2018-11-13T00:05:36.000Z","updated":"2018-11-13T08:12:58.760Z","comments":true,"path":"2018/11/13/ovs-study2/","link":"","permalink":"http://www.isimble.com/2018/11/13/ovs-study2/","excerpt":"控制管理 创建网桥 1$ ovs-vsctl add-br testbr 查看网桥和端口 1234567$ ovs-vsctl showb66c0897-27c9-441a-9486-42cfb65a4649 Bridge testbr Port testbr Interface testbr type: internal ovs_version: \"2.5.5\"","text":"控制管理 创建网桥 1$ ovs-vsctl add-br testbr 查看网桥和端口 1234567$ ovs-vsctl showb66c0897-27c9-441a-9486-42cfb65a4649 Bridge testbr Port testbr Interface testbr type: internal ovs_version: \"2.5.5\" 网桥端口操作 12$ ovs-vsctl add-port br0 eth1$ ovs-vsctl del-port br0 eth1 查看流表 123$ ovs-ofctl dump-flows testbrNXST_FLOW reply (xid=0x4): cookie=0x0, duration=364.789s, table=0, n_packets=0, n_bytes=0, idle_age=364, priority=0 actions=NORMAL 控制器设置 123456789101112131415161718192021222324252627282930# 设置控制器$ ovs-vsctl set-controller testbr tcp:10.180.9.62:6633$ ovs-vsctl showb66c0897-27c9-441a-9486-42cfb65a4649 Bridge testbr Controller \"tcp:10.180.9.62:6633\" Port testbr Interface testbr type: internal ovs_version: \"2.5.5\"# 查看控制器列表$ ovs-vsctl list controller_uuid : 2fe35662-3f4f-446b-9296-6f1eae38ba5econnection_mode : []controller_burst_limit: []controller_rate_limit: []enable_async_messages: []external_ids : &#123;&#125;inactivity_probe : []is_connected : truelocal_gateway : []local_ip : []local_netmask : []max_backoff : []other_config : &#123;&#125;role : otherstatus : &#123;sec_since_connect=\"4\", state=ACTIVE&#125;target : \"tcp:10.180.9.62:6633\"# 删除控制器$ ovs-vsctl del-controller testbr 接口相关 1234567891011121314$ ovs-ofctl dump-ports s1OFPST_PORT reply (xid=0x2): 3 ports port LOCAL: rx pkts=0, bytes=0, drop=94, errs=0, frame=0, over=0, crc=0 tx pkts=0, bytes=0, drop=0, errs=0, coll=0 port 1: rx pkts=124, bytes=8418, drop=0, errs=0, frame=0, over=0, crc=0 tx pkts=130, bytes=8898, drop=0, errs=0, coll=0 port 2: rx pkts=123, bytes=8340, drop=0, errs=0, frame=0, over=0, crc=0 tx pkts=130, bytes=8886, drop=0, errs=0, coll=0$ ovs-appctl dpif/showsystem@ovs-system: hit:318465 missed:735 s1: s1 65534/3: (internal) s1-eth1 1/2: (system) s1-eth2 2/1: (system) 流表类 流表操作 查看流表 1234$ ovs-ofctl dump-flows s1NXST_FLOW reply (xid=0x4): cookie=0x0, duration=356.689s, table=0, n_packets=708, n_bytes=42480, idle_age=0, priority=65535,dl_dst=01:80:c2:00:00:0e,dl_type=0x88cc actions=CONTROLLER:65535 cookie=0x0, duration=356.699s, table=0, n_packets=94, n_bytes=7488, idle_age=346, priority=0 actions=CONTROLLER:65535 添加普通流表 123456$ ovs-ofctl add-flow s1 in_port=1,actions=drop$ ovs-ofctl dump-flows s1NXST_FLOW reply (xid=0x4): cookie=0x0, duration=441.879s, table=0, n_packets=878, n_bytes=52680, idle_age=0, priority=65535,dl_dst=01:80:c2:00:00:0e,dl_type=0x88cc actions=CONTROLLER:65535 cookie=0x0, duration=2.861s, table=0, n_packets=0, n_bytes=0, idle_age=2, in_port=1 actions=drop cookie=0x0, duration=441.889s, table=0, n_packets=94, n_bytes=7488, idle_age=432, priority=0 actions=CONTROLLER:65535 按照匹配删除流表 12345$ ovs-ofctl del-flows s1 \"in_port=1\"mininet&gt; sh ovs-ofctl dump-flows s1NXST_FLOW reply (xid=0x4): cookie=0x0, duration=521.249s, table=0, n_packets=1036, n_bytes=62160, idle_age=0, priority=65535,dl_dst=01:80:c2:00:00:0e,dl_type=0x88cc actions=CONTROLLER:65535 cookie=0x0, duration=521.259s, table=0, n_packets=94, n_bytes=7488, idle_age=511, priority=0 actions=CONTROLLER:65535 常用匹配项 VLAN Tag 123456$ vs-ofctl add-flow s1 priority=401,in_port=1,dl_vlan=777,actions=output:2$ ovs-ofctl dump-flows s1NXST_FLOW reply (xid=0x4): cookie=0x0, duration=663.043s, table=0, n_packets=1318, n_bytes=79080, idle_age=0, priority=65535,dl_dst=01:80:c2:00:00:0e,dl_type=0x88cc actions=CONTROLLER:65535 cookie=0x0, duration=3.022s, table=0, n_packets=0, n_bytes=0, idle_age=3, priority=401,in_port=1,dl_vlan=777 actions=output:2 cookie=0x0, duration=663.053s, table=0, n_packets=94, n_bytes=7488, idle_age=653, priority=0 actions=CONTROLLER:65535 MAC 123456$ ovs-ofctl add-flow s3 in_port=1,dl_src=0a:f6:95:7e:c6:4a/0a:f6:95:7e:c6:4a,action=output:3$ ovs-ofctl add-flow s3 in_port=1,dl_dst=be:7c:6a:e9:e6:b1/be:7c:6a:e9:e6:b1,action=output:2$ sh ovs-ofctl dump-flows s3NXST_FLOW reply (xid=0x4): cookie=0x0, duration=69.067s, table=0, n_packets=0, n_bytes=0, idle_age=69, in_port=1,dl_src=0a:f6:95:7e:c6:4a/0a:f6:95:7e:c6:4a actions=output:3 cookie=0x0, duration=14.496s, table=0, n_packets=0, n_bytes=0, idle_age=14, in_port=1,dl_dst=be:7c:6a:e9:e6:b1/be:7c:6a:e9:e6:b1 actions=output:2 IP 12345678$ ovs-ofctl add-flow s3 ip,in_port=1,nw_src=192.168.0.0/16,action=drop$ ovs-ofctl add-flow s3 ip,in_port=1,nw_dst=192.168.0.0/16,action=drop$ ovs-ofctl dump-flows s3NXST_FLOW reply (xid=0x4): cookie=0x0, duration=119.033s, table=0, n_packets=119, n_bytes=7140, idle_age=0, priority=65535,dl_dst=01:80:c2:00:00:0e,dl_type=0x88cc actions=CONTROLLER:65535 cookie=0x0, duration=28.864s, table=0, n_packets=0, n_bytes=0, idle_age=28, ip,in_port=1,nw_src=192.168.0.0/16 actions=drop cookie=0x0, duration=10.036s, table=0, n_packets=0, n_bytes=0, idle_age=10, ip,in_port=1,nw_dst=192.168.0.0/16 actions=drop cookie=0x0, duration=119.057s, table=0, n_packets=90, n_bytes=7164, idle_age=109, priority=0 actions=CONTROLLER:65535 其他 匹配项 关键字 条件 举例 以太网类型 dl_type in_port=1,dl_type=0x0806,actions=output:2 协议号 nw_proto 指定dl_type=0x0800或者ip ip,in_port=1,nw_proto=1,actions=output:2 TCP flags tcp_flags 指定TCP tcp,tcp_flags=ack,actions=output:2 一些速记符 速记符 匹配项 ip dl_type=0x800 ipv6 dl_type=0x86dd icmp dl_type=0x0800,nw_proto=1 icmp6 dl_type=0x86dd,nw_proto=58 tcp dl_type=0x0800,nw_proto=6 tcp6 dl_type=0x86dd,nw_proto=6 udp dl_type=0x0800,nw_proto=17 udp6 dl_type=0x86dd,nw_proto=17 arp dl_type=0x0806 指令动作（actions） 基础动作 动作 说明 举例 normal L2/L3处理 actions=normal output 出接口 actions=output:2 group 指定的group actions=group:1 flood 从所有物理接口转发出去，除了入接口和已关闭flooding的接口 actions=flood all 从所有物理接口转发出去，除了入接口 actions=all local 转发给本地网桥 actions=local in_port 从入接口转发出去 actions=in_port controller 以packet-in消息上送给控制器 actions=controller drop 丢弃数据包 actions=drop 修改VLAN ID 关键字： mod_vlan_vid 举例 1$ ovs-ofctl add-flow s1 in_port=1,actions=mod_vlan_vid:1034,output:2 剥除VLAN 关键字： strip_vlan 举例 1$ ovs-ofctl add-flow s1 in_port=1,actions=strip_vlan,output:2 弹出最外层VLAN 关键字： pop_vlan 举例 1$ ovs-ofctl add-flow br0 in_port=1,dl_type=0x8100,dl_vlan=777,actions=pop_vlan,output:2 修改源/目的MAC 关键字：mod_dl_src / mod_dl_dst 举例 12$ ovs-ofctl add-flow s1 in_port=1,actions=mod_dl_src:01:80:c2:00:00:0e,output:2$ ovs-ofctl add-flow s1 in_port=1,actions=mod_dl_dst:01:80:c2:00:00:0e,output:2 修改源/目的IP 关键字： mod_nw_src/mod_nw_dst 举例 12$ ovs-ofctl add-flow s1 in_port=1,actions=mod_nw_src:192.168.0.10,output:2$ ovs-ofctl add-flow s1 in_port=1,actions=mod_nw_dst:192.168.0.10,output:2 修改TCP/UDP端口 关键字：mod_tp_src/mod_tp_dst 举例 1234$ ovs-ofctl add-flow s1 tcp,in_port=1,actions=mod_tp_src:1039,output:2$ ovs-ofctl add-flow s1 tcp,in_port=1,actions=mod_tp_dst:21,output:2$ ovs-ofctl add-flow s1 udp,in_port=1,actions=mod_tp_src:1039,output:2$ ovs-ofctl add-flow s1 udp,in_port=1,actions=mod_tp_dst:53,output:2 VxLan 创建VxLAN接口 12345678910111213141516171819$ ovs-vsctl add-port s3 vxlan1 -- set Interface vxlan1 type=vxlan options:remote_ip=1.1.1.1 ofport_request=2000$ ovs-vsctl show Bridge \"s3\" Controller \"tcp:10.180.9.62:6633\" Controller \"ptcp:6636\" fail_mode: secure Port \"s3-eth2\" Interface \"s3-eth2\" Port \"vxlan1\" Interface \"vxlan1\" type: vxlan options: &#123;remote_ip=\"1.1.1.1\"&#125; Port \"s3-eth3\" Interface \"s3-eth3\" Port \"s3-eth1\" Interface \"s3-eth1\" Port \"s3\" Interface \"s3\" type: internal VxLAN流表 123456$ ovs-ofctl add-flow s3 ip,in_port=1,nw_dst=192.168.0.0/16,actions=output:2000$ ovs-ofctl add-flow s3 in_port=2000,actions=output:1$ ovs-ofctl dump-flows s3NXST_FLOW reply (xid=0x4): cookie=0x0, duration=35.227s, table=0, n_packets=0, n_bytes=0, idle_age=35, ip,in_port=1,nw_dst=192.168.0.0/16 actions=output:2000 cookie=0x0, duration=2.469s, table=0, n_packets=0, n_bytes=0, idle_age=2, in_port=2000 actions=output:1 实验 拓扑 12345678910111213 +------------+ | s2 | +---+----+---+ | | +----------+ +----------+ | |+--------+--------+ +--------+--------+| s3 | | s4 |+---+---------+---+ +---+---------+---+ | | | |+---+--+ +--+---+ +---+--+ +--+---+| h1 | | h2 | | h3 | | h4 |+------+ +------+ +------+ +------+ 实验要求 h1可以与h3通信，但不可以与h2和h4通信 h2可以与h4通信，但不可以与h1和h3通信 具体操作 由h1送出的报文，在s3上打上vlan tag 1000 随后s3将报文送往s2 s2收到s3的vlan1000的报文，直接转送s4 s4收到vlan1000的报文后，剥离vlan，送到h3 h3收到请求报文后，返回响应报文，送往s4 s4收到h3的报文后，打上vlan tag 1000 随后s4将报文送往s2 s2收到s4的vlan1000的报文，直接送往s3 s3收到s2的vlan1000报文后，剥离vlan，送往h1 h1-h3流表 1234567891011121314$ ovs-ofctl dump-flows s2NXST_FLOW reply (xid=0x4): cookie=0x0, duration=1664.027s, table=0, n_packets=79, n_bytes=4026, idle_age=803, in_port=1,dl_vlan=1000 actions=output:2 cookie=0x0, duration=1642.112s, table=0, n_packets=10, n_bytes=852, idle_age=803, in_port=2,dl_vlan=1000 actions=output:1$ ovs-ofctl dump-flows s3NXST_FLOW reply (xid=0x4): cookie=0x0, duration=2826.459s, table=0, n_packets=330, n_bytes=14700, idle_age=807, in_port=1 actions=mod_vlan_vid:1000,output:3 cookie=0x0, duration=1895.062s, table=0, n_packets=10, n_bytes=852, idle_age=807, in_port=3,dl_vlan=1000 actions=strip_vlan,output:1$ ovs-ofctl dump-flows s4NXST_FLOW reply (xid=0x4): cookie=0x0, duration=2776.175s, table=0, n_packets=229, n_bytes=10010, idle_age=810, in_port=1 actions=mod_vlan_vid:1000,output:3 cookie=0x0, duration=1507.500s, table=0, n_packets=10, n_bytes=852, idle_age=810, in_port=3,dl_vlan=1000 actions=strip_vlan,output:1 h2-h4流表 1234567891011121314$ ovs-ofctl dump-flows s2NXST_FLOW reply (xid=0x4): cookie=0x0, duration=827.167s, table=0, n_packets=60, n_bytes=2816, idle_age=698, in_port=2,dl_vlan=2000 actions=output:1 cookie=0x0, duration=812.342s, table=0, n_packets=60, n_bytes=2816, idle_age=698, in_port=1,dl_vlan=2000 actions=output:2 $ ovs-ofctl dump-flows s3NXST_FLOW reply (xid=0x4): cookie=0x0, duration=1347.703s, table=0, n_packets=60, n_bytes=2576, idle_age=702, in_port=2 actions=mod_vlan_vid:2000,output:3 cookie=0x0, duration=710.639s, table=0, n_packets=3, n_bytes=194, idle_age=702, in_port=3,dl_vlan=2000 actions=strip_vlan,output:2 $ ovs-ofctl dump-flows s4NXST_FLOW reply (xid=0x4): cookie=0x0, duration=1133.570s, table=0, n_packets=60, n_bytes=2576, idle_age=705, in_port=2 actions=mod_vlan_vid:2000,output:3 cookie=0x0, duration=1088.590s, table=0, n_packets=60, n_bytes=2816, idle_age=705, in_port=3,dl_vlan=2000 actions=strip_vlan,output:2 命令列表 h1-h3 123456ovs-ofctl add-flow s2 in_port=1,dl_vlan=1000,actions=output:2ovs-ofctl add-flow s2 in_port=2,dl_vlan=1000,actions=output:1ovs-ofctl add-flow s3 in_port=1,actions=mod_vlan_vid:1000,output:3ovs-ofctl add-flow s3 in_port=3,dl_vlan=1000,actions=strip_vlan,output:1ovs-ofctl add-flow s4 in_port=1,actions=mod_vlan_vid:1000,output:3ovs-ofctl add-flow s4 in_port=3,dl_vlan=1000,actions=strip_vlan,output:1 h2-h4 123456ovs-ofctl add-flow s2 in_port=2,dl_vlan=2000,actions=output:1ovs-ofctl add-flow s2 in_port=1,dl_vlan=2000,actions=output:2ovs-ofctl add-flow s3 in_port=2,actions=mod_vlan_vid:2000,output:3ovs-ofctl add-flow s3 in_port=3,dl_vlan=2000,actions=strip_vlan,output:2ovs-ofctl add-flow s4 in_port=2,actions=mod_vlan_vid:2000,output:3ovs-ofctl add-flow s4 in_port=3,dl_vlan=2000,actions=strip_vlan,output:2","categories":[{"name":"network","slug":"network","permalink":"http://www.isimble.com/categories/network/"}],"tags":[{"name":"ovs","slug":"ovs","permalink":"http://www.isimble.com/tags/ovs/"},{"name":"network","slug":"network","permalink":"http://www.isimble.com/tags/network/"}]},{"title":"Mininet自定义topo","slug":"mininet-user-define-topo","date":"2018-11-08T01:04:26.000Z","updated":"2018-11-08T09:09:23.533Z","comments":true,"path":"2018/11/08/mininet-user-define-topo/","link":"","permalink":"http://www.isimble.com/2018/11/08/mininet-user-define-topo/","excerpt":"mininet自带topo 通过mn -h可以看到Mininet自带的几种topo类型，分别有Linear, minimal, reversed, single, torus和tree类型，但有时候这些类型无法满足需求，需要自定义topo","text":"mininet自带topo 通过mn -h可以看到Mininet自带的几种topo类型，分别有Linear, minimal, reversed, single, torus和tree类型，但有时候这些类型无法满足需求，需要自定义topo 12345678910$ mn --helpUsage: mn [options](type mn -h for details)... --topo=TOPO linear|minimal|reversed|single|torus|tree[,param=value ...] linear=LinearTopo torus=TorusTopo tree=TreeTopo single=SingleSwitchTopo reversed=SingleSwitchReversedTopo minimal=MinimalTopo 获取示例 Mininet提供了topo-2sw-2host的示例，可以通过Mininet github的custom目录下获取 12345678910111213141516171819202122232425262728293031\"\"\"Custom topology exampleTwo directly connected switches plus a host for each switch: host --- switch --- switch --- hostAdding the 'topos' dict with a key/value pair to generate our newly definedtopology enables one to pass in '--topo=mytopo' from the command line.\"\"\"from mininet.topo import Topoclass MyTopo( Topo ): \"Simple topology example.\" def __init__( self ): \"Create custom topo.\" # Initialize topology Topo.__init__( self ) # Add hosts and switches leftHost = self.addHost( 'h1' ) rightHost = self.addHost( 'h2' ) leftSwitch = self.addSwitch( 's3' ) rightSwitch = self.addSwitch( 's4' ) # Add links self.addLink( leftHost, leftSwitch ) self.addLink( leftSwitch, rightSwitch ) self.addLink( rightSwitch, rightHost )topos = &#123; 'mytopo': ( lambda: MyTopo() ) &#125; 自定义topo 如要创建如下topo 1234567891011 +------------+ +----------+ s3 +---------+ | +------------+ | | | +----+----+ +----+----+ +-----+ s1 +-----+ +-----+ s2 +-----+ | +---------+ | | +---------+ | | | | |+--+--+ +--+--+ +--+--+ +--+--+| h1 | | h2 | | h3 | | h4 |+-----+ +-----+ +-----+ +-----+ 代码 123456789101112131415161718192021222324252627282930from mininet.topo import Topoclass MyTopo( Topo ): \"Simple topology example.\" def __init__( self ): \"Create custom topo.\" # Initialize topology Topo.__init__( self ) # Add hosts and switches h1 = self.addHost( 'h1' ) h2 = self.addHost( 'h2' ) h3 = self.addHost( 'h3' ) h4 = self.addHost( 'h4' ) s1 = self.addSwitch( 's1' ) s2 = self.addSwitch( 's2' ) s3 = self.addSwitch( 's3' ) # Add links self.addLink( h1, s1 ) self.addLink( s1, h2 ) self.addLink( s1, s3 ) self.addLink( s3, s2 ) self.addLink( h3, s2 ) self.addLink( s2, h4 )topos = &#123; 'mytopo': ( lambda: MyTopo() ) &#125; 使用自定义topo 1234567891011121314151617root@mininet:~# mn --custom ./testtopo.py --topo mytopo --controller=remote,ip=127.0.0.1,port=6633*** Creating network*** Adding controller*** Adding hosts:h1 h2 h3 h4*** Adding switches:s1 s2 s3*** Adding links:(h1, s1) (h3, s2) (s1, h2) (s1, s3) (s2, h4) (s3, s2)*** Configuring hostsh1 h2 h3 h4*** Starting controllerc0*** Starting 3 switchess1 s2 s3 ...*** Starting CLI:mininet&gt; 查看连接状态 123456789101112131415mininet&gt; linksh1-eth0&lt;-&gt;s1-eth1 (OK OK)h3-eth0&lt;-&gt;s2-eth2 (OK OK)s1-eth2&lt;-&gt;h2-eth0 (OK OK)s1-eth3&lt;-&gt;s3-eth1 (OK OK)s2-eth3&lt;-&gt;h4-eth0 (OK OK)s3-eth2&lt;-&gt;s2-eth1 (OK OK)mininet&gt; pingall*** Ping: testing ping reachabilityh1 -&gt; h2 h3 h4h2 -&gt; h1 h3 h4h3 -&gt; h1 h2 h4h4 -&gt; h1 h2 h3*** Results: 0% dropped (12/12 received)mininet&gt;","categories":[{"name":"sdn","slug":"sdn","permalink":"http://www.isimble.com/categories/sdn/"}],"tags":[{"name":"network","slug":"network","permalink":"http://www.isimble.com/tags/network/"},{"name":"sdn","slug":"sdn","permalink":"http://www.isimble.com/tags/sdn/"}]},{"title":"Opendaylight Oxygen环境准备","slug":"odl-setup","date":"2018-11-07T06:53:08.000Z","updated":"2018-11-07T15:12:37.548Z","comments":true,"path":"2018/11/07/odl-setup/","link":"","permalink":"http://www.isimble.com/2018/11/07/odl-setup/","excerpt":"安装Opendaylight 环境准备 基础包 1$ apt-get install unzip lrzsz 安装jdk 1$ apt-get install openjdk-8-jdk 设置JAVA_HOME 在/etc/environment的末尾添加JAVA_HOME=&quot;/usr/lib/jvm/java-8-openjdk-amd64&quot;，需要退出当前终端重新登陆","text":"安装Opendaylight 环境准备 基础包 1$ apt-get install unzip lrzsz 安装jdk 1$ apt-get install openjdk-8-jdk 设置JAVA_HOME 在/etc/environment的末尾添加JAVA_HOME=&quot;/usr/lib/jvm/java-8-openjdk-amd64&quot;，需要退出当前终端重新登陆 获取安装包 https://docs.opendaylight.org/en/latest/downloads.html 运行karaf 123456789101112131415161718192021$ unzip karaf-0.8.3.zip ...$ cd karaf-0.8.3/$ ./bin/karafApache Karaf starting up. Press Enter to open the shell now...100% [========================================================================]Karaf started in 1s. Bundle stats: 54 active, 55 total ________ ________ .__ .__ .__ __ \\_____ \\ ______ ____ ____ \\______ \\ _____ ___.__.| | |__| ____ | |___/ |_ / | \\\\____ \\_/ __ \\ / \\ | | \\\\__ \\&lt; | || | | |/ ___\\| | \\ __\\ / | \\ |_&gt; &gt; ___/| | \\| ` \\/ __ \\\\___ || |_| / /_/ &gt; Y \\ | \\_______ / __/ \\___ &gt;___| /_______ (____ / ____||____/__\\___ /|___| /__| \\/|__| \\/ \\/ \\/ \\/\\/ /_____/ \\/Hit '&lt;tab&gt;' for a list of available commandsand '[cmd] --help' for help on a specific command.Hit '&lt;ctrl-d&gt;' or type 'system:shutdown' or 'logout' to shutdown OpenDaylight.opendaylight-user@root&gt; 安装feature 说起来，这真的是一件让人崩溃的事情，不同的版本，安装feature不同，在什么都还不懂的情况下安装feature，遇到了无数的问题，终于当我将要换到更老的版本之前（0.7.3），让我找到了**Oxygen（0.8.3）**的feature 1opendaylight-user@root&gt;feature:install odl-restconf odl-l2switch-switch odl-dlux-core odl-dluxapps-nodes odl-dluxapps-topology odl-dluxapps-yangui odl-dluxapps-yangvisualizer odl-dluxapps-yangman 我有必要把这些feature再次列出来 12345678feature:install odl-restconffeature:install odl-l2switch-switch feature:install odl-dlux-core feature:install odl-dluxapps-nodes feature:install odl-dluxapps-topology feature:install odl-dluxapps-yangui feature:install odl-dluxapps-yangvisualizer feature:install odl-dluxapps-yangman 务必按照顺序安装，如果出现错误，删了目录重新来过吧 登陆Web 使用admin:admin登陆即可","categories":[{"name":"sdn","slug":"sdn","permalink":"http://www.isimble.com/categories/sdn/"}],"tags":[{"name":"network","slug":"network","permalink":"http://www.isimble.com/tags/network/"},{"name":"sdn","slug":"sdn","permalink":"http://www.isimble.com/tags/sdn/"},{"name":"opendaylight","slug":"opendaylight","permalink":"http://www.isimble.com/tags/opendaylight/"}]},{"title":"Mininet基础使用","slug":"mininet-base","date":"2018-11-06T19:00:33.000Z","updated":"2018-11-07T03:09:12.588Z","comments":true,"path":"2018/11/07/mininet-base/","link":"","permalink":"http://www.isimble.com/2018/11/07/mininet-base/","excerpt":"Mininet是什么 Mininet是一个网络模拟器，可以创建虚拟主机，交换机，控制器和链接的网络。 Mininet的交换机支持OpenFlow，可实现高度灵活的自定义路由和SDN，为开发和测试SDN提供实验环境","text":"Mininet是什么 Mininet是一个网络模拟器，可以创建虚拟主机，交换机，控制器和链接的网络。 Mininet的交换机支持OpenFlow，可实现高度灵活的自定义路由和SDN，为开发和测试SDN提供实验环境 Mininet用途 为开发OpenFlow应用程序提供简单而廉价的网络测试平台 允许多个并发开发人员在同一拓扑上独立工作 支持系统级回归测试，这些测试可重复且易于打包 支持复杂的拓扑测试，无需连接物理网络 包括具有拓扑感知和OpenFlow感知的CLI，用于调试或运行网络范围的测试 支持任意自定义拓扑，并包括一组基本的参数化拓扑 可以在没有编程的情况下开箱即用 提供了一个简单易用的**Python API，**用于网络创建和实验 安装 Ubuntu16.04 1apt install mininet 测试 1sudo mn --test pingall 使用 常用命令 进入mininet命令行模式 12345678910111213141516171819root@mininet:~# mn*** No default OpenFlow controller found for default switch!*** Falling back to OVS Bridge*** Creating network*** Adding controller*** Adding hosts:h1 h2*** Adding switches:s1*** Adding links:(h1, s1) (h2, s1)*** Configuring hostsh1 h2*** Starting controller*** Starting 1 switchess1 ...*** Starting CLI:mininet&gt; 查看节点 1234mininet&gt; nodesavailable nodes are:h1 h2 s1mininet&gt; 可以看到当前包含3个节点，包括两个host，一个switch 查看连接状况 12345mininet&gt; neth1 h1-eth0:s1-eth1h2 h2-eth0:s1-eth2s1 lo: s1-eth1:h1-eth0 s1-eth2:h2-eth0mininet&gt; h1的eth0与s1的eth1相连 h2的eth0与s1的eth2相连 查看详细信息 12345mininet&gt; dump&lt;Host h1: h1-eth0:10.0.0.1 pid=6896&gt;&lt;Host h2: h2-eth0:10.0.0.2 pid=6899&gt;&lt;OVSBridge s1: lo:127.0.0.1,s1-eth1:None,s1-eth2:None pid=6905&gt;mininet&gt; 环境清理 123456789101112131415161718192021222324# mn -c*** Removing excess controllers/ofprotocols/ofdatapaths/pings/noxeskillall controller ofprotocol ofdatapath ping nox_core lt-nox_core ovs-openflowd ovs-controller udpbwtest mnexec ivs 2&gt; /dev/nullkillall -9 controller ofprotocol ofdatapath ping nox_core lt-nox_core ovs-openflowd ovs-controller udpbwtest mnexec ivs 2&gt; /dev/nullpkill -9 -f \"sudo mnexec\"*** Removing junk from /tmprm -f /tmp/vconn* /tmp/vlogs* /tmp/*.out /tmp/*.log*** Removing old X11 tunnels*** Removing excess kernel datapathsps ax | egrep -o 'dp[0-9]+' | sed 's/dp/nl:/'*** Removing OVS datapathsovs-vsctl --timeout=1 list-brovs-vsctl --timeout=1 list-br*** Removing all links of the pattern foo-ethXip link show | egrep -o '([-_.[:alnum:]]+-eth[[:digit:]]+)'ip link show*** Killing stale mininet node processespkill -9 -f mininet:*** Shutting down stale tunnelspkill -9 -f Tunnel=Ethernetpkill -9 -f .ssh/mnrm -f ~/.ssh/mn/**** Cleanup complete.root@mininet:~# 节点命令 1234567891011121314151617181920mininet&gt; h1 ifconfigh1-eth0 Link encap:Ethernet HWaddr a2:5f:da:ed:6a:74 inet addr:10.0.0.1 Bcast:10.255.255.255 Mask:255.0.0.0 inet6 addr: fe80::a05f:daff:feed:6a74/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:15 errors:0 dropped:0 overruns:0 frame:0 TX packets:8 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:1206 (1.2 KB) TX bytes:648 (648.0 B)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)mininet&gt; Run http server 1mininet&gt; h1 python -m SimpleHTTPServer 80 &amp; Http client 123456789101112mininet&gt; h2 wget h1--2018-11-07 10:52:29-- http://10.0.0.1/Connecting to 10.0.0.1:80... connected.HTTP request sent, awaiting response... 200 OKLength: 370 [text/html]Saving to: 'index.html'index.html 100%[===================&gt;] 370 --.-KB/s in 0s2018-11-07 10:52:29 (62.3 MB/s) - 'index.html' saved [370/370]mininet&gt;","categories":[{"name":"sdn","slug":"sdn","permalink":"http://www.isimble.com/categories/sdn/"}],"tags":[{"name":"network","slug":"network","permalink":"http://www.isimble.com/tags/network/"},{"name":"sdn","slug":"sdn","permalink":"http://www.isimble.com/tags/sdn/"}]},{"title":"OpenvSwitch学习笔记1","slug":"ovs-study-overview","date":"2018-09-20T21:46:41.000Z","updated":"2018-09-21T05:53:35.313Z","comments":true,"path":"2018/09/21/ovs-study-overview/","link":"","permalink":"http://www.isimble.com/2018/09/21/ovs-study-overview/","excerpt":"功能支持 VLAN with trunk and access ports 绑定NIC(with/without LAC) 可见性：NetFlow, sFlow®, and mirroring QoS Geneve, GRE, VXLAN, STT, and LISP 隧道 连接故障管理 openflow 1.0及其扩展 使用Linux内核进行高性能转发","text":"功能支持 VLAN with trunk and access ports 绑定NIC(with/without LAC) 可见性：NetFlow, sFlow®, and mirroring QoS Geneve, GRE, VXLAN, STT, and LISP 隧道 连接故障管理 openflow 1.0及其扩展 使用Linux内核进行高性能转发 主要组成部分 ovs-vswitchd: 实现交换功能的守护进程，与Linux内核模块实现flow-based switching ovsdb-server: 用以保存ovs配置信息的轻量级的数据库 ovs-dpctl: 用以配置交换机内核模块的工具 ovs-vsctl: 查看和更新ovs配置信息的工具 ovs-ofctl: 配置和查看OpenFlow的控制和交换。主要用来操作OpenFlow流表 场景 多服务器虚拟化部署场景 高动态的end-points 维护的是逻辑抽象 状态迁移 响应网络动态修改 维护逻辑标签 包处理流程 如上图 当包被从物理网卡上收到之后，如果是第一次收到包，ovs的kernel datapath不知道该如何处理，于是，将其送往ovs-vswitchd。 ovs-vswitchd决定这个包该如何处理之后，回送到kernel datapath kernel datapath根据ovs-vswitchd执行相应的动作，并缓存这个动作 当再次收到相关包之后，kernel datapath已经存在之前缓存好的动作，则直接执行该动作 包处理流程 因为Flow table在内核中有一份，当从物理网卡收到包后，通过key查找内核中的flow table，即可以得到action，然后执行action 如果没有查找到，则通过upcall调用，将数据包以netlink协议上传到vswitchd vswitchd将数据包在ovsdb中进行查表匹配，若能匹配，则转到第五步 若不能匹配，则通过 OpenFlow协议与控制器通信，控制器下发流表项，Vswitchd解析流表项得到相应的动作，同时将流表存入ovsdb。 将匹配的流表项通过netlink下发到内核的Flow-table中 通过reinject，使用netlink将包重新送回内核 匹配流表项并根据相应的动作执行","categories":[{"name":"network","slug":"network","permalink":"http://www.isimble.com/categories/network/"}],"tags":[{"name":"ovs","slug":"ovs","permalink":"http://www.isimble.com/tags/ovs/"},{"name":"network","slug":"network","permalink":"http://www.isimble.com/tags/network/"},{"name":"openvswitch","slug":"openvswitch","permalink":"http://www.isimble.com/tags/openvswitch/"}]},{"title":"Hexo开启disqus评论系统","slug":"hexo-enable-disqus","date":"2018-09-12T17:40:20.000Z","updated":"2019-11-18T14:11:01.042Z","comments":true,"path":"2018/09/13/hexo-enable-disqus/","link":"","permalink":"http://www.isimble.com/2018/09/13/hexo-enable-disqus/","excerpt":"之前一直使用Hexo的Next主题，开启的是Valine评论系统。有博文来说明配置过程。然而，一个主题再好，看的时间久了，还是想要换换。于是使用了现在的MaterialFlow，刚好也换换评论系统 相比Valine，disqus开启真的是太简单了","text":"之前一直使用Hexo的Next主题，开启的是Valine评论系统。有博文来说明配置过程。然而，一个主题再好，看的时间久了，还是想要换换。于是使用了现在的MaterialFlow，刚好也换换评论系统 相比Valine，disqus开启真的是太简单了 注册disqus账号 其实我很早之前就注册了账号，直接使用facebook关联过去的. 网站上开启 登陆disqus后，在首页点击右侧设置-Add Disqus To Site 在随后弹出的页面点击Start - I want to install Disqus on my site 配置Hexo 修改hexo的_config.yml，增加disqus_shortname 1disqus_shortname: your_shortname 大功告成 欢迎大家留言","categories":[{"name":"hexo","slug":"hexo","permalink":"http://www.isimble.com/categories/hexo/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://www.isimble.com/tags/hexo/"}]},{"title":"openstack接口状态异常","slug":"openstack-neutron-problem1","date":"2018-09-11T01:52:40.000Z","updated":"2018-09-11T09:57:14.404Z","comments":true,"path":"2018/09/11/openstack-neutron-problem1/","link":"","permalink":"http://www.isimble.com/2018/09/11/openstack-neutron-problem1/","excerpt":"错误信息 neutron错误日志Exit code: 2; Stdin: ; Stdout: ; Stderr: sudo: unable to resolve host pxea4badb2 问题现象 openstack的router中创建了一个联通两个vlan的路由器，发现两个接口轮流处于build状态","text":"错误信息 neutron错误日志Exit code: 2; Stdin: ; Stdout: ; Stderr: sudo: unable to resolve host pxea4badb2 问题现象 openstack的router中创建了一个联通两个vlan的路由器，发现两个接口轮流处于build状态 查看日志 /var/log/neutron/neutron-linuxbridge-agent.log 1234567891011121314151617181920212223242526115265 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent File \"/usr/lib/python2.7/dist-packages/oslo_utils/excutils.py\", line 196, in force_reraise115266 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent six.reraise(self.type_, self.value, self.tb)115267 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent File \"/usr/lib/python2.7/dist-packages/neutron/plugins/ml2/drivers/linuxbridge/agent/linuxbridge_neut ron_agent.py\", line 419, in add_tap_interface115268 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent tap_device_name, device_owner)115269 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent File \"/usr/lib/python2.7/dist-packages/neutron/plugins/ml2/drivers/linuxbridge/agent/linuxbridge_neut ron_agent.py\", line 451, in _add_tap_interface115270 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent segmentation_id)115271 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent File \"/usr/lib/python2.7/dist-packages/neutron/plugins/ml2/drivers/linuxbridge/agent/linuxbridge_neut ron_agent.py\", line 403, in ensure_physical_in_bridge115272 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent physical_interface)115273 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent File \"/usr/lib/python2.7/dist-packages/neutron/plugins/ml2/drivers/linuxbridge/agent/linuxbridge_neut ron_agent.py\", line 221, in ensure_flat_bridge115274 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent gateway):115275 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent File \"/usr/lib/python2.7/dist-packages/neutron/plugins/ml2/drivers/linuxbridge/agent/linuxbridge_neut ron_agent.py\", line 362, in ensure_bridge115276 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent self.update_interface_ip_details(bridge_name, interface, ips, gateway)115277 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent File \"/usr/lib/python2.7/dist-packages/neutron/plugins/ml2/drivers/linuxbridge/agent/linuxbridge_neut ron_agent.py\", line 301, in update_interface_ip_details115278 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent dst_device.addr.add(cidr=ip['cidr'])115279 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent File \"/usr/lib/python2.7/dist-packages/neutron/agent/linux/ip_lib.py\", line 597, in add115280 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent self._as_root([net.version], tuple(args))115281 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent File \"/usr/lib/python2.7/dist-packages/neutron/agent/linux/ip_lib.py\", line 384, in _as_root115282 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent use_root_namespace=use_root_namespace)115283 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent File \"/usr/lib/python2.7/dist-packages/neutron/agent/linux/ip_lib.py\", line 96, in _as_root115284 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent log_fail_as_error=self.log_fail_as_error)115285 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent File \"/usr/lib/python2.7/dist-packages/neutron/agent/linux/ip_lib.py\", line 105, in _execute115286 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent log_fail_as_error=log_fail_as_error)115287 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent File \"/usr/lib/python2.7/dist-packages/neutron/agent/linux/utils.py\", line 146, in execute115288 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent raise ProcessExecutionError(msg, returncode=returncode)115289 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent ProcessExecutionError: Exit code: 2; Stdin: ; Stdout: ; Stderr: sudo: unable to resolve host pxea4badb2 17413115290 2018-09-11 16:59:59.274 1553 ERROR neutron.plugins.ml2.drivers.agent._common_agent RTNETLINK answers: File exists 问题原因 sudo: unable to resolve host pxea4badb2 无法解析主机名 查看/etc/hosts中缺少了本地hostname的IP地址映射 Exit code: 2; Stdin: ; Stdout: ; Stderr: 问题比较不直观，google之后，有人说是因为bridge上配置了IP，导致冲突了 经查看controller上，发现一个bridge和一个无力网卡配置了同样的IP，可能是因为之前使用flat网络后切换为vlan网络，没有清理openstack中创建的配置导致。 将bridge上的IP地址删除后，恢复正常","categories":[{"name":"cloud","slug":"cloud","permalink":"http://www.isimble.com/categories/cloud/"}],"tags":[{"name":"openstack","slug":"openstack","permalink":"http://www.isimble.com/tags/openstack/"}]},{"title":"Ubuntu在Openstack中启动密码注入不生效问题定位1","slug":"openstack-metadata","date":"2018-09-10T22:17:32.000Z","updated":"2018-09-11T06:20:19.516Z","comments":true,"path":"2018/09/11/openstack-metadata/","link":"","permalink":"http://www.isimble.com/2018/09/11/openstack-metadata/","excerpt":"问题说明 使用Ubuntu的cloud镜像创建实例，在创建时使用脚本修改密码","text":"问题说明 使用Ubuntu的cloud镜像创建实例，在创建时使用脚本修改密码 12345#!/bin/bashpasswd ubuntu&lt;&lt;EOFubuntuubuntuEOF 系统启动后，发现密码修改未成功 定位思路 查看实例启动日志 ➡️ 查看openstack的neutron agent状态 ➡️ 查看meta-data服务日志 step by step 查看实例启动日志 123[ 731.435163] cloud-init[845]: 2018-09-11 01:42:24,574 - url_helper.py[WARNING]: Calling 'http://169.254.169.254/2009-04-04/meta-data/instance-id' failed [119/120s]: request error [HTTPConnectionPool(host='169.254.169.254', port=80): Max retries exceeded with url: /2009-04-04/meta-data/instance-id (Caused by NewConnectionError('&lt;requests.packages.urllib3.connection.HTTPConnection object at 0x7fa26f1eb6d8&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',))][ 738.460169] cloud-init[845]: 2018-09-11 01:42:31,597 - DataSourceEc2.py[CRITICAL]: Giving up on md from ['http://169.254.169.254/2009-04-04/meta-data/instance-id'] after 126 seconds[ 738.470572] cloud-init[845]: 2018-09-11 01:42:31,608 - util.py[WARNING]: Getting data from &lt;class 'cloudinit.sources.DataSourceCloudStack.DataSourceCloudStack'&gt; failed 以上日志说明cloud-init在向meta-data server获取meta-data时失败 查看neutron agent状态 123456789101112root@pxea4badb217413:~# neutron agent-list+--------------------------------------+--------------------+-----------------+-------------------+-------+----------------+---------------------------+| id | agent_type | host | availability_zone | alive | admin_state_up | binary |+--------------------------------------+--------------------+-----------------+-------------------+-------+----------------+---------------------------+| 459740a7-b60c-4db7-9ee0-3b45d6c3b2de | Linux bridge agent | pxe1418773526e5 | | :-) | True | neutron-linuxbridge-agent || 4ec74c6a-c392-4216-bddd-789ec5aa3d86 | DHCP agent | pxea4badb217413 | nova | :-) | True | neutron-dhcp-agent || 6d992669-fbe6-412c-a717-5c2b61a2b901 | Metadata agent | pxea4badb217413 | | :-) | True | neutron-metadata-agent || 77dc04a5-0e43-4907-be3a-6f165e77807c | Linux bridge agent | pxe74867aee16bc | | :-) | True | neutron-linuxbridge-agent || c54c7593-5973-41ed-86c8-7e22ac1e95ec | Linux bridge agent | pxea4badb217413 | | :-) | True | neutron-linuxbridge-agent || e882e27d-45de-4cb3-9284-7e9101e2b39b | L3 agent | pxea4badb217413 | nova | :-) | True | neutron-l3-agent |+--------------------------------------+--------------------+-----------------+-------------------+-------+----------------+---------------------------+root@pxea4badb217413:~# 检查neutron-metadata日志 123456789101112131415161718192021222018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent [-] Failed reporting state!2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent Traceback (most recent call last):2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent File \"/usr/lib/python2.7/dist-packages/neutron/agent/metadata/agent.py\", line 266, in _report_state2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent use_call=self.agent_state.get('start_flag'))2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent File \"/usr/lib/python2.7/dist-packages/neutron/agent/rpc.py\", line 87, in report_state2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent return method(context, 'report_state', **kwargs)2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent File \"/usr/lib/python2.7/dist-packages/oslo_messaging/rpc/client.py\", line 158, in call2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent retry=self.retry)2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent File \"/usr/lib/python2.7/dist-packages/oslo_messaging/transport.py\", line 90, in _send2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent timeout=timeout, retry=retry)2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent File \"/usr/lib/python2.7/dist-packages/oslo_messaging/_drivers/amqpdriver.py\", line 470, in send2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent retry=retry)2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent File \"/usr/lib/python2.7/dist-packages/oslo_messaging/_drivers/amqpdriver.py\", line 459, in _send2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent result = self._waiter.wait(msg_id, timeout)2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent File \"/usr/lib/python2.7/dist-packages/oslo_messaging/_drivers/amqpdriver.py\", line 342, in wait2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent message = self.waiters.get(msg_id, timeout=timeout)2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent File \"/usr/lib/python2.7/dist-packages/oslo_messaging/_drivers/amqpdriver.py\", line 244, in get2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent 'to message ID %s' % msg_id)2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent MessagingTimeout: Timed out waiting for a reply to message ID 5b145001c0a34d37a68337821e64908f2018-09-05 19:07:06.404 10617 ERROR neutron.agent.metadata.agent2018-09-05 19:07:06.405 10617 WARNING oslo.service.loopingcall [-] Function 'neutron.agent.metadata.agent.UnixDomainMetadataProxy._report_state' run outlasted interval by 30.00 sec 12342018-09-06 16:23:48.658 1238 ERROR oslo.messaging._drivers.impl_rabbit [-] AMQP server on controller:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 1 seconds.2018-09-06 16:23:49.752 1238 ERROR oslo.messaging._drivers.impl_rabbit [-] AMQP server on controller:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 2 seconds.2018-09-06 16:23:51.766 1238 ERROR oslo.messaging._drivers.impl_rabbit [-] AMQP server on controller:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 4 seconds.2018-09-06 16:23:55.779 1238 ERROR oslo.messaging._drivers.impl_rabbit [-] AMQP server on controller:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 6 seconds. 以上显示为AMQP server连接不到 检查meta-data的配置 123456789 /etc/neutron/metadata_agent.ini # IP address used by Nova metadata server. (string value)#nova_metadata_ip = 127.0.0.1nova_metadata_ip = controller# TCP Port used by Nova metadata server. (port value)# Minimum value: 0# Maximum value: 65535#nova_metadata_port = 8775 查看controller上端口监听状况 1234567891011121314151617181920212223242526272829303132root@pxea4badb217413:~# netstat -lnpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1149/sshdtcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN 2316/exim4tcp 0 0 127.0.0.1:6010 0.0.0.0:* LISTEN 18284/12tcp 0 0 127.0.0.1:6011 0.0.0.0:* LISTEN 18861/20tcp 0 0 0.0.0.0:9696 0.0.0.0:* LISTEN 1243/pythontcp 0 0 0.0.0.0:6080 0.0.0.0:* LISTEN 1247/pythontcp 0 0 0.0.0.0:8774 0.0.0.0:* LISTEN 1237/pythontcp 0 0 0.0.0.0:8775 0.0.0.0:* LISTEN 1237/pythontcp 0 0 0.0.0.0:9191 0.0.0.0:* LISTEN 1242/pythontcp 0 0 0.0.0.0:25672 0.0.0.0:* LISTEN 2922/beam.smptcp 0 0 0.0.0.0:8776 0.0.0.0:* LISTEN 1235/pythontcp 0 0 10.160.17.196:3306 0.0.0.0:* LISTEN 1769/mysqldtcp 0 0 10.160.17.196:11211 0.0.0.0:* LISTEN 2443/memcachedtcp 0 0 0.0.0.0:9292 0.0.0.0:* LISTEN 1246/pythontcp6 0 0 :::21 :::* LISTEN 833/vsftpdtcp6 0 0 :::22 :::* LISTEN 1149/sshdtcp6 0 0 ::1:25 :::* LISTEN 2316/exim4tcp6 0 0 ::1:6010 :::* LISTEN 18284/12tcp6 0 0 ::1:6011 :::* LISTEN 18861/20tcp6 0 0 :::35357 :::* LISTEN 3904/apache2tcp6 0 0 :::5000 :::* LISTEN 3904/apache2tcp6 0 0 :::80 :::* LISTEN 3904/apache2tcp6 0 0 :::8081 :::* LISTEN 4749/cmatcp6 0 0 :::4369 :::* LISTEN 2586/epmdudp 0 0 0.0.0.0:123 0.0.0.0:* 3218/chronydudp 0 0 0.0.0.0:161 0.0.0.0:* 3189/snmpdudp 0 0 0.0.0.0:323 0.0.0.0:* 3218/chronydudp6 0 0 :::123 :::* 3218/chronydudp6 0 0 :::323 :::* 3218/chronyd 以上，发现5762端口未监听 尝试启动rabbitmq-server 123service rabbitmq-server start * Starting RabbitMQ Messaging Server rabbitmq-server * RabbitMQ Messaging Server already running [ OK ] 再次查看端口监听状态 1tcp6 0 0 :::5672 :::* LISTEN 2922/beam.smp 重启meta-data的service，发现日志已经不再报错了 然而，这竟然不是最终原因，最终的问题发现是网络问题","categories":[{"name":"cloud","slug":"cloud","permalink":"http://www.isimble.com/categories/cloud/"}],"tags":[{"name":"openstack","slug":"openstack","permalink":"http://www.isimble.com/tags/openstack/"}]},{"title":"sqlalchemy连接docker的mysql问题记录","slug":"sqlalchemy-mysql-docker","date":"2018-08-14T19:42:30.000Z","updated":"2018-08-15T03:46:05.443Z","comments":true,"path":"2018/08/15/sqlalchemy-mysql-docker/","link":"","permalink":"http://www.isimble.com/2018/08/15/sqlalchemy-mysql-docker/","excerpt":"又开始鼓捣flask+mysql了 👶 不成想，又一次踩了好多雷 😂 有一种打怪升级的感觉","text":"又开始鼓捣flask+mysql了 👶 不成想，又一次踩了好多雷 😂 有一种打怪升级的感觉 环境说明 Mac+flask docker+mysql 问题1 create_engine报错 错误信息 1234567891011&gt;&gt;&gt; from sqlalchemy import create_engine&gt;&gt;&gt; engine = create_engine('mysql+mysqldb://root:123456@localhost/beta_monitor')Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; File \"/Users/bqi/venv/beta-monitor/lib/python2.7/site-packages/sqlalchemy/engine/__init__.py\", line 424, in create_engine return strategy.create(*args, **kwargs) File \"/Users/bqi/venv/beta-monitor/lib/python2.7/site-packages/sqlalchemy/engine/strategies.py\", line 81, in create dbapi = dialect_cls.dbapi(**dbapi_args) File \"/Users/bqi/venv/beta-monitor/lib/python2.7/site-packages/sqlalchemy/dialects/mysql/mysqldb.py\", line 102, in dbapi return __import__('MySQLdb')ImportError: No module named MySQLdb 问题原因 没有安装python的mysql包，需要安装mysql-python和/或mysqlclient，然后就遇到了第二个问题 问题2 pip install mysql-python失败 错误信息 123456789101112131415161718⇒ pip install mysql-pythonCollecting mysql-python Downloading https://files.pythonhosted.org/packages/a5/e9/51b544da85a36a68debe7a7091f068d802fc515a3a202652828c73453cad/MySQL-python-1.2.5.zip (108kB) 100% |████████████████████████████████| 112kB 153kB/s Complete output from command python setup.py egg_info: sh: mysql_config: command not found Traceback (most recent call last): File \"&lt;string&gt;\", line 1, in &lt;module&gt; File \"/private/var/folders/3z/tqw46wwj7xb1d2ftp578x5vm0000gn/T/pip-install-ViJMnc/mysql-python/setup.py\", line 17, in &lt;module&gt; metadata, options = get_config() File \"setup_posix.py\", line 43, in get_config libs = mysql_config(\"libs_r\") File \"setup_posix.py\", line 25, in mysql_config raise EnvironmentError(\"%s not found\" % (mysql_config.path,)) EnvironmentError: mysql_config not found ----------------------------------------Command \"python setup.py egg_info\" failed with error code 1 in /private/var/folders/3z/tqw46wwj7xb1d2ftp578x5vm0000gn/T/pip-install-ViJMnc/mysql-python/ 问题原因 mysql_config不存在，原来是系统必须安装mysql客户端 解决方法 brew install mysql 123456⇒ brew install mysqlUpdating Homebrew...==&gt; Downloading https://homebrew.bintray.com/bottles/mysql-5.7.22.high_sierra.bottle.tar.gz######################################################################## 100.0%==&gt; Pouring mysql-5.7.22.high_sierra.bottle.tar.gz... 问题3 连接mysql服务器失败 错误信息 Authentication plugin 'caching_sha2_password' cannot be loaded 1234567891011121314&gt;&gt;&gt; engine = create_engine('mysql+mysqldb://root:123456@127.0.0.1/beta_monitor')&gt;&gt;&gt; connection = engine.connect()Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; File \"/Users/bqi/venv/beta-monitor/lib/python2.7/site-packages/sqlalchemy/engine/base.py\", line 2102, in connect return self._connection_cls(self, **kwargs)... File \"/Users/bqi/venv/beta-monitor/lib/python2.7/site-packages/MySQLdb/__init__.py\", line 81, in Connect return Connection(*args, **kwargs) File \"/Users/bqi/venv/beta-monitor/lib/python2.7/site-packages/MySQLdb/connections.py\", line 193, in __init__ super(Connection, self).__init__(*args, **kwargs2)sqlalchemy.exc.OperationalError: (_mysql_exceptions.OperationalError) (2059, \"Authentication plugin 'caching_sha2_password' cannot be loaded: dlopen(/usr/local/Cellar/mysql/5.7.22/lib/plugin/caching_sha2_password.so, 2): image not found\") (Background on this error at: http://sqlalche.me/e/e3q8) 问题原因 上网查了一通，似乎说从某个版本开始，mysql用了一种认证方式导致问题。根据解决方法看，更换了认证方式就可以了 解决方法 用docker启动mysql时增加参数 --default-authentication-plugin=mysql_native_password 12~|⇒ docker run -p 3306:3306 -d -e MYSQL_ROOT_PASSWORD=123456 -e MYSQL_DATABASE=beta_monitor -e MYSQL_USER=test -e MYSQL_PASSWORD=123456 mysql --default-authentication-plugin=mysql_native_passwordd8f5e623dc595df19b9d6cce52780381b625c1565622f5867f2ad3aeafdca499 如果安装在服务器上，则在my.cnf中修改相关配置 测试 本地连接测试 123456789101112131415161718~|⇒ mysql -utest -p123456 -h 127.0.0.1mysql: [Warning] Using a password on the command line interface can be insecure.ERROR 2013 (HY000): Lost connection to MySQL server at &apos;reading initial communication packet&apos;, system error: 0~|⇒ mysql -utest -p -h 127.0.0.1Enter password:Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 8Server version: 8.0.12 MySQL Community Server - GPLCopyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\\h&apos; for help. Type &apos;\\c&apos; to clear the current input statement.mysql&gt; quit 通过connection = engine.connect()无异常 作为一个暗夜精灵玩家，强烈谴责希尔瓦纳斯烧了我老家的卑劣行径，暴雪怎么洗也没用","categories":[{"name":"python","slug":"python","permalink":"http://www.isimble.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://www.isimble.com/tags/python/"},{"name":"sqlalchemy","slug":"sqlalchemy","permalink":"http://www.isimble.com/tags/sqlalchemy/"},{"name":"flask","slug":"flask","permalink":"http://www.isimble.com/tags/flask/"}]},{"title":"美签记录","slug":"us-visa","date":"2018-07-30T01:17:21.000Z","updated":"2018-08-02T09:07:02.000Z","comments":true,"path":"2018/07/30/us-visa/","link":"","permalink":"http://www.isimble.com/2018/07/30/us-visa/","excerpt":"","text":"记第一次拿到十年美签 时间 提交DS160: 2018-07-08 预约面签时间: 2018-07-27 前面排队人太多 Issued: 2018-07-30 护照已从领事馆那边收回，目前正在安排运送: 2018-07-30 收到护照: 2018-08-02 持续时间 面签持续时间：1分钟不到 AP持续时间：2个工作日 Issued到送到苏州：3个工作日 面签问题 你去美国干什么？ 要去美国多久？ 你是做什么工作的？ 你们公司是做什么的？ 你是一个人去么？ “你通过了！”","categories":[{"name":"旅行","slug":"旅行","permalink":"http://www.isimble.com/categories/%E6%97%85%E8%A1%8C/"}],"tags":[{"name":"visa","slug":"visa","permalink":"http://www.isimble.com/tags/visa/"}]},{"title":"Openstack学习笔记1","slug":"openstack-study1","date":"2018-06-28T01:57:29.000Z","updated":"2018-06-28T10:00:04.000Z","comments":true,"path":"2018/06/28/openstack-study1/","link":"","permalink":"http://www.isimble.com/2018/06/28/openstack-study1/","excerpt":"该系列用来记录本人使用Openstack的一些笔记和心得","text":"该系列用来记录本人使用Openstack的一些笔记和心得 Openstack一些理解 服务说明 keystone - 必选 认证服务，是其他所有服务的基础 glance - 必选 镜像服务，用来存储镜像文件，如iso, vmdk, qcow2等 nova - 必选 计算服务，包括计算，调度，管理，api等，是openstack的核心服务 neutron - 必选 网络服务 horizon - 可选（推荐选择） 管理界面，图形化管理界面，方便使用 cinder - 可选 块存储服务，用来创建虚拟机的磁盘 一些理解 openstack可以理解为插件型的，灵活就体现于此。 计算服务，控制节点可以只做计算的调度，管理，也可以在控制节点上启动计算服务 同样，可以在任意一个节点上起块存储服务 关于网络 刚开始的时候很疑惑，为什么每个计算节点上都要配置一个额外的网卡，并将其连在一起 需要说明的是： 虚拟机的第一个网口是openstack的各个节点用来通信的网卡 第二个网口是用来实现在openstack上启动的实例彼此通信的（即东西向流量） 也可以不用第二个网口，这时，两个实例可以通过基于第一个网口之间建立的隧道进行通信 如果要让实例可以访问外网，则需要为其分配专门的访问外网的网口 openstack并不直接管理网口或网桥，要么通过linux bridge，或者是openvSwitch 在openstack的UI上看到的网口，都是代号 测试环境准备 两台虚拟机（当然也可以all in one） 硬件准备 每台虚拟机应包含 虚机可上网 另外包含一张额外的网卡 内存尽量多（8G） CPU尽量多（8 cpus） controller硬盘稍微大一些 基本准备 Ubuntu16.04 + Q版本（Queens） 配置NTP 安装Openstack基础包 准备仓库 12# apt install software-properties-common# add-apt-repository cloud-archive:queens 安装openstack包 123$ apt update$ apt upgrade #经实际测试，如果用apt dist-upgrade可能会出现问题，安装的不是最新的包$ apt install python-openstackclient controller controller上需要安装的东西最多 依赖 数据库: MariaDB 消息队列: RabbitMQ 服务及身份认证: Memcached etcd openstack服务 认证服务: keystone 镜像服务: glance 计算服务（指计算的调度，管理，api等服务）: nova 网络服务（网络管理，调度等服务）: neutron 管理界面: horizon 块存储服务（可选）: cinder compute 计算节点相对来说简单了很多 openstack服务 计算服务（专指计算服务）: nova-compute 网络服务: neutron 可在controller上同样起计算服务 详细安装过程可参看官方文档","categories":[{"name":"cloud","slug":"cloud","permalink":"http://www.isimble.com/categories/cloud/"}],"tags":[{"name":"openstack","slug":"openstack","permalink":"http://www.isimble.com/tags/openstack/"}]},{"title":"使用markdown写PPT","slug":"marp","date":"2018-06-21T05:42:00.000Z","updated":"2018-11-15T05:46:30.039Z","comments":true,"path":"2018/06/21/marp/","link":"","permalink":"http://www.isimble.com/2018/06/21/marp/","excerpt":"自打接触Markdown以来，深深的爱上了这种格式的书写，先是弄网站，然后又弄电子书。最近要给新员工培训，要准备4个PPT，于是就想有没有直接用md来写PPT的呢？上网一搜，还真有。","text":"自打接触Markdown以来，深深的爱上了这种格式的书写，先是弄网站，然后又弄电子书。最近要给新员工培训，要准备4个PPT，于是就想有没有直接用md来写PPT的呢？上网一搜，还真有。 有Reveal.js, LandSlide, GitPitch等，但也许是我研究的不深吧，gitpitch是在线的，LandSlide要用pip来安装。Reveal.js倒是之前见team里的大神有写过，但现在好像变成了slides了，也是在线的。最终，主角出场，我选择了Marp。 优点 书写简单 提供两套模版 可实时预览 只需要一个md文件即可，不需要其他诸如yaml之类的东西 支持Mac，Win和Linux 缺点 没有动画 只能导出为pdf格式，不能生成PPT格式 仅支持基础的markdown语法 虽然有着上面的缺点，然而，对于写技术类PPT，我觉得够用了 用法 Marp提供两套主题，都说专治选择障碍。特别是默认主题，简直是选择障碍患者的福音。 下载完安装后，两个例子中已经有比较基本的说明了。但有一些东西还是很容易被忽略。 设置页码 页码是可以随时开启，随时关闭的。 1&lt;!-- page_number: true --&gt; 在任意页输入以上代码就可以开启，但如果你要在哪页关闭显示页码，可以用false 背景设置 因为只有两套模版，所以，想要好看点，还是需要一些背景图片的。刚开始我只知道用![bg](aaa.png)的方式引入背景，但后来发现背景图片其实是和背景主题叠加了的。 后来细细看了文档才发现，原来可以用开关开启，还可以设置背景的大小 1![bg original 70%](aaa.png) 使用了original之后，背景就变成了纯图片，而70%则可以指定图片大小。 当然，更有趣的是，可以用多个背景并排的方式来完成部分植入。 比如： 则是用如下代码完成 123456![bg 450% original](robot2.png)![bg]()![bg]()![bg]()![bg]()![bg 450% original](robot2.png) 还有一个有意思的是，如果不设置图片的比例，那么会按照扩充满整个屏幕来设置，但如果设置比例，如上面的代码，则100%是整个页面宽度/6之后的图片大小。 当然，灵活运用这一个特性，也能带来很多意想不到的效果。 emoji 值得一提的是Marp的emoji表情选择的很是我喜欢的那种，而且可以按照段落来调整大小 比如： 1234# :cat:## :fish:### :tiger:#### :bird: 表情的大小会随着段落比较而变化。还是很不错的。 另外，似乎软件的作者正在开发新的软件，很是期待 🐱","categories":[{"name":"工具","slug":"工具","permalink":"http://www.isimble.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"markdown","slug":"markdown","permalink":"http://www.isimble.com/tags/markdown/"}]},{"title":"Ubuntu16.04安装Open vSwitch","slug":"ovs-install","date":"2018-05-29T22:11:23.000Z","updated":"2018-05-30T06:19:38.000Z","comments":true,"path":"2018/05/30/ovs-install/","link":"","permalink":"http://www.isimble.com/2018/05/30/ovs-install/","excerpt":"环境准备 获取安装包 1git clone https://github.com/openvswitch/ovs.git 安装必要的依赖 1apt install autoconf libtool make libssl-dev libcap-ng-dev","text":"环境准备 获取安装包 1git clone https://github.com/openvswitch/ovs.git 安装必要的依赖 1apt install autoconf libtool make libssl-dev libcap-ng-dev 安装 当使用源代码时，需要自己创建configure脚本 1$ ./boot.sh 配置并开启内核模块 1$ ./configure --prefix=/usr --localstatedir=/var --sysconfdir=/etc --with-linux=/lib/modules/$(uname -r)/build 安装 12$ make$ make install 安装内核模块 该步骤可能会报错，请参看后文 1$ make modules_install 官方安装文档中提到，你有可能之前已经安装了ovs的模块，为了确保使用的是你刚才编译的，则需要在/etc/depmod.d/中添加如下内容 1234567$ config_file=&quot;/etc/depmod.d/openvswitch.conf&quot;$ for module in datapath/linux/*.ko; do modname=&quot;$(basename $&#123;module&#125;)&quot; echo &quot;override $&#123;modname%.ko&#125; * extra&quot; &gt;&gt; &quot;$config_file&quot; echo &quot;override $&#123;modname%.ko&#125; * weak-updates&quot; &gt;&gt; &quot;$config_file&quot; done$ depmod -a 加载内核模块 1$ /sbin/modprobe openvswitch 验证 123456789$ /sbin/lsmod | grep openvswitchopenvswitch 303104 0tunnel6 16384 1 openvswitchnf_nat_ipv6 16384 1 openvswitchnf_defrag_ipv6 36864 2 openvswitch,nf_conntrack_ipv6nf_nat_ipv4 16384 2 openvswitch,iptable_natnf_nat 28672 6 nf_nat_redirect,openvswitch,nf_nat_ipv4,nf_nat_ipv6,xt_nat,nf_nat_masquerade_ipv4nf_conntrack 106496 11 xt_CT,openvswitch,nf_nat,nf_nat_ipv4,nf_nat_ipv6,xt_conntrack,nf_nat_masquerade_ipv4,nf_conntrack_netlink,xt_connmark,nf_conntrack_ipv4,nf_conntrack_ipv6libcrc32c 16384 2 raid456,openvswitch 启动服务 官方说明中提到有一个ovs-ctl的命令，然而，我安装完之后并没有这个命令。可能官方文档比较老了吧。 创建必要的目录 123$ mkdir -p /etc/openvswitch$ mkdir -p /var/log/openvswitch$ mkdir -p /var/run/openvswitch 配置ovsdb-server 从源文件目录创建conf.db 12ovs$ ovsdb-tool create /etc/openvswitch/conf.db \\ vswitchd/vswitch.ovsschema 配置 123456ovsdb-server --remote=punix:/var/run/openvswitch/db.sock \\ --remote=db:Open_vSwitch,Open_vSwitch,manager_options \\ --private-key=db:Open_vSwitch,SSL,private_key \\ --certificate=db:Open_vSwitch,SSL,certificate \\ --bootstrap-ca-cert=db:Open_vSwitch,SSL,ca_cert \\ --pidfile --detach --log-file 初始化数据库 1ovs-vsctl --no-wait init 启动ovs进程 1ovs-vswitchd --pidfile --detach --log-file 验证 创建bridge 1$ ovs-vsctl add-br br0 为br0添加接口 1$ ovs-vsctl add-port br0 eth1 查看配置 1$ ovs-vsctl show 问题及解决 安装内核模块时出现如下错误，忽略即可（我还以为很严重，搜了一阵，发现不用管） 123456789101112131415161718192021222324252627282930313233343536373839$ make modules_installcd datapath/linux &amp;&amp; make modules_installmake[1]: Entering directory &apos;/root/ovs/datapath/linux&apos;make -C /lib/modules/4.4.0-127-generic/build M=/root/ovs/datapath/linux modules_installmake[2]: Entering directory &apos;/usr/src/linux-headers-4.4.0-127-generic&apos; INSTALL /root/ovs/datapath/linux/openvswitch.koAt main.c:222:- SSL error:02001002:system library:fopen:No such file or directory: bss_file.c:175- SSL error:2006D080:BIO routines:BIO_new_file:no such file: bss_file.c:178sign-file: certs/signing_key.pem: No such file or directory INSTALL /root/ovs/datapath/linux/vport-geneve.koAt main.c:222:- SSL error:02001002:system library:fopen:No such file or directory: bss_file.c:175- SSL error:2006D080:BIO routines:BIO_new_file:no such file: bss_file.c:178sign-file: certs/signing_key.pem: No such file or directory INSTALL /root/ovs/datapath/linux/vport-gre.koAt main.c:222:- SSL error:02001002:system library:fopen:No such file or directory: bss_file.c:175- SSL error:2006D080:BIO routines:BIO_new_file:no such file: bss_file.c:178sign-file: certs/signing_key.pem: No such file or directory INSTALL /root/ovs/datapath/linux/vport-lisp.koAt main.c:222:- SSL error:02001002:system library:fopen:No such file or directory: bss_file.c:175- SSL error:2006D080:BIO routines:BIO_new_file:no such file: bss_file.c:178sign-file: certs/signing_key.pem: No such file or directory INSTALL /root/ovs/datapath/linux/vport-stt.koAt main.c:222:- SSL error:02001002:system library:fopen:No such file or directory: bss_file.c:175- SSL error:2006D080:BIO routines:BIO_new_file:no such file: bss_file.c:178sign-file: certs/signing_key.pem: No such file or directory INSTALL /root/ovs/datapath/linux/vport-vxlan.koAt main.c:222:- SSL error:02001002:system library:fopen:No such file or directory: bss_file.c:175- SSL error:2006D080:BIO routines:BIO_new_file:no such file: bss_file.c:178sign-file: certs/signing_key.pem: No such file or directory DEPMOD 4.4.0-127-genericmake[2]: Leaving directory &apos;/usr/src/linux-headers-4.4.0-127-generic&apos;depmod `sed -n &apos;s/#define UTS_RELEASE &quot;\\([^&quot;]*\\)&quot;/\\1/p&apos; /lib/modules/4.4.0-127-generic/build/include/generated/utsrelease.h`make[1]: Leaving directory &apos;/root/ovs/datapath/linux&apos;","categories":[{"name":"cloud","slug":"cloud","permalink":"http://www.isimble.com/categories/cloud/"}],"tags":[{"name":"ovs","slug":"ovs","permalink":"http://www.isimble.com/tags/ovs/"}]},{"title":"gitbook+gitlab发布私有的图书仓库","slug":"gitbook-gitlab","date":"2018-05-10T22:28:34.000Z","updated":"2018-11-15T05:46:30.039Z","comments":true,"path":"2018/05/11/gitbook-gitlab/","link":"","permalink":"http://www.isimble.com/2018/05/11/gitbook-gitlab/","excerpt":"近期带着一些同事学习python，开始用markdown的格式写了很多练习题，一直有想法将其发布成一本电子书在公司内部分享。但由于有些内容可能涉及公司相关，无法直接对外发布。而公司内部自建了一个gitlab服务器，一直用于托管一些不是很重要的代码。所以，便有了将markdown托管到gitlab上，然后使用gitbook在内部进行发布。 另一方面，由于最近痴迷于Docker，所以，也同样将gitbook打包成docker image，用于快速发布。这篇文章主要用于记录整个操作过程。","text":"近期带着一些同事学习python，开始用markdown的格式写了很多练习题，一直有想法将其发布成一本电子书在公司内部分享。但由于有些内容可能涉及公司相关，无法直接对外发布。而公司内部自建了一个gitlab服务器，一直用于托管一些不是很重要的代码。所以，便有了将markdown托管到gitlab上，然后使用gitbook在内部进行发布。 另一方面，由于最近痴迷于Docker，所以，也同样将gitbook打包成docker image，用于快速发布。这篇文章主要用于记录整个操作过程。 gitbook+docker 本地安装 可参看gitbook的官方文档 环境需求 gitbook需要nodejs环境 NodeJS (v4.0.0 and above is recommended) 安装gitbook-cli 1npm install gitbook-cli -g 创建一本书 当使用gitlab时，可跳过此步骤 创建存放书籍的目录 1$ mkdir mybook 初始化 1$ gitbook init 初始化完成后，将生成SUMMARY.md和README.md 插件安装 如果没有生成book.json，可自行创建 12345678910111213&#123;&quot;root&quot;: &quot;./&quot;,&quot;title&quot;: &quot;mybook&quot;,&quot;head_title&quot;: &quot;My first book&quot;,&quot;description&quot;: &quot;test with gitbook&quot;,&quot;author&quot;: &quot;myname&quot;,&quot;output.name&quot;: &quot;practice&quot;,&quot;gitbook&quot;: &quot;&gt;= 3.0.0&quot;,&quot;language&quot;: &quot;zh-hans&quot;,&quot;plugins&quot;: [ ] 将插件在plugins字段中进行声明后，执行以下命令进行安装 1$ gitbook install 预览和发布自己的书 1$ gitbook serve 随后可登陆http://localhost:4000来预览自己的书 docker镜像 其实在docker hub上搜索便可以得到gitbook的镜像，但本着学习的态度，还是自己动手练习制作自己的docker镜像。 由于nodejs有官方提供的docker镜像，所以，一切变得很简单 Dockerfile 12345678910FROM node:8-alpineMAINTAINER Bo Qi &lt;simble1986@gmail.com&gt;RUN npm install gitbook-cli -g &amp;&amp; npm install &amp;&amp; gitbook installWORKDIR /bookEXPOSE 4000 35729CMD gitbook install &amp;&amp; gitbook serve 编译docker镜像 1$ docker build --tag mygitbook . 书的结构 当使用gitbook init后会在当前目录生成两个文件：README.md和SUMMARY.md. 其中，README.md用来对本书进行一些说明 而SUMMARY.md则用来创建目录结构。 SUMMARY.md 1234567891011121314151617# Summary* [介绍](README.md)* [Git使用](gitSetup.md)* [Python基础](part1/README.md) * [练习1-列表](part1/1.md) * [练习2-字典](part1/2.md) * [练习3-数据结构嵌套](part1/3.md) * [练习4-运算符](part1/4.md) * [练习5-逻辑控制](part1/5.md) * [练习6-异常处理](part1/6.md) * [练习7-函数](part1/7.md) * [练习8-文件操作](part1/8.md) * [轻松一刻-猜数字游戏](part1/happy1.md) * [练习9-类.1](part1/9.md) * [练习10-类.2](part1/10.md) * [练习11-类的继承](part1/11.md) book.json 需要自己创建book.json 12345678910111213141516171819202122232425262728293031323334353637&#123; \"root\": \"./\", \"title\": \"练习python\", \"head_title\": \"通过练习的方式来学习python\", \"description\": \"通过小练习一点一点熟悉python\", \"author\": \"myname(simble1986@gmail.com)\", \"output.name\": \"通过练习学脚本\", \"gitbook\": \"3.2.3\", \"language\": \"zh-hans\", \"links\" : &#123; \"sidebar\" : &#123; \"Home\" : \"http://www.simble.site\" &#125; &#125;, \"plugins\": [ \"autotheme\", \"prism\", \"prism-themes\", \"-highlight\", \"-search\", \"search-pro\", \"emphasize\", \"splitter\", \"tbfed-pagefooter\", \"toggle-chapters\", \"codeblock-filename\", \"ace\", \"simple-page-toc\", \"edit-link\", \"copy-code-button\", \"alerts\", \"anchor-navigation-ex\", \"theme-comscore\" ]&#125; git-lab归档 在gitlab上创建自己的project并归档 在服务器上用docker启动预览 从gitlab上clone书的结构 启动docker并挂载gitbook的目录到docker中 1$ docker run -d -p 80:4000 -v /mybook:/book mygitbook 插件及说明 非常感谢Zhangjikai的插件说明文档，然后我发现Zhangjikai和我一样使用了Hexo搭建了自己的blog，并且同样适用了Next的主题","categories":[{"name":"docker","slug":"docker","permalink":"http://www.isimble.com/categories/docker/"}],"tags":[]},{"title":"python virtenv环境搭建","slug":"python-virtenv","date":"2018-05-06T18:32:01.000Z","updated":"2018-11-15T05:46:30.039Z","comments":true,"path":"2018/05/07/python-virtenv/","link":"","permalink":"http://www.isimble.com/2018/05/07/python-virtenv/","excerpt":"近期由于组内测试框架更新频繁，且由于框架采用了插件形式，安装包很多。当使用同一台服务器来安装时，可能会导致生产环境破坏。为此，必须采用virtenv方式。 virtualenv的环境建立并不复杂，但每次都需要去查一番。特此记录","text":"近期由于组内测试框架更新频繁，且由于框架采用了插件形式，安装包很多。当使用同一台服务器来安装时，可能会导致生产环境破坏。为此，必须采用virtenv方式。 virtualenv的环境建立并不复杂，但每次都需要去查一番。特此记录 安装 安装virtualenv python2.7 1pip install virtualenv python3 1pip3 install virtualenv 使用 创建工作目录 12root@vm1:/home/test# mkdir myprojectroot@vm1:/home/test# cd myproject 创建独立的python运行环境 1234root@vm1:/home/test# virtualenv venvNew python executable in /home/test/myproject/venv/bin/pythonInstalling setuptools, pip, wheel...done.root@vm1:/home/test# 引用新的环境变量 12root@vm1:/home/test# source venv/bin/activate(venv)root@vm1:/home/test# 开始使用 1(venv)root@vm1:/home/test# pip install docker 退出当前venv环境 12(venv)root@vm1:/home/test# deactivateroot@vm1:/home/test# 一个小问题 git clone时遇到server certificate verification failed. CAfile: /etc/ssl/certs/ca-certificates.crt CRLfile: none的解决方法 1export GIT_SSL_NO_VERIFY=1","categories":[{"name":"python","slug":"python","permalink":"http://www.isimble.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://www.isimble.com/tags/python/"}]},{"title":"Docker API一种连接到PTY的交互方法","slug":"docker-pexpect","date":"2018-05-04T01:35:45.000Z","updated":"2018-05-04T09:51:16.000Z","comments":true,"path":"2018/05/04/docker-pexpect/","link":"","permalink":"http://www.isimble.com/2018/05/04/docker-pexpect/","excerpt":"折腾了一周多，终于搞定了在docker的python API下，当执行exec_run时，如何连接container的PTY。 当弄明白之后，才发现原来是那么简单。之前几乎搜遍了google和百度，都没有找到相关的文章","text":"折腾了一周多，终于搞定了在docker的python API下，当执行exec_run时，如何连接container的PTY。 当弄明白之后，才发现原来是那么简单。之前几乎搜遍了google和百度，都没有找到相关的文章 前言 这两年，docker的发展如火如荼，作为网络测试，我们也在尝试着将docker引入测试中，来更多的模拟真实用户，并实现自动化。 Pexpect是一个非常强大且好用的工具，当需要与设备和PC连接时，基本上都会用到。而之前都直接使用spawn一个命令行来进行连接 本篇博文将介绍一种使用pexpect的fdspawn，通过socket方式连接到container的方法，以便与远程的container进行交互 自动化思路 client通过python API连接到docker 创建一个container并保持运行 使用exec_run()新建一个连接，运行/bin/bash，并开启socket方式 使用pexpect的fdspawn连接exec_run()返回的socket 环境准备 Docker开启remote API 参见之前博文 Docker Client 安装必要的包 12$ pip install docker$ pip install pexpect 开始使用 创建container 123&gt;&gt;&gt; import docker&gt;&gt;&gt; client=docker.DockerClient(base_url='tcp://10.0.0.10:1234')&gt;&gt;&gt; c1 = client.containers.run(\"ubuntu\", detach=True, tty=True) 连接container 123456&gt;&gt;&gt; res = c1.exec_run(\"/bin/bash\", socket=True, stdin=True, tty=True)&gt;&gt;&gt; resExecResult(exit_code=None, output=&lt;socket object, fd=15, family=1, type=1, protocol=0&gt;)&gt;&gt;&gt; sock = res.output&gt;&gt;&gt; sock&lt;socket object, fd=15, family=1, type=1, protocol=0&gt; 使用pexpect连接 1234567891011&gt;&gt;&gt; import pexpect.fdpexpect&gt;&gt;&gt; session=pexpect.fdpexpect.fdspawn(sock.fileno(),timeout=10)&gt;&gt;&gt; &gt;&gt;&gt; session.send(\"ls\\n\")3&gt;&gt;&gt; session.expect(\"#\")0&gt;&gt;&gt; session.before' ls\\r\\n\\x1b[0m\\x1b[01;34mbin\\x1b[0m \\x1b[01;34mdev\\x1b[0m \\x1b[01;34mhome\\x1b[0m \\x1b[01;34mlib64\\x1b[0m \\x1b[01;34mmnt\\x1b[0m \\x1b[01;34mproc\\x1b[0m \\x1b[01;34mrun\\x1b[0m \\x1b[01;34msrv\\x1b[0m \\x1b[30;42mtmp\\x1b[0m \\x1b[01;34mvar\\x1b[0m\\r\\n\\x1b[01;34mboot\\x1b[0m \\x1b[01;34metc\\x1b[0m \\x1b[01;34mlib\\x1b[0m \\x1b[01;34mmedia\\x1b[0m \\x1b[01;34mopt\\x1b[0m \\x1b[01;34mroot\\x1b[0m \\x1b[01;34msbin\\x1b[0m \\x1b[01;34msys\\x1b[0m \\x1b[01;34musr\\x1b[0m\\r\\n\\x1b]0;root@6a097ddbe55d: /\\x07root@6a097ddbe55d:/'&gt;&gt;&gt; session.after'#'&gt;&gt;&gt; 注意事项 在使用exec_run()执行开启命令时，需要指定stdin=True，否则，pexpect的send()将无法将命令发送至container 同样，在使用exec_run()时，需要指定tty=True，否则，将没有命令行提示符，无法进行匹配","categories":[{"name":"docker","slug":"docker","permalink":"http://www.isimble.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://www.isimble.com/tags/docker/"},{"name":"pexpect","slug":"pexpect","permalink":"http://www.isimble.com/tags/pexpect/"},{"name":"自动化","slug":"自动化","permalink":"http://www.isimble.com/tags/%E8%87%AA%E5%8A%A8%E5%8C%96/"}]},{"title":"dockerpty使用","slug":"dockerpty-usage","date":"2018-05-03T02:34:40.000Z","updated":"2018-05-04T09:58:47.000Z","comments":true,"path":"2018/05/03/dockerpty-usage/","link":"","permalink":"http://www.isimble.com/2018/05/03/dockerpty-usage/","excerpt":"本篇博客将介绍在使用docker API时，如何监管container的PTY实现交互 问题引入 docker官方已经提供了API用来管理client，container，image，network等，基本的操作覆盖了docker CLI相关功能，但docker的API现在只能使用exec_run来执行一条命令，中间无法进行交互，希望能有一个类似于-it的方式来完成交互操作。 经过几天的学习和测试，发现其实docker的containers.run()和containers.exec_run()都是可以设置stdin=True, tty=True。但开启这些之后，将返回一个socket，需要自己来进行处理。 google大法后，找到了一个dockerpty的python lib，可以完成这件事情","text":"本篇博客将介绍在使用docker API时，如何监管container的PTY实现交互 问题引入 docker官方已经提供了API用来管理client，container，image，network等，基本的操作覆盖了docker CLI相关功能，但docker的API现在只能使用exec_run来执行一条命令，中间无法进行交互，希望能有一个类似于-it的方式来完成交互操作。 经过几天的学习和测试，发现其实docker的containers.run()和containers.exec_run()都是可以设置stdin=True, tty=True。但开启这些之后，将返回一个socket，需要自己来进行处理。 google大法后，找到了一个dockerpty的python lib，可以完成这件事情 环境搭建 根据dockerpty的github上提到的安装过程，只需要pip install dockerpty即可完成安装。 但源码已经有两年没有更新了，该版本无法在新的docker API上正常工作 fork了工程后，对其中的代码涉及到的docker API进行更新后，测试可以正常工作，最新的代码已经上传到git上simble1986/dockerpty 依赖 原有的project上提到依赖的docker api为docker-py&gt;=0.3.2，但docker的python API已经更新 安装docker API 1pip install docker 安装步骤 获取源码 1git clone https://github.com/simble1986/dockerpty.git 安装 123$ pip uninstall dockerpty$ cd dockerpty$ python setup.py install 相关API 参看docker官方API文档，以下主要对container相关参数加以说明 tty (bool) – Allocate a pseudo-TTY. stdin_open (bool) – Keep STDIN open even if not attached. 基本使用 连接client 1234567root@slt-docker:/home/bqi# pythonPython 2.7.12 (default, Dec 4 2017, 14:50:18)[GCC 5.4.0 20160609] on linux2Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.&gt;&gt;&gt; import docker&gt;&gt;&gt; import dockerpty&gt;&gt;&gt; client=docker.from_env() 注： 支持远程API 创建container 1&gt;&gt;&gt; test1 = client.containers.create(\"ubuntu\",\"/bin/bash\",tty=True,stdin_open=True) 使用dockerpty 123456&gt;&gt;&gt; dockerpty.start(client,test1)root@d6ddcf619602:/# lsbin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr varroot@d6ddcf619602:/# exitexit&gt;&gt;&gt;","categories":[{"name":"docker","slug":"docker","permalink":"http://www.isimble.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://www.isimble.com/tags/docker/"},{"name":"dockerpty","slug":"dockerpty","permalink":"http://www.isimble.com/tags/dockerpty/"}]},{"title":"Docker中无密码apt安装mysql","slug":"docker-install-mysql-server-no-passwd","date":"2018-03-29T18:39:33.000Z","updated":"2018-11-15T05:46:30.039Z","comments":true,"path":"2018/03/30/docker-install-mysql-server-no-passwd/","link":"","permalink":"http://www.isimble.com/2018/03/30/docker-install-mysql-server-no-passwd/","excerpt":"问题出现 Linux在第一次安装有些软件时会有交互的输入的需求，比如mysql在首次安装时需要设置root的密码。这在正常配置过程中没什么问题，但在使用DockerFile创建docker镜像时，则遇到了麻烦。","text":"问题出现 Linux在第一次安装有些软件时会有交互的输入的需求，比如mysql在首次安装时需要设置root的密码。这在正常配置过程中没什么问题，但在使用DockerFile创建docker镜像时，则遇到了麻烦。 解决思路 如果将安装好的mysql-server使用apt remove从系统中卸载后，再次重新安装，则不再需要输入密码。另外当安装完一些软件后，可以使用dpkg-config来重新配置。 这样，就可以在安装软件前先对系统做好相关配置。接下来，就是需要获取软件的必要配置项 获取软件必要配置项 下载软件包 可以通过网上搜索方式下载相关的deb包，但众所周知，Linux的软件包版本多，很多情况下并不知道需要安装哪个版本。但可以使用apt来下载相应的软件包 1$ apt-get -d install -y mysql-server 使用apt-get的-d参数，将只会下载，不进行安装。下载完毕后，软件包位于/var/cache/apt/archives目录下。 获取配置项 进入软件包的存放目录，然后执行 1dpkg-preconfigure mysql-server-5.1_5.1.49-3_amd64.deb 使用debconfig-show来查看相应的配置项 12345$ debconf-show mysql-server mysql-server/root_password: (password omitted) mysql-server/root_password_again: (password omitted) mysql-server/error_setting_password: ... 可以看到，有两项是必须配置的：mysql-server/root_password和mysql-server/root_password_again 预配置 创建配置文件 在合适的目录下创建一个文件，例如mysql-passwd，输入以下内容 12debconf mysql-server/root_password password 123456debconf mysql-server/root_password_again password 123456 加载配置 1$ debconf-set-selections mysql-passwd 测试结果 再次安装mysql-server，将不再需要输入密码 1$ apt-get install -y mysql-server DockerFile处理 DockerFile应当尽量避免不需要的操作，所以，获取配置项的操作可以提前在实验环境中完成。 可将获取的命令行保存为本地文件，使用时copy过去。 12COPY ./mysql-passwd /tmp/mysql-passwdRUN apt-get update &amp;&amp; debconf-set-selections /tmp/mysql-passwd &amp;&amp; apt-get install -yqq mysql-server &amp;&amp; rm -rf /var/lib/apt/lists/* 或者是在DockerFile中直接生成，这时则可以将mysql passwd设置为参数 12RUN echo debconf mysql-server/root_password password 123456 &gt; /tmp/mysql-passwd &amp;&amp; echo debconf mysql-server/root_password_again password 123456 &gt;&gt; /tmp/mysql-passwdRUN apt-get update &amp;&amp; debconf-set-selections /tmp/mysql-passwd &amp;&amp; apt-get install -yqq mysql-server &amp;&amp; rm -rf /var/lib/apt/lists/*","categories":[{"name":"docker","slug":"docker","permalink":"http://www.isimble.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://www.isimble.com/tags/docker/"},{"name":"DockerFile","slug":"DockerFile","permalink":"http://www.isimble.com/tags/DockerFile/"},{"name":"mysql-server","slug":"mysql-server","permalink":"http://www.isimble.com/tags/mysql-server/"}]},{"title":"Hexo+Next上开启Valine评论系统","slug":"hexo-next-valine","date":"2018-03-27T13:11:51.000Z","updated":"2018-03-30T02:37:58.000Z","comments":true,"path":"2018/03/27/hexo-next-valine/","link":"","permalink":"http://www.isimble.com/2018/03/27/hexo-next-valine/","excerpt":"朋友之前问怎么没有开评论系统，倒不是不想开，而是刚开始建站的时候浏览了好多博文，似乎很多原来的接口都在hexo不太好用了。特别是很多博文都是两年前写的，当然，大多数其他功能都没问题","text":"朋友之前问怎么没有开评论系统，倒不是不想开，而是刚开始建站的时候浏览了好多博文，似乎很多原来的接口都在hexo不太好用了。特别是很多博文都是两年前写的，当然，大多数其他功能都没问题 简介 Valine Valine是一款基于Leancloud的快速、简洁且高效的无后端评论系统。 Leancloud 我的理解，Leancloud相当于是一个数据托管平台，可以帮助应用存储相关数据。Valine主要用到的是其中的数据存储——comments 环境说明 使用了最新版的Hexo以及最新版的Next主题 获取AppID 注册Leancloud 访问Leancloud，点击免费试用就会跳转到注册/登陆页面。当前支持通过Github，Weibo以及QQ进行注册 注册完后需要验证邮箱 创建应用 访问控制台，在控制台中创建新应用 获取应用Key 点击新创建的应用——设置——应用Key，保存页面上的App ID以及App Key以备后续使用 配置Valine 在最新版的Next主题中，已经合入了Valine的配置代码，使得配置起来非常快捷。访问Hexo中使用Valine，点击merged，会跳转到Next主题的merge历史 检查相关文件 可以再次检查并确认主题配置文件_config.xml，layout/_macro/post.swig和layout/_third-party/comments/valine.swig是否都已经合入了相关代码 配置AppKey 编辑主题配置文件_config.xml中的valine部分内容 12345678910valine: enable: true appid: $Your APP ID # your leancloud application appid appkey: $Your APP Key # your leancloud application appkey notify: false # mail notifier , https://github.com/xCss/Valine/wiki verify: false # Verification code placeholder: 随便说些什么吧 # comment box placeholder avatar: mm # gravatar style guest_info: nick,mail,link # custom comment header pageSize: 10 # pagination size 重新生成页面 执行命令，重新生成并部署 1$ hexo clean &amp;&amp; hexo g &amp;&amp; hexo d 到这里，查看页面已经可以看到评论系统 创建留言页面 可以为站点创建一个单独的留言板页面 创建页面 1$ hexo new page guestbook 配置主题 修改主题配置文件 在主题配置文件_config.xml的menu字段新增guestbook字段 12345678910menu: home: / || home tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive #schedule: /schedule/ || calendar #sitemap: /sitemap.xml || sitemap #commonweal: /404/ || heartbeat about: /about/ || user guestbook: /guestbook/ || comments 可以访问fontawesome选择自己喜欢的图标来作为留言板的图标 本地化处理 编辑对应语言的配置文件themes/next/languages/zh-CN.yml，在menu中增加guestbook的中文 1234567891011menu: home: 首页 archives: 归档 categories: 分类 tags: 标签 about: 关于 search: 搜索 schedule: 日程表 sitemap: 站点地图 commonweal: 公益 404 guestbook: 留言 编辑页面 编辑之前生成的guestbook页面 123456---title: 留言板date: 2018-03-26 23:36:19comments: true---&lt;center&gt;既然来了，就是一种缘分，留下点什么吧:cat:&lt;/center&gt; 重新部署之后就可以看到留言板了 😃 清除测试留言 为了确保留言功能已经正常工作，都会测试一下。测试完毕后，可以通过Leancloud的控制台清除测试数据 点击myblog——存储——Comments，即可查看当前留言，选中测试时的留言，删除即可","categories":[{"name":"hexo","slug":"hexo","permalink":"http://www.isimble.com/categories/hexo/"}],"tags":[{"name":"blog","slug":"blog","permalink":"http://www.isimble.com/tags/blog/"},{"name":"Next","slug":"Next","permalink":"http://www.isimble.com/tags/Next/"},{"name":"Valine","slug":"Valine","permalink":"http://www.isimble.com/tags/Valine/"}]},{"title":"在Ubuntu 16.04上开启Docker的Remote API","slug":"enable-docker-remote-api","date":"2018-03-27T06:42:58.000Z","updated":"2018-05-04T09:59:45.000Z","comments":true,"path":"2018/03/27/enable-docker-remote-api/","link":"","permalink":"http://www.isimble.com/2018/03/27/enable-docker-remote-api/","excerpt":"由于自动化的考虑，需要用docker的remote API，尝试了多种方法，最终才找到了可行的方法 可行的方法 编辑/lib/systemd/system/docker.service","text":"由于自动化的考虑，需要用docker的remote API，尝试了多种方法，最终才找到了可行的方法 可行的方法 编辑/lib/systemd/system/docker.service 1$ vim /lib/systemd/system/docker.service 修改ExecStart的参数 1ExecStart=/usr/bin/dockerd -H fd:// -H tcp://0.0.0.0:2375 随后执行service docker restart时会提示Warning: docker.service changed on disk. Run 'systemctl daemon-reload' to reload units.则表示配置成功 1systemctl daemon-reload 重启docker服务 1service docker restart 测试是否成功 12$ curl http://localhost:2375/containers/json[&#123;\"Id\":\"30c8e35f1292421d11f6b09385a4fc980d6abaca591d0f52b18dbad8e4f5be04\",\"Names\":[\"/clever_murdock\"],\"Image\":\"portainer/portainer\",\"ImageID\":\"sha256:a8f2aeb34cf69178be1d152759fb17ccff7915faf750c82cd7d1851b12ec7b37\",\"Command\":\"/portainer\",\"Created\":1520845664,\"Ports\":[&#123;\"IP\":\"0.0.0.0\",\"PrivatePort\":9000,\"PublicPort\":9000,\"Type\":\"tcp\"&#125;],\"Labels\":&#123;&#125;,\"State\":\"running\",\"Status\":\"Up 13 minutes\",\"HostConfig\":&#123;\"NetworkMode\":\"default\"&#125;,\"NetworkSettings\":&#123;\"Networks\":&#123;\"bridge\":&#123;\"IPAMConfig\":null,\"Links\":null,\"Aliases\":null,\"NetworkID\":\"78fa057306e70838bab1e18359c86bd8eff7de2285c351784ad951cd7a73f8d1\",\"EndpointID\":\"e99ca98169320155c8833a8746be7d0e1c8d98186c75fba9d9bf2486367a4e00\",\"Gateway\":\"172.17.0.1\",\"IPAddress\":\"172.17.0.2\",\"IPPrefixLen\":16,\"IPv6Gateway\":\"\",\"GlobalIPv6Address\":\"\",\"GlobalIPv6PrefixLen\":0,\"MacAddress\":\"02:42:ac:11:00:02\",\"DriverOpts\":null&#125;&#125;&#125;,\"Mounts\":[&#123;\"Type\":\"bind\",\"Source\":\"/opt/portainer\",\"Destination\":\"/data\",\"Mode\":\"\",\"RW\":true,\"Propagation\":\"rprivate\"&#125;,&#123;\"Type\":\"bind\",\"Source\":\"/var/run/docker.sock\",\"Destination\":\"/var/run/docker.sock\",\"Mode\":\"\",\"RW\":true,\"Propagation\":\"rprivate\"&#125;]&#125;] 不可行的方法 同时列出在Ubuntu上不可行的方法 修改/etc/default/docker中的DOCKER_OPTS 1DOCKER_OPTS='-H fd:// -H tcp://0.0.0.0:2375' 修改/etc/init/docker.conf中的DOCKER_OPTS 网上有人说在Ubuntu14.04上可以生效 123# modify these in /etc/default/$UPSTART_JOB (/etc/default/docker) DOCKERD=/usr/bin/dockerd DOCKER_OPTS='-H unix:///var/run/docker.sock -H tcp://0.0.0.0:2375'","categories":[{"name":"docker","slug":"docker","permalink":"http://www.isimble.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://www.isimble.com/tags/docker/"},{"name":"自动化","slug":"自动化","permalink":"http://www.isimble.com/tags/%E8%87%AA%E5%8A%A8%E5%8C%96/"}]},{"title":"使用hexo搭建自己的博客——创建站点","slug":"setup-hexo-create-site","date":"2018-03-25T15:29:30.000Z","updated":"2018-03-30T02:37:58.000Z","comments":true,"path":"2018/03/25/setup-hexo-create-site/","link":"","permalink":"http://www.isimble.com/2018/03/25/setup-hexo-create-site/","excerpt":"安装及配置 简介 Hexo是一个快速、简介且高效的博客框架，可以使用Markdown解析文章并生成网站","text":"安装及配置 简介 Hexo是一个快速、简介且高效的博客框架，可以使用Markdown解析文章并生成网站 说明 站点配置文件 $site_dir/_config.xml 主题配置文件$site_dir/theme/$theme_dir/_config 安装hexo 当环境准备好git以及nodejs后便可以安装hexo 1npm install hexo-cli -g 建站 安装Hexo后，创建站点文件存放的文件夹，如blog，然后执行 123$ hexo init blog$ cd blog$ npm install 完成后，blog目录结构 12345678.├── _config.yml├── package.json├── scaffolds├── source| ├── _drafts| └── _posts└── themes 配置 修改_config.yml 123456title: your site tilesubtitle: description: descriptionauthor: your namelanguage: zh-Hanstimezone: Asia/Shanghai 常用命令 清除缓存文件及已生成的静态文件 1$ hexo clean 启动服务器 1$ hexo server 默认状况下，服务器将使用4000端口。可以通过hexo server -p 80来将server绑定至80端口 部署网站 可将网站按照_config.yml中的配置直接部署于github等（后续看心情写步骤） 1$ hexo deploy 主题修改 可访问hexo官方主题库选择自己喜欢的主题，并下载至theme目录下 修改_config.xml中的theme字段 1theme: landscape 写作 有两种方法创建文章 创建文章 hexo命令行方式 1$ hexo new &lt;layout&gt; title layout为模版 命令执行后，默认文章将位于source/_post/目录下，文件内容为： 12345---title: testdate: 2018-03-25 23:10:04tags:--- 直接创建文件 12$ cd source/_post/$ touch myfirstblog.md 此时文件内容为空，需要自己添加相关内容 创建标签页 配置确认 确认站点配置文件中有以下内容 1tag_dir: tags 确认主题配置文件中tags打开 123menu: 主页: / || home 标签: /tags/ || tags 创建标签页 1$ hexo new page tags 修改tags/index.md中的type为&quot;tags&quot; 12345---title: Tagclouddate: 2018-03-23 01:18:00type: \"tags\"--- 创建分类页面 与创建标签页相似 确认配置 站点配置文件中有category_dir: categories 主题配置文件中的分类: /categories/ || th开启 创建分类也 1$ hexo new page categories","categories":[{"name":"hexo","slug":"hexo","permalink":"http://www.isimble.com/categories/hexo/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://www.isimble.com/tags/hexo/"},{"name":"blog","slug":"blog","permalink":"http://www.isimble.com/tags/blog/"},{"name":"markdown","slug":"markdown","permalink":"http://www.isimble.com/tags/markdown/"}]},{"title":"docker基本操作（一）","slug":"docker_study_cmd1","date":"2018-03-23T01:14:57.000Z","updated":"2018-05-04T09:58:13.000Z","comments":true,"path":"2018/03/23/docker_study_cmd1/","link":"","permalink":"http://www.isimble.com/2018/03/23/docker_study_cmd1/","excerpt":"本文作为docker使用笔记供小伙伴们参考 准备工作 安装最新版的docker-ce，会将自动命令行补齐安装在/usr/share/bash-completion/completions/docker目录","text":"本文作为docker使用笔记供小伙伴们参考 准备工作 安装最新版的docker-ce，会将自动命令行补齐安装在/usr/share/bash-completion/completions/docker目录 为了方便操作，在ubuntu上打开docker命令行自动补齐功能 编辑/etc/bash.bashrc文件，查找completion段，将该段内容前的#删除即可 12345678# enable bash completion in interactive shellsif ! shopt -oq posix; then if [ -f /usr/share/bash-completion/bash_completion ]; then . /usr/share/bash-completion/bash_completion elif [ -f /etc/bash_completion ]; then . /etc/bash_completion fifi image操作 build image 1$ docker build -t mytest:latest . 查看image 12345$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEnginx &lt;none&gt; 73acd1f0cfad 8 days ago 109MBmongo 3 5b1317f8158f 8 days ago 366MB$ dockeer image ls 删除image 12$ docker image rm &lt;id/name&gt;$ docker rmi &lt;id/name&gt; 从docker hub上获取image 国内访问docker hub较慢，可使用阿里云的docker镜像服务 1$ docker pull nginx continer相关 创建docker 1$ docker run -d --name mytest -p 80:80 mynginx 常用参数说明 –rm 当运行结束（当CMD或entrypoint或docker run命令行指定的命令运行结束时，容器停止）时自动删除docker -it 重定向docker终端 -d 在后台执行 -e 添加运行时的参数，常被用于docker CMD执行时增加参数 docker支持长id和短id方式索引，亦可通过名称进行索引 stop/start/restart容器 通过docker stop可以停止运行的容器，也可以使用docker kill来快速停止一个容器 docker start会保留容器的第一次启动时的所有参数 docker restart可以重启容器 可以在启动容器时设置–restart来自动重启容器 删除容器 可以使用docker ps列出当前正在运行的容器 容器停止运行不代表容器已经被删除，可以使用docker ps -a 使用docker rm来删除一个容器 使用docker rmi则会删除docker的image 查看及操作 查看容器状态 123$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES30c8e35f1292 portainer/portainer \"/portainer\" 10 days ago Up 2 days 0.0.0.0:9000-&gt;9000/tcp clever_murdock 使用docker ps -a查看所有容器（包含休眠状态） 1234$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES30c8e35f1292 portainer/portainer \"/portainer\" 10 days ago Up 2 days 0.0.0.0:9000-&gt;9000/tcp clever_murdockef58155f9957 registry:2 \"/entrypoint.sh /etc…\" 2 days ago Exited (137) 41 hours ago sltregistry 连接到容器的终端 尽量使用exec方法，attach连入后可查看当前容器命令运行的日志，但不当的操作容易使运行中的容器退出 使用attach 1$ docker attach &lt;id/name&gt; 使用exec 1$ docker exec -it &lt;id/name&gt; /bin/sh 查看容器运行的日志 容器以-d参数运行时，可以使用docker logs查看运行过程中的日志 123$ docker logs mytest_haproxy[WARNING] 079/100555 (1) : [haproxy.main()] Cannot raise FD limit to 200000011, limit is 1048576.[WARNING] 079/100555 (1) : [haproxy.main()] FD limit (1048576) too low for maxconn=100000000/maxsock=200000011. Please raise 'ulimit-n' to 200000011 or more to avoid any trouble.","categories":[{"name":"docker","slug":"docker","permalink":"http://www.isimble.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://www.isimble.com/tags/docker/"}]},{"title":"在树莓派上部署nodejs","slug":"nodejs-setup-pi","date":"2018-03-22T17:32:39.000Z","updated":"2018-03-30T02:37:58.000Z","comments":true,"path":"2018/03/23/nodejs-setup-pi/","link":"","permalink":"http://www.isimble.com/2018/03/23/nodejs-setup-pi/","excerpt":"终于有时间在树莓派上部署一个自己的小博客了，平时用惯了ubuntu，到了树莓派上，发现并没有那么简单。首先就是apt install nodejs，安装完没有npm。不过，经过一番折腾，便有了现在的小站点","text":"终于有时间在树莓派上部署一个自己的小博客了，平时用惯了ubuntu，到了树莓派上，发现并没有那么简单。首先就是apt install nodejs，安装完没有npm。不过，经过一番折腾，便有了现在的小站点 安装nodejs nodejs可以使用源码编译和二进制包来安装。考虑到树莓派的处理能力，要编译一个nodejs，太过耗时。直接选用官网提供的二进制包完成安装 安装包获取 官网上ARM的bin包有3个，分别是v6，v7和v8。而树莓派的版本是v7 12$ uname -aLinux raspberrypi 4.9.80-v7+ #1098 SMP Fri Mar 9 19:11:42 GMT 2018 armv7l GNU/Linux 获取安装包 1$ wget https://nodejs.org/dist/v8.10.0/node-v8.10.0-linux-armv7l.tar.xz 安装nodejs 解压缩 1234567$ tar -Jxv -f node-v8.10.0-linux-armv7l.tar.xznode-v8.10.0-linux-armv7l/node-v8.10.0-linux-armv7l/README.mdnode-v8.10.0-linux-armv7l/bin/node-v8.10.0-linux-armv7l/bin/nodenode-v8.10.0-linux-armv7l/bin/npm... 可根据个人喜好重命名文件夹，此处重命名为node，分别验证版本信息 1234pi@raspberrypi:~/node $ ./bin/node -vv8.10.0pi@raspberrypi:~/node $ ./bin/npm -v5.6.0 配置node和npm为全局命令 12pi@raspberrypi:~/node $ sudo ln /home/pi/node/bin/node /usr/local/bin/nodepi@raspberrypi:~/node $ sudo ln -s /home/pi/node/lib/node_modules/npm/bin/npm /usr/local/bin/npm 此时执行npm会报错 123456pi@raspberrypi:~/node $ npm -vmodule.js:471 throw err; ^Error: Cannot find module '/usr/local/bin/node_modules/npm/bin/npm-cli.js' 需要修改/usr/local/bin/目录下的npm文件，将$basedir替换为绝对路径，此处为/home/pi/node/ 12345678910111213141516171819202122232425262728293031323334#!/bin/sh(set -o igncr) 2&gt;/dev/null &amp;&amp; set -o igncr; # cygwin encoding fixbasedir=`dirname \"$0\"`case `uname` in *CYGWIN*) basedir=`cygpath -w \"$basedir\"`;;esacNODE_EXE=\"/home/pi/node/bin/node.exe\"if ! [ -x \"$NODE_EXE\" ]; then NODE_EXE=nodefiNPM_CLI_JS=\"/home/pi/node/lib/node_modules/npm/bin/npm-cli.js\"case `uname` in *MINGW*) NPM_PREFIX=`\"$NODE_EXE\" \"$NPM_CLI_JS\" prefix -g` NPM_PREFIX_NPM_CLI_JS=\"$NPM_PREFIX/node_modules/npm/bin/npm-cli.js\" if [ -f \"$NPM_PREFIX_NPM_CLI_JS\" ]; then NPM_CLI_JS=\"$NPM_PREFIX_NPM_CLI_JS\" fi ;; *CYGWIN*) NPM_PREFIX=`\"$NODE_EXE\" \"$NPM_CLI_JS\" prefix -g` NPM_PREFIX_NPM_CLI_JS=\"$NPM_PREFIX/node_modules/npm/bin/npm-cli.js\" if [ -f \"$NPM_PREFIX_NPM_CLI_JS\" ]; then NPM_CLI_JS=\"$NPM_PREFIX_NPM_CLI_JS\" fi ;;esac\"$NODE_EXE\" \"$NPM_CLI_JS\" \"$@\" 再次验证，npm已经可以正常工作","categories":[{"name":"树莓派","slug":"树莓派","permalink":"http://www.isimble.com/categories/%E6%A0%91%E8%8E%93%E6%B4%BE/"}],"tags":[{"name":"树莓派","slug":"树莓派","permalink":"http://www.isimble.com/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"},{"name":"nodejs","slug":"nodejs","permalink":"http://www.isimble.com/tags/nodejs/"},{"name":"pi","slug":"pi","permalink":"http://www.isimble.com/tags/pi/"}]},{"title":"Hello World","slug":"hello-world","date":"2018-03-22T11:14:57.000Z","updated":"2018-03-30T02:37:58.000Z","comments":true,"path":"2018/03/22/hello-world/","link":"","permalink":"http://www.isimble.com/2018/03/22/hello-world/","excerpt":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new \"My New Post\" More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}